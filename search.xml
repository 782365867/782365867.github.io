<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[hive 之join的用法]]></title>
    <url>%2F2019%2F07%2F21%2Fhive%2F</url>
    <content type="text"><![CDATA[Join 在 hive 中可将两个表连接在一起。包括join,inner join,left join, right join, full outer join,left semi join等，下面进行一一讲解。 ##join(等同于inner join) id name age 1 小明 11 2 小红 22 3 小白 33]]></content>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux之scp]]></title>
    <url>%2F2019%2F07%2F13%2Flinux%2F</url>
    <content type="text"><![CDATA[1scp local_file remote_username@remote_ip:remote_folder 或者 1scp local_file remote_username@remote_ip:remote_file 或者 1scp local_file remote_ip:remote_folder 或者 1scp local_file remote_ip:remote_file 若需要传文件夹，则加 -r]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive笔记1]]></title>
    <url>%2F2019%2F07%2F11%2Fhive%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[#hive笔记1 ##查看 这是加删除线的文字 列表内容 列表内容 列表内容 1 2 3 sdfsdfsdf sdf sdfs sdfs sssdf sdf 3 1234function fun()&#123; echo &quot;这是一句非常牛逼的代码&quot;;&#125;fun(); ##select语法结构12345678SELECT [ALL | DISTINCT] select_expr, select_expr, ... FROM table_reference[WHERE where_condition] [GROUP BY col_list [HAVING condition]] [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list] ] [LIMIT number] 其中，CLUSTER BY col_list就等同于[DISTRIBUTE BY col_list]+[SORT BY| 外部表和内部表的区别：外部表在drop时，只删表结构和表，不删原数据，即集群数据还在。 sort by 和 order by的区别： sort by 组内有序 order by 全局排序，只有一个reduce]]></content>
      <tags>
        <tag>hive</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1042. 托普利兹矩阵 python]]></title>
    <url>%2F2018%2F09%2F11%2Finterview-3%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728描述“托普利兹矩阵”是指如果从左上角到右下角的同一条主斜线上每个元素都相等的矩阵.给定一个M x N矩阵，判断是否为“托普利兹矩阵”.matrix 是一个二维整数数组.matrix 的行列范围都为 [1, 20].matrix[i][j] 的整数取值范围为[0, 99].您在真实的面试中是否遇到过这个题？ 是题目纠错样例样例 1:输入: matrix = [[1,2,3,4],[5,1,2,3],[9,5,1,2]]输出: True解释:123451239512在上述矩阵中，主斜线上元素分别为 &quot;[9]&quot;, &quot;[5, 5]&quot;, &quot;[1, 1, 1]&quot;, &quot;[2, 2, 2]&quot;, &quot;[3, 3]&quot;, &quot;[4]&quot;, 每一条主斜线上元素都相等，所以返回`True`.样例 2:输入: matrix = [[1,2],[2,2]]输出: False解释:主斜线 &quot;[1, 2]&quot; 有不同的元素. 简单题，直接代码：python123456789101112class Solution: &quot;&quot;&quot; @param matrix: the given matrix @return: True if and only if the matrix is Toeplitz &quot;&quot;&quot; def isToeplitzMatrix(self, matrix): # Write your code here for i in range(1,len(matrix)): for j in range(1,len(matrix[0])): if matrix[i][j]!=matrix[i-1][j-1]: return False return True java12345678910111213141516171819public class Solution &#123; /** * @param matrix: the given matrix * @return: True if and only if the matrix is Toeplitz */ public boolean isToeplitzMatrix(int[][] matrix) &#123; // Write your code here int m = matrix.length; int n = matrix[0].length; for(int i=1;i&lt;m;i++)&#123; for(int j=1;j&lt;n;j++)&#123; if(matrix[i][j]!=matrix[i-1][j-1])&#123; return false; &#125; &#125; &#125; return true; &#125;&#125;]]></content>
      <tags>
        <tag>面试</tag>
        <tag>python</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lintcode 814. Shortest Path in Undirected Graph 之python 和 java 实现]]></title>
    <url>%2F2018%2F09%2F11%2Finterview-2%2F</url>
    <content type="text"><![CDATA[最近没有动力学习，今天看到小宝贝那么努力学注会，顿感羞愧。 虽然收到了offercall，但还没有确定之前，还是好好刷刷题。既然有自己的目标，就应该给出自己的态度。1234567814. 无向图中的最短路径给出一个undirected graph，其中每条边的长度为1，并从图中给出两个节点。我们需要找到给定两个节点之间最短路径的长度。样例给定图= &#123;1,2,4#2,1,4#3,5#4,1,2#5,3&#125;，nodeA = 3, nodeB = 5。 首先想到的是深度优先搜索。写了好久，关于如何确定遍历完一层，res++的地方没理清楚。于是在网上搜了下资料，有c和java版的，就看了下别人的代码，再用python给实现一下。AC了 哈哈。其实我也不知道刷题用什么好，Python简洁好写，在写代码时候比Java省不少事，之前有个快排的题，同样思路，我用Java和Python分别实现，发现Python比Java短了不只一倍，看着很舒服，有点像伪代码。但是Java代码AC了，用了60ms多，但是Python 超时，6000多ms.. 很尴尬。 个人还是喜欢Python，希望笔试不要出现这种问题。 12345678910思路：广度优先搜索，用到队列1 取A结点入队，标记已访问，res=0，up_last = A2 当队列不为空时： repeat: 取队头元素——&gt;now_node 如果 now_node = B 取now_node的邻接nodes: 将没有访问的node,全入队并标记访问，然后last指向最后一个 如果当前指针now_node等于层指针up_last，层指针指向last,并且res++3 return 这里Python不叫指针，一般和Java一样，叫引用或者别的啥，我叫指针只是形象一些。 看代码也比较简单python3.5 123456789101112131415161718192021222324252627282930class Solution: &quot;&quot;&quot; @param graph: a list of Undirected graph node @param A: nodeA @param B: nodeB @return: the length of the shortest path &quot;&quot;&quot; def shortestPath(self, graph, A, B): import queue # Write your code here conti = queue.Queue() visited = [] conti.put(A) visited.append(A) last = A up_last = A res=0 while not conti.empty(): now = conti.get() if now == B: return res for i in now.neighbors: if i not in visited: conti.put(i) visited.append(i) last = i if up_last==now: up_last = last res +=1 return res java 参考https://blog.csdn.net/u014115273/article/details/80299072，对比看下吧1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * Definition for graph node. * class GraphNode &#123; * int label; * ArrayList&lt;UndirectedGraphNode&gt; neighbors; * UndirectedGraphNode(int x) &#123; * label = x; neighbors = new ArrayList&lt;UndirectedGraphNode&gt;(); * &#125; * &#125;; */public class Solution &#123; /** * @param graph: a list of Undirected graph node * @param A: nodeA * @param B: nodeB * @return: the length of the shortest path */ public int shortestPath(List&lt;UndirectedGraphNode&gt; graph, UndirectedGraphNode A, UndirectedGraphNode B) &#123; // Write your code here Queue&lt;UndirectedGraphNode&gt; queue = new LinkedList&lt;&gt;(); Set&lt;UndirectedGraphNode&gt; visited = new HashSet&lt;&gt;(); int res = 0; UndirectedGraphNode last = null; UndirectedGraphNode upperLast = null; queue.offer(A); visited.add(A); upperLast = A; while(!queue.isEmpty())&#123; UndirectedGraphNode now = queue.poll(); if(now.label == B.label)&#123; return res; &#125; for(UndirectedGraphNode next: now.neighbors)&#123; if(!visited.contains(next))&#123; queue.offer(next); visited.add(next); last = next; &#125; &#125; if(now == upperLast)&#123; upperLast = last; res++; &#125; &#125; return -1;//not connected &#125;&#125;]]></content>
      <tags>
        <tag>面试</tag>
        <tag>python</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[44.最小子数组（python3 实现）]]></title>
    <url>%2F2018%2F08%2F21%2Finterview-1%2F</url>
    <content type="text"><![CDATA[前两天在忙着搞大数据和论文，论文写差不多了，开始刷刷题，前几天笔试做的一塌糊涂，有的题明明感觉做对了，但是AC不了。趁着最近有空，赶紧找找感觉。一道简单的贪心题 44.最小子数组 给定一个整数数组，找到一个具有最小和的子数组。返回其最小和。 样例 给出数组[1, -1, -2, 1]，返回 -3 注意事项 子数组最少包含一个数字 解： 从第一个数往后遍历i=1-&gt;n将前k个数视为一个整体，如果这个整体&gt;0,则后面无论是什么，只要加上这个整体，肯定不会变小，所以整体直接舍弃，剩下的为新的数组nums.如果，前面的数为负，如前3个数的和为-7，第4个数为8，则-7+8=1,要不要舍弃呢？后面可能没有小于-7的子数组了呀？所以 我们还要有个变量res，用来保存各阶段的最小值，如这里的-7，如果后面有比-7更小的子数组，就更新这个变量的值，如果没有，res就是一直是-7。python3代码1234567891011121314151617class Solution: """ @param: nums: a list of integers @return: A integer indicate the sum of minimum subarray """ def minSubArray(self, nums): # write your code here sum = 0 res = nums[0] for i in nums: sum+=i if sum &lt; res: res = sum if sum &gt; 0: sum = 0 return res 刚开始不通过,看了下数据[1,1],全是正数，而我当时设的res初值为0.本来以为至少会有1个负值。所以把初值改为了第一个数。 每天一点 进步不难]]></content>
      <tags>
        <tag>面试</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop异常之java.lang.RuntimeException com.mysql.jdbc.exceptions.jdbc4.CommunicationsException Communications link failure]]></title>
    <url>%2F2018%2F08%2F13%2Fsqoop%2F</url>
    <content type="text"><![CDATA[sqoop是apache旗下一款“Hadoop和关系数据库服务器之间传送数据”的工具。导入数据：MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统；导出数据：从Hadoop的文件系统中导出数据到关系数据库 将导入或导出命令翻译成mapreduce程序来实现,在翻译出的mapreduce中主要是对inputformat和outputformat进行定制. 1 sqoop安装安装sqoop的前提是已经具备java和hadoop的环境 2 修改配置文件$ cd $SQOOP_HOME/conf$ mv sqoop-env-template.sh sqoop-env.sh打开sqoop-env.sh并编辑下面几行：export HADOOP_COMMON_HOME=/home/hadoop/hadoop-2.7.3/export HADOOP_MAPRED_HOME=/home/hadoop/hadoop-2.7.3/export HIVE_HOME=/home/work/hive/这里没用HBASE，用的时候再配。 3 将jdbc数据包mysql-connector-java-5.1.44.jar 拷贝到 $SQOOP_HOME/lib下 4 检查安装时否功$ cd $SQOOP_HOME/bin$ sqoop-version预期的输出：15/12/17 14:52:32 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6Sqoop 1.4.6 git commit id 5b34accaca7de251fc91161733f906af2eddbe83Compiled by abe on Fri Aug 1 11:19:26 PDT 2015到这里，整个Sqoop安装工作完成。 5 但是，有时候虽然安装成功了，但在使用中会出现各种Bug比如:MySQL数据库服务器中的xing/ emp表导入HDFS。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697[root@shizhan1 bin]# ./sqoop import \&gt; --connect jdbc:mysql://localhost:3306/xing \&gt; --username root \&gt; --password xing \&gt; --table emp \&gt; --m 1 Warning: /home/work/sqoop/bin/../../hbase does not exist! HBase imports will fail.Please set $HBASE_HOME to the root of your HBase installation.Warning: /home/work/sqoop/bin/../../hcatalog does not exist! HCatalog jobs will fail.Please set $HCAT_HOME to the root of your HCatalog installation.Warning: /home/work/sqoop/bin/../../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.18/08/13 10:42:47 INFO sqoop.Sqoop: Running Sqoop version: 1.4.618/08/13 10:42:47 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.18/08/13 10:42:48 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.18/08/13 10:42:48 INFO tool.CodeGenTool: Beginning code generation18/08/13 10:42:48 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `emp` AS t LIMIT 118/08/13 10:42:48 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `emp` AS t LIMIT 118/08/13 10:42:48 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /home/hadoop/hadoop-2.7.3注: /tmp/sqoop-root/compile/0cbad6aa7f1605ddcd735b7cd9f9e47a/emp.java使用或覆盖了已过时的 API。注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。18/08/13 10:42:49 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/0cbad6aa7f1605ddcd735b7cd9f9e47a/emp.jar18/08/13 10:42:49 WARN manager.MySQLManager: It looks like you are importing from mysql.18/08/13 10:42:49 WARN manager.MySQLManager: This transfer can be faster! Use the --direct18/08/13 10:42:49 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.18/08/13 10:42:49 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)18/08/13 10:42:49 INFO mapreduce.ImportJobBase: Beginning import of emp18/08/13 10:42:49 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar18/08/13 10:42:50 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps18/08/13 10:42:50 INFO client.RMProxy: Connecting to ResourceManager at /192.168.14.151:803218/08/13 10:42:53 INFO db.DBInputFormat: Using read commited transaction isolation18/08/13 10:42:54 INFO mapreduce.JobSubmitter: number of splits:118/08/13 10:42:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1532423449295_003118/08/13 10:42:54 INFO impl.YarnClientImpl: Submitted application application_1532423449295_003118/08/13 10:42:54 INFO mapreduce.Job: The url to track the job: http://shizhan1:8088/proxy/application_1532423449295_0031/18/08/13 10:42:54 INFO mapreduce.Job: Running job: job_1532423449295_003118/08/13 10:43:02 INFO mapreduce.Job: Job job_1532423449295_0031 running in uber mode : false18/08/13 10:43:02 INFO mapreduce.Job: map 0% reduce 0%18/08/13 10:43:09 INFO mapreduce.Job: Task Id : attempt_1532423449295_0031_m_000000_0, Status : FAILEDError: java.lang.RuntimeException: java.lang.RuntimeException: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failureThe last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server. at org.apache.sqoop.mapreduce.db.DBInputFormat.setConf(DBInputFormat.java:167) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:749) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)Caused by: java.lang.RuntimeException: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failureThe last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server. at org.apache.sqoop.mapreduce.db.DBInputFormat.getConnection(DBInputFormat.java:220) at org.apache.sqoop.mapreduce.db.DBInputFormat.setConf(DBInputFormat.java:165) ... 9 moreCaused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failureThe last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server. at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at com.mysql.jdbc.Util.handleNewInstance(Util.java:425) at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989) at com.mysql.jdbc.MysqlIO.&lt;init&gt;(MysqlIO.java:341) at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2189) at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2222) at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2017) at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:779) at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:47) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at com.mysql.jdbc.Util.handleNewInstance(Util.java:425) at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:389) at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:330) at java.sql.DriverManager.getConnection(DriverManager.java:571) at java.sql.DriverManager.getConnection(DriverManager.java:215) at org.apache.sqoop.mapreduce.db.DBConfiguration.getConnection(DBConfiguration.java:302) at org.apache.sqoop.mapreduce.db.DBInputFormat.getConnection(DBInputFormat.java:213) ... 10 moreCaused by: java.net.ConnectException: 拒绝连接 at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:579) at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:211) at com.mysql.jdbc.MysqlIO.&lt;init&gt;(MysqlIO.java:300) ... 26 more 可以看到，报错1Caused by: java.lang.RuntimeException: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure 应该是主要和数据库相关，后面还有一个拒绝连接的异常。 在网上找了些资料，有说把--connect jdbc:mysql://localhost:3306/xing这里的主机名改成IP，改了无果。仔细想了下，这里连不上数据库，可能是数据库配置问题，是不是Mysql不允许外部连接导致的，于是进入Mysql12use mysql;select host,user,password from user; 看到里面乱七八糟，host为shizhan1的user是空，还有一个host为shizhan1的user是root,但是密码为空，于是把第一个记录直接delete,第二个记录的password改成root的密码。GRANT ALL PRIVILEGES ON . TO root@”shizhan1” IDENTIFIED BY “ password “; //为root添加远程连接的能力，（mysql安装shizhan1上） 最后flush一下flush privileges; 重新跑一下我们的sqoop，但是会报错，提示一个exist。因为虽然之前没有跑通，但是在hdfs对应目录上生成了一个文件夹，就是这个文件夹exist了。删了就好，hadoop fs -rm -r /user/root/emp 可以重新跑sqoop了可以看到success，搞定！]]></content>
      <tags>
        <tag>异常解决</tag>
        <tag>mysql</tag>
        <tag>大数据</tag>
        <tag>sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[echart的series中label formatter回调函数使用及样式。]]></title>
    <url>%2F2018%2F08%2F03%2Fechart%2F</url>
    <content type="text"><![CDATA[今天我家宝贝儿写论文用到Echart做图，但是个性化的需要不知怎么做，找我帮忙。我很久没看前端的东西了，不过Echart那玩意好像很有意思，所以就接了下来。 先看需求。1 将紫色框部分改成自己的内容2 将红色框内容去掉3 将蓝色框内容改成自己定义的类别，（三种） 于是看了下代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596app.title = '嵌套环形图';option = &#123; tooltip: &#123; trigger: 'item', formatter: "&#123;a&#125; &lt;br/&gt;&#123;b&#125;: &#123;c&#125; (&#123;d&#125;%)" &#125;, legend: &#123; orient: 'vertical', x: 'left', data:['直达','营销广告','搜索引擎','邮件营销','联盟广告','视频广告','百度','谷歌','必应','其他'] &#125;, series: [ &#123; name:'访问来源', type:'pie', selectedMode: 'single', radius: [0, '30%'], label: &#123; normal: &#123; position: 'inner' &#125; &#125;, labelLine: &#123; normal: &#123; show: false &#125; &#125;, data:[ &#123;value:335, name:'直达', selected:true&#125;, &#123;value:679, name:'营销广告'&#125;, &#123;value:1548, name:'搜索引擎'&#125; ] &#125;, &#123; name:'访问来源', type:'pie', radius: ['40%', '55%'], label: &#123; normal: &#123; formatter: '&#123;a|&#123;a&#125;&#125;&#123;abg|&#125;\n&#123;hr|&#125;\n &#123;b|&#123;b&#125;：&#125;&#123;c&#125; &#123;per|&#123;d&#125;%&#125; ', backgroundColor: '#eee', borderColor: '#aaa', borderWidth: 1, borderRadius: 4, // shadowBlur:3, // shadowOffsetX: 2, // shadowOffsetY: 2, // shadowColor: '#999', // padding: [0, 7], rich: &#123; a: &#123; color: '#999', lineHeight: 22, align: 'center' &#125;, // abg: &#123; // backgroundColor: '#333', // width: '100%', // align: 'right', // height: 22, // borderRadius: [4, 4, 0, 0] // &#125;, hr: &#123; borderColor: '#aaa', width: '100%', borderWidth: 0.5, height: 0 &#125;, b: &#123; fontSize: 16, lineHeight: 33 &#125;, per: &#123; color: '#eee', backgroundColor: '#334455', padding: [2, 4], borderRadius: 2 &#125; &#125; &#125; &#125;, data:[ &#123;value:335, name:'直达'&#125;, &#123;value:310, name:'邮件营销'&#125;, &#123;value:234, name:'联盟广告'&#125;, &#123;value:135, name:'视频广告'&#125;, &#123;value:1048, name:'百度'&#125;, &#123;value:251, name:'谷歌'&#125;, &#123;value:147, name:'必应'&#125;, &#123;value:102, name:'其他'&#125; ] &#125; ]&#125;; 经过注释调试，发现需求1只要改最后一个data集合就好，把name和value换成自己自定义的内容。而需求2 3 主要在'&#123;a|&#123;a&#125;&#125;&#123;abg|&#125;\n&#123;hr|&#125;\n &#123;b|&#123;b&#125;：&#125;&#123;c&#125; &#123;per|&#123;d&#125;%&#125; '```这里。把、`&#123;c&#125;`去掉即可。12345678最难搞的是需求3，用了2小时搞明白的。这里是官方文档里的介绍http://echarts.baidu.com/option.html#grid.tooltip.formatterformatter有两种写法：一种字符串模板，另一种是回调函数。字符串模型里写的是，我们要改的地方是该series中name的值，这里name只能有一个值，改name是不能了，所以我们可以像改需求1那样，在data中传一个变量。用回调函数，文档中对回调函数也没有写明白。这里用简单的语言再梳理一下。主要有两点：将需要的个性化信息显示出来，然后展现出好看的样式。第一点：我们的个性化需求在data里写 data:[ {value:335, name:’直达’,flag:’type_1’}, {value:310, name:’邮件营销’,flag:’type_1’}, {value:234, name:’联盟广告’,flag:’type_1’}, {value:135, name:’视频广告’,flag:’type_2’}, {value:1048, name:’百度’,flag:’type_2’}, {value:251, name:’谷歌’,flag:’type_2’}, {value:147, name:’必应’,flag:’type_3’}, {value:102, name:’其他’,flag:’type_3’}]1234567也就是把我们需求3的内容```type_1,type_2,type_3```写在变量flag中，然后通过回调函数取出来，这里谁回调的先不管。回调函数的写法：```js formatter:function(params)&#123; str = params.data.flag+ params.name+params.percent return str&#125;, 这里params就是我们的data，一条一条往里传。看效果：可以看到，内容改过了，就是丑了点。我们想改成原来的样子怎么办呢？这里正是官方文档中最让我头疼的，找了半天没找到，在网上也不找到解释的比较好的。我们套用原有样式，只需要把我们的参数params.name这种替换到原来{a}这种的位置，我们的参数和其他内容用+连接。如下：12345 formatter:function(params)&#123; str = '&#123;a|'+ params.data.flag + '&#125;&#123;abg|&#125;\n&#123;hr|&#125;\n &#123;b|'+params.name+'：&#125; &#123;per|'+params.percent + '%&#125;' return str&#125;,// &#123;a| 这种表示a样式，而&#123;a&#125;这种有固定的含义，&#123;a&#125;（系列名称），&#123;b&#125;（数据项名称），&#123;c&#125;（数值）, &#123;d&#125;（百分比），样式里有条竖杠！记得区分。 最终效果：]]></content>
      <tags>
        <tag>echart</tag>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装zookeeper时候，可以查看进程启动，但是状态显示报错 Error contacting service. It is probably not running]]></title>
    <url>%2F2018%2F07%2F12%2Fzookeeper%2F</url>
    <content type="text"><![CDATA[最近想学一下大数据，这年头，不会大数据，都不敢说自己是21世纪新青年。 大数据有很多要学的框架，比如hadoop spark storm zookeeper等等，今天拿zookeeper入个门，安装过程中遇到点坑，分享一下。 Zookeeper是一个分布式协调服务；就是为用户的分布式应用程序提供协调服务A、zookeeper是为别的分布式程序服务的B、Zookeeper本身就是一个分布式程序（只要有半数以上节点存活，zk就能正常服务）C、Zookeeper所提供的服务涵盖：主从协调、服务器节点动态上下线、统一配置管理、分布式共享锁、统一名称服务……D、虽然说可以提供各种服务，但是zookeeper在底层其实只提供了两个功能： —管理(存储，读取)用户程序提交的数据； —并为用户程序提供数据节点监听服务； 1 部署用的是三台虚拟机，centOS 7jdk 8zookeeper3.4.5三台机器网络己配好，host配置如下1234567[root@shizhan1 conf]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6127.0.0.1 shizhan1192.168.33.3 shizhan1192.168.33.4 shizhan2192.168.33.5 shizhan3 2 安装https://pan.baidu.com/s/1O-E6pq3zbzoqRvxHoj66eQ 放我网盘了，zookeeper3.4.5 下载到shizhan1（第一台机子）中，我放在了/root/work/下解压：tar -zxvf zookeeper-3.4.5.tar.gz重命名：mv zookeeper-3.4.5 zookeeper（重命名文件夹zookeeper-3.4.5为zookeeper,太长 写起来好麻烦） 3 环境变量：vi /etc/profile添加内容：12export ZOOKEEPER_HOME=/root/work/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin source 一下，也就是把你写的环境变量激活一下source /etc/profile 注意，这里三台机子都要改环境变量哟，第二部安装可以只在一个机子上装好，然后拷到其他机子上去，但是环境变量三个都要改。 4 配置文件cd zookeeper/conf进入conf目录cp zoo_sample.cfg zoo.cfg 把例子copy一个出来，命名为zoo.cfg,我们要改的就是这个。vi zoo.cfg添加内容：12345dataDir=/root/zkdata （要创建文件夹）dataLogDir=/root/zklogdata （要创建文件夹）server.1=shizhan1:2888:3888 (主机名, 心跳端口、数据端口)server.2=shizhan2:2888:3888server.3=shizhan3:2888:3888 5 然后创建上面说的那俩文件夹,12mkdir /root/zkdatamkdir /root/zklogdata 注意，这里三台机子都要创建。6 将配好的zookeeper拷到另外两台机子上12scp -r /root/work/zookeeper hadoop@slave2:/home/hadoop/scp -r /root/work/zookeeper hadoop@slave3:/home/hadoop/ 7 在/root/zkdata/中创建myid文件，注意每台机子都要创建，但是值不一样，在步骤4中,如第一台，server.1=shizhan1:2888:3888，shizhan1是主机名，前面的serverl.1中的1,就是shizhan1这台机子上myid文件的值。以这台机子为例，其他两台的值分别为2 和 3。12cd /root/zkdata/echo 1 &gt; myid 8 启动服务1234[root@shizhan1 bin]# zkServer.sh startJMX enabled by defaultUsing config: /root/work/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 9 查看服务1234[root@shizhan2 conf]# zkServer.sh statusJMX enabled by defaultUsing config: /root/work/zookeeper/bin/../conf/zoo.cfgMode: follower 只有出现上面类似内容才是成功了。 排坑–今天启动后，发现jps里有quorumpeermain1234Starting zookeeper ... already running as process 1398.[root@shizhan2 ~]# jps1398 QuorumPeerMain1559 Jps 但是状态显示报错：Error contacting service. It is probably not running 1）在网上查阅资料一共有几种解决方法：打开zkServer.sh 找到status) STAT=echo stat | nc localhost $(grep clientPort &quot;$ZOOCFG&quot; | sed -e &#39;s/.*=//&#39;) 2&gt; /dev/null| grep Mode在nc与localhost之间加上 -q 1 （是数字1而不是字母l）如果已存在则去掉注:因为我用的zookeeper是3.4.5版本，所以在我的zkServer.sh脚本文件里根本没有这一行,所以没有生效 2）调用sh zkServer.sh status 遇到这个问题。百度，google了后发现有人是修改sh脚本里的一个nc的参数来解决，可在3.4.5的sh文件里并没有找到nc的调用。配置文档里指定的log目录没有创建导致出错，手动增加目录后重启，问题解决。注:我想不是日志的问题所以这个方法根本就没有试 3）创建数据目录,也就是在你zoo.cfg配置文件里dataDir指定的那个目录下创建myid文件,并且指定id,改id为你zoo.cfg文件中server.1=shizhan1:2888:3888中的 1.只要在myid头部写入1即可. 4）因为防火墙没有关闭。关闭防火墙：12345678910#查看防火墙状态 service iptables status #关闭防火墙 service iptables stop#查看防火墙开机启动状态 chkconfig iptables --list#关闭防火墙开机启动 chkconfig iptables off 注意：我的确在开始时候没有关闭防火墙，但是当我关闭防火墙之后也没有解决问题。 如果是centOS 7，12[root@shizhan2 conf]# systemctl stop firewalld.service[root@shizhan2 conf]# systemctl disable firewalld.service 我就是关防火墙的时候，用了iptables 那个命令，提示中显示如下，我就大概一看，以为是说防火墙没开，其实人家早就换新命令啦。123[root@shizhan1 work]# service iptables stopRedirecting to /bin/systemctl stop iptables.serviceFailed to stop iptables.service: Unit iptables.service not loaded. 5） 没有建立主机和ip之间的映射关系。 建立主机和ip之间映射关系的命令为 vim /etc/hosts 在文件的末端加入各个主机和ip地址之间的映射关系就可以了。 注意：只有在建立了映射关系之后，才可以将在同一个网段下的机器利用主机名进行文件传递。问题解决！]]></content>
      <tags>
        <tag>异常解决</tag>
        <tag>zookeeper</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ERROR 1130 (HY000) Host '114.213.234.22' is not allowed to connect to this MySQL server]]></title>
    <url>%2F2018%2F07%2F05%2Fmysql%2F</url>
    <content type="text"><![CDATA[给学校一个部门写了个网站，这个网站要用到学校的人事信息表，但是人事信息经常变动，所以跟学校人事部门联系下，每天将新的人事表推送到我们的数据库中。 然后人事部门发来个截图，说连接不上。 大概感觉是权限问题，查了些资料，原来mysql的安全机制，默认只允许本地连接。于是进入数据库中，改了下配置。1234use mysql;update user set host = &apos;%&apos; where user = &apos;root&apos;;flush privileges;quit 把 host 设为 % 就是解除了本地主机访问的限制。]]></content>
      <tags>
        <tag>异常解决</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[揭开迷雾，来一顿美味的Capsule盛宴]]></title>
    <url>%2F2018%2F06%2F22%2Fcapsnet%2F</url>
    <content type="text"><![CDATA[由深度学习先驱 Hinton 开源的 Capsule 论文《Dynamic Routing Between Capsules》，无疑是去年深度学习界最热点的消息之一。得益于各种媒体的各种吹捧，Capsule 被冠以了各种神秘的色彩，诸如“抛弃了梯度下降”、“推倒深度学习重来”等字眼层出不穷，但也有人觉得 Capsule 不外乎是一个新的炒作概念。 本文试图揭开让人迷惘的云雾，领悟 Capsule 背后的原理和魅力，品尝这一顿 Capsule 盛宴。同时，笔者补做了一个自己设计的实验，这个实验能比原论文的实验更有力说明 Capsule 的确产生效果了。 1 前言Capsule 的论文已经放出几个月了，网上已经有很多大佬进行解读，也有大佬开源实现了 CapsuleNet，这些内容都加速了我对 Capsule 的理解。然而，我觉得美中不足的是，网上多数的解读，都只是在论文的翻译上粉饰了一点文字，并没有对 Capsule 的原理进行解读。比如“动态路由”那部分，基本上就是照搬论文的算法，然后说一下迭代 3 次就收敛了。但收敛出什么来？论文没有说，解读也没有说，这显然是不能让人满意的。也难怪知乎上有读者评论说： 所谓的 capsule 为 dl 又贡献了一个花里胡哨的 trick 概念。说它是 trick，因为 hinton 没有说为什么 routing 算法为什么需要那么几步，循环套着循环，有什么理论依据吗？还是就是凑出来的？如何评价深度学习之父 Hinton 发布的 Capsule 论文？ - 知乎 这个评论虽然过激，然而也是很中肯的：凭啥 Hinton 摆出来一套算法又不解释，我们就要稀里糊涂的跟着玩？ 2 Capsule 盛宴2.1 宴会特色这次 Capsule 盛宴的特色是“vector in vector out”，取代了以往的“scaler in scaler out”，也就是神经元的输入输出都变成了向量，从而算是对神经网络理论的一次革命。然而真的是这样子吗？难道我们以往就没有做过“vector in vector out”的任务了吗？有，而且多的是！NLP 中，一个词向量序列的输入，不就可以看成“vector in”了吗？这个词向量序列经过 RNN/CNN/Attention 的编码，输出一个新序列，不就是“vector out”了吗？在目前的深度学习中，从来不缺乏“vector in vector out”的案例，因此显然这不能算是 Capsule 的革命。 Capsule 的革命在于：它提出了一种新的“vector in vector out”的传递方案，并且这种方案在很大程度上是可解释的。 如果问深度学习（神经网络）为什么有效，我一般会这样回答：神经网络通过层层叠加完成了对输入的层层抽象，这个过程某种程度上模拟了人的层次分类做法，从而完成对最终目标的输出，并且具有比较好的泛化能力。的确，神经网络应该是这样做的，然而它并不能告诉我们它确确实实是这样做的，这就是神经网络的难解释性，也就是很多人会将深度学习视为黑箱的原因之一。 让我们来看 Hinton 是怎么来通过 Capsule 突破这一点的。 2.2 大盆菜如果要用一道菜来比如 Capsule，我想到了“大盆菜”： 盆菜作为客家菜的菜式出现由来以久，一般也称为大盘菜，大盘菜源于客家人传统的“发财大盘菜”，顾名思义就是用一个大大的盘子，将食物都放到里面，融汇出一种特有滋味。丰富的材料一层层叠进大盘之中，最易吸收肴汁的材料通常放在下面。吃的时候每桌一盘，一层一层吃下去，汁液交融，味道馥郁而香浓，令人大有渐入佳景之快。 Capsule 就是针对着这个“层层递进”的目标来设计的，但坦白说，Capsule 论文的文笔真的不敢恭维，因此本文尽量不与论文中的符号相同，以免读者再次云里雾里。让我们来看个图。如图所示，底层的胶囊和高层的胶囊构成一些连接关系。等等，什么是“胶囊”？其实，只要把一个向量当作一个整体来看，它就是一个“胶囊”，是的，你没看错，你可以这样理解：神经元就是标量，胶囊就是向量，就这么粗暴！Hinton 的理解是：每一个胶囊表示一个属性，而胶囊的向量则表示这个属性的“标架”。也就是说，我们以前只是用一个标量表示有没有这个特征（比如有没有羽毛），现在我们用一个向量来表示，不仅仅表示有没有，还表示“有什么样的”（比如有什么颜色、什么纹理的羽毛），如果这样理解，就是说在对单个特征的表达上更丰富了。 说到这里，我感觉有点像 NLP 中的词向量，以前我们只是用 one hot 来表示一个词，也就是表示有没有这个词而已。现在我们用词向量来表示一个词，显然词向量表达的特征更丰富，不仅可以表示有没有，还可以表示哪些词有相近含义。词向量就是 NLP 中的“胶囊”？这个类比可能有点牵强，但我觉得意思已经对了。 那么，这些胶囊要怎么运算，才能体现出“层层抽象”、“层层分类”的特性呢？让我们先看其中一部分连接： 图上只展示了 \(\mu_1\)的连接。这也就是说，目前已经有了\(\mu_1\)这个特征，（假设是羽毛），那么我想知道它属于上层特征\(\nu_{1}，\nu_{2}，\nu_{3}，\nu_{4}\)假设分别代表了鸡、鸭、鱼、狗）中的哪一个。分类问题我们显然已经是很熟悉了，不就是内积后 softmax 吗？于是单靠\(\mu_{1}\)这个特征，我们推导出它是属于鸡、鸭、鱼、狗的概率分别是$$(P_{1|1},P_{2|1},P_{3|1},P_{4|1})=\frac{1}{Z_1}(e^\left \langle \mu1,\nu1 \right \rangle,e^\left \langle \mu1,\nu2 \right \rangle,e^\left \langle \mu1,\nu3 \right \rangle,e^\left \langle \mu1,\nu4 \right \rangle)\tag{1}$$我们当然期望 \(P_{1|1}\)和\(P_{2|1}\)会明显大于 \(P_{3|1}\)和\(P_{4|1}\)。不过，单靠这个特征还不够，我们还需要综合各个特征，于是可以把上述操作对各个 \(\mu_i\)都做一遍，继而得到\((P_{1|2},P_{2|2},P_{3|2},P_{4|2})\)、\((P_{1|3},P_{2|3},P_{3|3},P_{4|3})\)、…问题是，现在得到这么多预测结果，那我究竟要选择哪个呢？而且我又不是真的要做分类，我要的是融合这些特征，构成更高级的特征。于是 Hinton 认为，既然\(\mu_i\)这个特征得到的概率分布是 \((P_{1|i},P_{2|i},P_{3|i},P_{4|i})\),那么我把这个特征切成四份，分别为 \((P_{1|i}\mu_i,P_{2|i}\mu_i,P_{3|i}\mu_i,P_{4|i}\mu_i)\)，然后把这几个特征分别传给 \(\nu_1,\nu_2,\nu_3,\nu_4\),最后\(\nu_1,\nu_2,\nu_3,\nu_4\)其实就是各个底层传入的特征的累加，这样不就好了？$$\nu_j=squash(\sum_{i}p_{j|i}\mu_i) = squash(\sum_i\frac{e^\left \langle \mu_i,\nu_j \right \rangle}{Z_i}\mu_i)\tag{2}$$从上往下看，那么 Capsule 就是每个底层特征分别做分类，然后将分类结果整合。这时 \(\nu_j\)应该尽量与所有\(\mu_i\)都比较靠近，靠近的度量是内积。因此，从下往上看的话，可以认为 \(\nu_j\)实际上就是各个\(\mu_i\)的某个聚类中心，而 Capsule 的核心思想就是输出是输入的某种聚类结果。 现在来看这个 squash 是什么玩意，它怎么来的呢？ 2.3 浓缩果汁squash 在英文中也有浓缩果汁之意，我们就当它是一杯果汁品尝吧。这杯果汁的出现，是因为 Hinton 希望 Capsule 能有的一个性质是：胶囊的模长能够代表这个特征的概率。 其实我不喜欢概率这个名词，因为概率让我们联想到归一化，而归一化事实上是一件很麻烦的事情。我觉得可以称为是特征的“显著程度”，这就好解释了，模长越大，这个特征越显著。而我们又希望有一个有界的指标来对这个“显著程度”进行衡量，所以就只能对这个模长进行压缩了，所谓“浓缩就是精华”嘛。Hinton 选取的压缩方案是： $$squash(x) = \frac{\left | x \right |^2}{1+\left | x \right |^2}\frac{x}{\left | x \right |}\tag{3}$$其中\(x/\left | x \right |\)是很好理解的，就是将模长变为 1，那么前半部分怎么理解呢？为什么这样选择？事实上，将模长压缩到 0～1 的方案有很多，并不确定 Hinton 选择目前这个方案的思路。然而，一个值得思考的问题是：如果在中间层，那么这个压缩处理是不是必要的呢？因为已经有了后面说的动态路由在里边，因此即使去掉squash 函数，网络也已经具有了非线性了，因此直觉上并没有必要在中间层也引入特征压缩，正如普通神经网络也不一定要用 sigmoid 函数压缩到 0～1。我觉得这个要在实践中好好检验一下。 3 动态路由注意到(2)式，为了求\(\nu_j\) 需要求 softmax，可是为了求 softmax 又需要知道 \(\nu_j\)，这不是个鸡生蛋、蛋生鸡的问题了吗？这时候就要上“主菜”了，即“动态路由”（Dynamic Routing），它能够根据自身的特性来更新（部分）参数，从而初步达到了 Hinton 的放弃梯度下降的目标。 这道“主菜”究竟是不是这样的呢？它是怎么想出来的？最终收敛到哪里去？让我们先上两道小菜，然后再慢慢来品尝这道主菜。 3.1 小菜 1让我们先回到普通的神经网络，大家知道，激活函数在神经网络中的地位是举足轻重的。当然，激活函数本身很简单，比如一个 tanh 激活的全连接层，用 tensorflow 写起来就是：12y = tf.matmul(W, x) + by = tf.tanh(y) 可是，如果我想用 \(x = y + cos(y)\)的反函数来激活呢？也就是说，你得给我解出 \(y=f(x)\)，然后再用它来做激活函数。 然而数学家告诉我们，这个东西的反函数是一个超越函数，也就是不可能用初等函数有限地表示出来。那这样不就是故意刁难么？不要紧，我们有迭代：$$y_n+1=x - cos y_n$$选择y0=x，代入上式迭代几次，基本上就可以得到比较准确的y 了。假如迭代三次，那就是\(y = x - cos(x - cos( x - cosx))\)用 tensorflow 写出来就是1234y = tf.matmul(W, x) + bY = yfor i in range(3): Y = y - tf.cos(Y) 如果读者已经“预习”过 Capsule，那么就会发现这跟 Capsule 的动态路由很像。 3.2 小菜 2再来看一个例子，这个例子可能在 NLP 中有很多对应的情景，但图像领域其实也不少。考虑一个向量序列 \(x_1,x_2,…x_n\)，我现在要想办法将这 n个向量整合成一个向量 x(encoder)，然后用这个向量来做分类。 也许读者会想到用 LSTM。但我这里仅仅想要将它表示为原来向量的线性组合，也就是：$$ x = \sum ^{n}_{i=1}\lambda_ix_i$$这里的\(\lambda_i\)相当于衡量\(x\)与\(x_i\)的相似度。然而问题来了，在 x 出现之前，凭什么能够确定这个相似度呢？这不也是一个鸡生蛋、蛋生鸡的问题吗？解决这个问题的一个方案也是迭代。首先我们也可以定义一个基于 softmax 的相似度指标，然后让$$x = \sum ^n_{i=1}\frac{e^\left \langle x,x_i \right \rangle}{Z}x_i$$一开始，我们一无所知，所以只好取 x为各个 \(x_i\) 的均值，然后代入右边就可以算出一个，再把它代入右边，反复迭代就行，一般迭代有限次就可以收敛，于是就可以将这个迭代过程嵌入到神经网络中了。 如果说小菜 1 跟动态路由只是神似，那么小菜 2 已经跟动态路由是神似＋形似了。不过我并没有看到已有的工作是这样做的，这个小菜只是我的头脑风暴。 3.3 上主菜～其实有了这两个小菜，动态路由这道主菜根本就不神秘了。为了得到各个 \(\nu_j\)，一开始先让它们全都等于\(\mu_i\)的均值，然后反复迭代就好。说白了，输出是输入的聚类结果，而聚类通常都需要迭代算法，这个迭代算法就称为“动态路由”。至于这个动态路由的细节，其实是不固定的，取决于聚类的算法，比如关于 Capsule 的新文章《MATRIX CAPSULES WITH EM ROUTING》就使用了 Gaussian Mixture Model 来聚类。 理解到这里，就可以写出本文的动态路由的算法了： 动态路由算法初始化\(b_{ij}=0\)迭代r次： \(c_i = softmax(b_j)\) \(s_j = \sum _ic_{ij}\mu_i\) \(\nu_j = squash(s_j)\) \(b_{ij} = b_{ij}+\left \langle \mu_i,\nu_j \right \rangle\)返回\(\nu_j\) 这里用的原文中的算法，苏老师根据自己的理解进行了改写，可以参考其博客。 4 模型细节下面介绍 Capsule 实现的细节 4.1 全连接版先不管是 Hinton 版还是我的版本，按照这个动态路由的算法，\(\nu_j\)能够迭代地算出来，那不就没有参数了吗？真的抛弃了反向传播了？ 非也非也～如果真的这样的话，各个 \(\nu_j\) 都一样了。前面已经说了，\(\nu_j\)是作为输入 \(\mu_i\)的某种聚类中心出现的，而从不同角度看输入，得到的聚类结果显然是不一样的。那么为了实现“多角度看特征”，于是可以在每个胶囊传入下一个胶囊之前，都要先乘上一个矩阵做变换，所以 (2)式实际上应该要变为$$\nu_j = squash(\sum_i \frac{e ^\left \langle \hat{\mu}_{j|i},\nu_j \right \rangle}{Z_i}\hat{\mu}{j|i}),\hat{\mu}{j|i} = W_{ji}\mu_i\tag{4}$$这里的 \(W_{ji}\)是待训练的矩阵，这里的乘法是矩阵乘法，也就是矩阵乘以向量。所以，Capsule 变成了下图 这时候就可以得到完整动态路由了 动态路由算法初始化\(b_{ij}=0\)迭代r次： \(c_i = softmax(b_j)\) \(s_j = \sum_ic_{ij}\hat{\mu}_{j|i}\) \(\nu_j = squash(s_j)\) \(b_{ij} = b_{ij}+\left \langle \hat{\mu}_{j|i},\nu_j \right \rangle\)返回\(\nu_j\)这样的 Capsule 层，显然相当于普通神经网络中的全连接层。 4.2 共享版众所周知，全连接层只能处理定长输入，全连接版的 Capsule 也不例外。而 CNN 处理的图像大小通常是不定的，提取的特征数目就不定了，这种情形下，全连接层的 Capsule 就不适用了。因为在前一图就可以看到，参数矩阵的个数等于输入输入胶囊数目乘以输出胶囊数目，既然输入数目不固定，那么就不能用全连接了。 所以跟 CNN 的权值共享一样，我们也需要一个权值共享版的 Capsule。所谓共享版，是指对于固定的上层胶囊j，它与所有的底层胶囊的连接的变换矩阵是共用的，即\(W_{ji} = W_{j}\)， 如图所示，共享版其实不难理解，就是自下而上地看，就是所有输入向量经过同一个矩阵进行映射后，完成聚类进行输出，将这个过程重复几次，就输出几个向量（胶囊）；又或者自上而下地看，将每个变换矩阵看成是上层胶囊的识别器，上层胶囊通过这个矩阵来识别出底层胶囊是不是有这个特征。因此很明显，这个版本的胶囊的参数量并不依赖于输入的胶囊个数，因此可以轻松接在 CNN 后面。对于共享版，(2)式要变为$$\nu_j = squash(\sum_i \frac{e ^\left \langle \hat{\mu}_{j|i},\nu_j \right \rangle}{Z_i}\hat{\mu}{j|i}),\hat{\mu}{j|i} = W_{ji}\mu_i\tag{5}$$至于动态路由算法就没有改变了。 4.3 反向传播尽管我不是很喜欢反向传播这个名词，然而这里似乎不得不用上这个名字了。 现在又有了 \(W_{ji}\)，那么这些参数怎么训练呢？答案是反向传播。读者也许比较晕的是：现在既有动态路由，又有反向传播了，究竟两者怎么配合？其实这个真的就最简单不过了。就好像“小菜 1”那样，把算法的迭代几步（论文中是 3 步），加入到模型中，从形式上来看，就是往模型中添加了三层罢了，剩下的该做什么还是什么，最后构建一个 loss 来反向传播。 这样看来，Capsule 里边不仅有反向传播，而且只有反向传播，因为动态路由已经作为了模型的一部分，都不算在迭代算法里边了。 4.4 做了什么是时候回顾一下了，Capsule 究竟做了什么？其实用一种最直接的方式来讲，Capsule 就是提供了一种新的“vector in vector out”的方案，这样看跟 CNN、RNN、Attention 层都没太大区别了；从 Hinton 的本意看，就是提供了一种新的、基于聚类思想来代替池化完成特征的整合的方案，这种新方案的特征表达能力更加强大。 本文转自苏老师：https://kexue.fm/archives/4819/]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>胶囊网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k-max-pooling的keras实现]]></title>
    <url>%2F2018%2F06%2F19%2Fkmaxpooling%2F</url>
    <content type="text"><![CDATA[pooling（池化）是卷积神经网络中常用操作，主要目的是减小张量大小，便于计算，可以认为是一步降维操作。前两天项目上需要用到k-max-pooling操作，项目代码是用keras写的，keras没有k-max-pooling层，所以要自己写。 卷积神经网络的最经典的模型是卷积+池化+全连接。其中卷积操作通过一个滑动的卷积窗口来强化提取特征，可以认为卷积窗是一个权重矩阵，而卷积操作中的每一步，都是一个矩阵的加权求和，随着卷积窗口滑动，每一步的和被记录下来，形成一个新的矩阵。我们有多少种卷积窗，经过一层卷积就留下几个矩阵。然而，如果经过多层卷积，那最后矩阵就越来越大，那我们对资源耗费太大，有没有什么好的操作呢？–pooling 简单理解，pooling就是把一个大矩阵，变成小矩阵。如max-pooling，如size=2,就是在原来的矩阵中，每2*2个块中，取其中最大的一个记录下来，然后下一个块，再下一个十几个，最后记录下来的值就成了一个新矩阵，而这个新矩阵的大小，比以前缩小了4倍。有人会问，那不是损失了信息了么？当然损失了信息，但是我们认为损失的信息并没那么重要，就像一幅超清图，和一张高清图，虽然没那么清晰了，但这张图所描述的信息我们还是一眼能看出来。max-pooling操作，可以这么理解，给你一张图片，你分成了好多块，每块里面，别的地方颜色很浅，就一个地方颜色很深，那你是不是第一眼先看到那个黑点呢？ 再说一下mean-pooling.和max-pooling类似，只不过把每块中取最大值的操作，换成了对每块取均值。这样就会把每个点的值都考虑到，没有max-pooling操作那样粗暴。 还有一种常用的操作是k-max-pooling.这种是在max-pooling上改进来的，因为max-pooling操作太简单粗暴了，k-max-pooling认为每一块不只一个点重要，前几个亮点都比较重要，所以在每一个pooling块中取了前k大的值。 keras中pooling操作好像只有maxpooling和meanpooing.于是自己写了个代码，实现k-max-pooling。因为项目中用的是LSTM处理文本，我这里就对LSTM层的结果进行处理了，LSTM层输出三维张量(shape=[in_dim1,in_dim2,in_dim3]),其中in_dim1表示样本数量，in_dim2表示一个样本（也就是一段话）的长度（如：这里认为一句话有100个词，那么短于100的句子后面补0，长于100的句子后面截断），in_dim3为lstm结点数量，也就是每个step输出的向量长度。k-max-pooling操作，就是在dim2中，取出k个最大值，可以想象是取出k个最重要的词。输入为LSTM层输出的三维张量，输出为一个二维张量，shape=[out_dim1,out_dim2],out_dim1为样本数，out_dim2为in_dim3*k。 代码如下：123456789101112131415161718192021222324252627def test59(): from keras.layers import Lambda, Input from keras.models import Model import tensorflow as tf l = [[1, 2, 3, 4, 5, 6, 7, 8, 9], [11, 22, 33, 44, 55, 66, 77, 88, 99]] data = np.reshape(l, [2, 3, 3]) print(data)###output####[[[ 1 2 3]# [ 4 5 6]# [ 7 8 9]]# [[11 22 33]# [44 55 66]# [77 88 99]]]#数据为2个样本，每个样本是3个step（可以认为是句子有3个词），每个step是一个长为3的向量（可以认为词向量长度为3） input = Input(shape=[3, 3], dtype='int32') la = Lambda(lambda x: tf.reshape(tf.nn.top_k(tf.transpose(x,[0,2,1]),k=2)[0],shape=[-1,6]))(input) model = Model(inputs=input, outputs=la) pre = model.predict(data) print(pre)#[[ 7 4 8 5 9 6]#[77 44 88 55 99 66]]if __name__=='__main__': test59() 这里的k-max-pooling用Lambda写的，tf.reshape(tf.nn.top_k(tf.transpose(x,[0,2,1]),k=2)[0],shape=[-1,6])，分解成三步：tf.transpose(x,[0,2,1]–&gt;因为tf有个top_k方法，可以对取张量最后一维的前k大的数，所以我们把step所在维度调整到最后。transpose是一个转置操作。tf.nn.top_k（，k=2）–&gt;取出最后一个维度前k大的数。tf.reshape(),top_k操作对每个样本取出的结果是一个二维矩阵in_dim3 × k，所以把它转成向量，向量长度为in_dim3*k 当然，也可以写一个自定义层，处理步骤类似 12345678910111213141516171819202122class KMaxPooling(Layer): """ k-max-pooling """ def __init__(self, k=1, **kwargs): super().__init__(**kwargs) self.input_spec = InputSpec(ndim=3) self.k = k def compute_output_shape(self, input_shape): return (input_shape[0], (input_shape[2] * self.k)) def call(self, inputs): # swap last two dimensions since top_k will be applied along the last dimension shifted_input = tf.transpose(inputs, [0, 2, 1]) # extract top_k, returns two tensors [values, indices] top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=True, name=None)[0] # return flattened output return Flatten()(top_k)]]></content>
      <tags>
        <tag>python</tag>
        <tag>深度学习</tag>
        <tag>Keras</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[n阶乘后面有多少个0]]></title>
    <url>%2F2018%2F06%2F11%2Finterview%2F</url>
    <content type="text"><![CDATA[开始整理下面试题和心得，这一部分包含Lintcode和平时遇到的一些题目。 题目描述：给定一个正整数n,返回出n的阶乘尾部0的个数。linkcode链接：https://lintcode.com/problem/trailing-zeros/description 分析：看到这一题第一感觉先求阶乘，然后求出结果中尾部有多少0。提交代码，超时。 于是查了下资料，一个数尾部有多少个0，先看这个数的因子，也就是因子分解，如10 分解为2 5，20 分解为2 2 5，100 分解为 2 2 5 5，多找几组数据，发现尾部0的个数为min{2的数，5的个数}。为什么会这样呢？因为尾部的0是由25得出的，当然也可以是45，但是4=2*2呀，最终0还是由2和5造出来的。一对2和5造出来一个0，n对2和5造出n个0，如果有10个2，8个5，则只能凑出8对，所以是min{…}.阶乘也就是因子分解的过程，而且因子从小到大排好了。2的个数肯定比5多，因为5大。所以这个题，看因子中5出现的次数就可以啦。 这里n阶乘用n!表示。5！ –&gt; 1 2 3 4 5 有一个510! –&gt; 1 2 3 4 5 6 7 8 9 10 有2个5（10=25）…25! –&gt; 1 2 3 4 5 …10 ..15 ..20 ..24 25 有6个5（25=55） 125！ = 5 10 15 …25 ..50 ..75 ..100 ..125 有多少呢？我们发现每隔5个数出现一个5，隔25个数多出来1个5..隔125个数再多出一个5也就是从n个抽出55 10 15 20 25 30 … = 5（1 2 3 4 5 ）然后从上面序列中再隔5抽25 50 75 100 125 … = 55（1 2 3 4 5 ）再隔5抽125 250 375 500 625 .. = 55*5（1 2 3 4 5 ） 规律很明显了 好了上代码 python3 1234567891011class Solution: &quot;&quot;&quot; @param: n: An integer @return: An integer, denote the number of trailing zeros in n! &quot;&quot;&quot; def trailingZeros(self, n): zeros = 0 while n &gt; 0: zeros += n // 5 n //= 5 return int(zeros)]]></content>
      <tags>
        <tag>面试</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python数据存储利器之pickle]]></title>
    <url>%2F2018%2F06%2F05%2Fpython-1%2F</url>
    <content type="text"><![CDATA[第一天上班那天，有位同事把部分工作交接给我，主要是处理过的一些数据，存的是txt格式。然后大概看了下，还挺整齐，用’,’分隔的。读进去才发现，本来应该是个List,然而由于存成了txt，所有的都变成了str。所以转来转去，费了两天时间。突然想起来好像有pickle这么个小玩意，就拿来试试。从此，再也不想用csv啥的存数据了 简单介绍下Picklepickle使用二进制协议完成对python对象的序列化和反序列化。“picking”是将python对象转为字节流，而“unpickling”则是从字节流中转出来。也就是说，无论你要存的内容是字符串、数字、数组、列表、矩阵、DataFrame、np.array()等等，当你进行picking操作时，统统将这些内容转换成字节流保存到文件中，一般是二进制文件。然后当你想使用这些内容时，再从二进制文件中读出来。1import pickle –picking–123data = ...with open('name.txt','wb') as f: pickle.dump(data,f,protocol=None) 这样就把数据写进name.txt文件里了，注意这里是二进制，你用记事本打开是乱码。有个参数，protocol,是你选用的协议，默认为3，在python2中默认为0，如果你在python3用dump写进文件，去python2里load读取文件,会报异常。当然这种奇怪的需求很少，如果遇到了，把dump里的protocol=0就可以了。当然，你的data可以有多个如：1234a= 1b =3with open('name.txt','wb') as f: pickle.dump([a,b],f) –unpicking–12with open('name.txt','rb') as f: data = pickle.load(f) 如果你存的数据是多个那么：123with open('name.txt','rb') as f: data= pickle.load(f) a,b = data 还有除了dump和load，还有dumps和loads。dumps()和loads()用的不太多，简单说下，和dump类似，dumps把数据写成字节对象，但不用保存到文件。loads则是从字节对象中读出，不用读文件。]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lda主题模型python实现篇]]></title>
    <url>%2F2018%2F06%2F01%2Flda%2F</url>
    <content type="text"><![CDATA[最近在做一个动因分析的项目，自然想到了主题模型LDA。这次先把模型流程说下，原理后面再讲。lda实现有很多开源库，这里用的是gensim. 1 文本预处理大概说下文本的样子，LDA是无监督模型，也就是说不需要标签，只要传入文本就好。LDA要学习文档-主题分布和主题-词分布，所以我们把一个人的数据join在一起作为一条文档。对文档进行分词，使用的jieba分词工具包。注意，这里要做去停用词处理，包括标点和一些没用的词，如“呵呵”，“哈哈”。做项目时，第一版没有去无用词，最后提出的主题都是“你”“我”“他”“你好”这样的东西，去掉之后可以较好提高结果质量。 2 将上步处理的结果进行格式化表示即将所有文档数表示成m*n的矩阵D,m表示有m篇文档，n表示这篇文档有n个词，n不定长。 3 生成词典用gensim.corpora.Dictionary包这个包讲下吧12345678910from gensim.corpora import Dictionarytext = [[&apos;我&apos;, &apos;想吃&apos;, &apos;大龙虾&apos;, &apos;和&apos;, &apos;烤猪蹄&apos;]]dictionary = Dictionary(text)print((dictionary))doc = dictionary.doc2bow([&apos;我&apos;, &apos;想吃&apos;, &apos;大龙虾&apos;, &apos;和&apos;, &apos;我&apos;,&apos;你&apos;,&apos;烤猪蹄&apos;])print(doc)#####output#####Dictionary(5 unique tokens: [&apos;我&apos;, &apos;大龙虾&apos;, &apos;想吃&apos;, &apos;和&apos;, &apos;烤猪蹄&apos;])[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1)] 可以看出，我们把我想吃大龙虾和烤猪蹄编成字典，共五个词，这里输出结果里看不出是个字典，其实是有下标的，0对应我，1对应大龙虾，…4对应烤猪蹄。然后我们在下一步将文本变成词袋，这里用的文本是[‘我’, ‘想吃’, ‘大龙虾’, ‘和’, ‘我’,’你’,’烤猪蹄’],注意文本格式也是词为元素的列表。这句话是我自己构造的，只是为了说两点，语法请忽略。第一点：“我”这个词出现了两次，所以下标为2的地方，值为2；第二点，“你”这个词出现了1次，可是在词典中没有，所有直接被忽略。 这样就可以用字典，将文本表示成词袋模型，词袋模型不懂的，见我另一篇文章，自然语言处理NLP的词如何表示。当我们做完了LDA模型后，对于新的文本，我们想看下它所在的主题分布，就要使用该字典再进行词袋编码，也就是说这个字典，我们以后也会用到，所以，我们在这里把词典保存起来。保存词典可以用pickle,很好用。不懂的见我另一篇文章，神奇的pickle。 4 训练LDA模型这里用的是gensim.models.ldamodel包ldamodel = LdaModel(text, num_topics=10, id2word=dictionary, passes=20)使用这句话就可以直接训练LDA模型了，讲一下参数吧。text:文本，已经表示成词袋了。num_topics: 提取的主题数id2word:词典passes:类似于在机器学习中常见的epoch，也就是训练了多少轮。 然后我们得到了训练好的ldamodel.用这个模型可以做哪些事情呢？ 5 ldamodel使用可以输出这个模型的各个主题下的主题词123456789101112print(ldamodel.print_topics(num_topics=10, num_words=10))###output###[(0, &apos;0.015*&quot;说&quot; + 0.011*&quot;吃&quot; + 0.008*&quot;想&quot; + 0.007*&quot;睡&quot; + 0.005*&quot;\u2005&quot; + 0.005*&quot;做&quot; + 0.005*&quot;明天&quot; + 0.005*&quot;买&quot; + 0.005*&quot;干嘛&quot; + 0.005*&quot;玩&quot;&apos;), (1, &apos;0.017*&quot; &quot; + 0.010*&quot;说&quot; + 0.004*&quot;吃&quot; + 0.004*&quot;许华升&quot; + 0.004*&quot;\x14&quot; + 0.004*&quot;想&quot; + 0.003*&quot;做&quot; + 0.003*&quot;买&quot; + 0.003*&quot;ÿ&quot; + 0.003*&quot;钱&quot;&apos;), (2, &apos;0.008*&quot;com&quot; + 0.007*&quot; &quot; + 0.004*&quot;手机&quot; + 0.003*&quot;女&quot; + 0.003*&quot;说&quot; + 0.003*&quot;www&quot; + 0.003*&quot;cc&quot; + 0.002*&quot;号&quot; + 0.002*&quot;qq&quot; + 0.002*&quot;视频&quot;&apos;), (3, &apos;0.007*&quot;com&quot; + 0.006*&quot; &quot; + 0.005*&quot;38&quot; + 0.004*&quot;号&quot; + 0.004*&quot;贷&quot; + 0.003*&quot;3000&quot; + 0.003*&quot;10&quot; + 0.003*&quot;做&quot; + 0.003*&quot;说&quot; + 0.002*&quot;111&quot;&apos;), (4, &apos;0.017*&quot; &quot; + 0.007*&quot;说&quot; + 0.006*&quot;做&quot; + 0.005*&quot;你好&quot; + 0.005*&quot;吃&quot; + 0.004*&quot;号&quot; + 0.004*&quot;\u2005&quot; + 0.004*&quot;想&quot; + 0.003*&quot;钱&quot; + 0.003*&quot;明天&quot;&apos;), (5, &apos;0.013*&quot; &quot; + 0.012*&quot;说&quot; + 0.007*&quot;吃&quot; + 0.006*&quot;想&quot; + 0.005*&quot;睡&quot; + 0.005*&quot;做&quot; + 0.004*&quot;钱&quot; + 0.004*&quot;买&quot; + 0.004*&quot;回来&quot; + 0.004*&quot;干嘛&quot;&apos;), (6, &apos;0.010*&quot; &quot; + 0.005*&quot;买大单&quot; + 0.005*&quot;贷&quot; + 0.004*&quot;说&quot; + 0.004*&quot;贷款&quot; + 0.004*&quot;钱&quot; + 0.003*&quot;com&quot; + 0.003*&quot;号&quot; + 0.003*&quot;奥特曼&quot; + 0.003*&quot;吃&quot;&apos;), (7, &apos;0.022*&quot; &quot; + 0.010*&quot;说&quot; + 0.008*&quot;做&quot; + 0.007*&quot;吃&quot; + 0.005*&quot;想&quot; + 0.004*&quot;钱&quot; + 0.003*&quot;\u2005&quot; + 0.003*&quot;买&quot; + 0.003*&quot;谢谢&quot; + 0.003*&quot;明天&quot;&apos;), (8, &apos;0.017*&quot; &quot; + 0.015*&quot;com&quot; + 0.006*&quot;www&quot; + 0.005*&quot;说&quot; + 0.004*&quot;号&quot; + 0.004*&quot;\u2005&quot; + 0.003*&quot;手机&quot; + 0.003*&quot;钱&quot; + 0.003*&quot;吃&quot; + 0.003*&quot;https&quot;&apos;), (9, &apos;0.011*&quot;\u2005&quot; + 0.011*&quot;说&quot; + 0.010*&quot;做&quot; + 0.009*&quot; &quot; + 0.004*&quot;钱&quot; + 0.004*&quot;买&quot; + 0.004*&quot;发&quot; + 0.003*&quot;谢谢&quot; + 0.003*&quot;吃&quot; + 0.003*&quot;玩&quot;&apos;)] 这里随便找了些数据，效果不是太明显，这里主要讲处理流程，不要被这结果干扰心情，不过工业应用中很多时候，实际结果和你理想的结果有很大差距。用一些正常的数据，是可以看出一些信息的。上次用汽车之家的评论数据做lda，主题信息就比较明显，有关于油耗的，有关于买车的等等。也可以对新文本，找出其所在的主题分布。1234567891011121314def to_lda_vec(model, dictionary, text_list=None): &apos;&apos;&apos; :param model: lda model :param dictionary: Dictionary for toBow :param text_list: texts :return: texts about one topic &apos;&apos;&apos; lda_list = [] for texts in text_list: doc_bow = dictionary.doc2bow(texts) doc_lda = model[doc_bow] lda_list.append(doc_lda) return lda_list 这个方法中的参数加了注释，这里可以看到有个参数是dictionary，这里就是我们前面训练lda时用的词典，前面保存的词典派上用场了。最后输出的lda_list是一个列表，列表中元素为每句话的doc_lda,doc_lda是这样子的[(5,0.342345),(6,0.1111)…]，也就是个list，无素为元组，元组包括两个值，第一个值表示主题id，第二个值表示属于该主题的概率。 也可以用于新文本数据的向量化，即将新的文本映射成主题向量，然后可以做分类，做聚类，做推荐。]]></content>
      <tags>
        <tag>python</tag>
        <tag>lda</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python之list.count()和Count()类]]></title>
    <url>%2F2018%2F05%2F23%2Fpython%2F</url>
    <content type="text"><![CDATA[Count()类的使用同事给了我一堆文本数据，让我帮个小忙。他想统计下每个词的词频，看看文本中提到最多的是什么，然后做后面分析。 不就是统计词频吗，虽然之前不经常做这个。但是拍脑袋一想，先分词，去停用词，把所有词放到一个列表里，统计，搞定。 于是五分钟写了个代码，计数那个地方，我用的List里的count方法。不怕丢人，我把代码放这了。。。12345678910111213141516171819202122count_word = []stop_words = []with open('data/count.txt', 'r', encoding='utf-8') as f: lines = f.readlines()with open('data/stop_words_new.txt', 'r', encoding='utf-8') as f: stop_lines = f.readlines()for word in stop_lines: stop_words.append(word.strip())for line in lines: words = jieba.lcut(line) for i in words: if i not in stop_words: count_word.append(i)count_word_set = list(set(count_word))counts = []for i in count_word_set: counts.append(count_word.count(i))print(len(count_word_set))print(len(counts))df = pd.DataFrame(data=&#123;'word': count_word_set, 'count': counts&#125;)df.sort_values(by='count')df.to_csv('data/count.txt', sep='\t', encoding='utf-8') 然而，等了一晚上，发现没出来结果。他说数据量不大，几百万条文本。我这里用的set去重，然后遍历set中的每个词，再用list.count(词)来得词频，笨方法行不通。12345678910111213141516171819202122232425262728所以今天试了Count()类。import jiebaimport pandas as pdfrom collections import Counterimport picklecount_word = []count = Counter()stop_words = []with open('data/count.txt', 'r', encoding='utf-8') as f: lines = f.readlines()with open('data/stop_words_new.txt', 'r', encoding='utf-8') as f: stop_lines = f.readlines()print('data finish')for word in stop_lines: stop_words.append(word.strip())for line in lines: words = jieba.lcut(line) for i in words: if i not in stop_words: count_word.append(i)print('cut finish')for word in count_word: count[word] += 1cou = count.most_common()print(cou)with open('data/common.txt', 'wb') as f: pickle.dump(cou, f) 分完词就结束了。。如果只是想看某个词的词频，用list的count()方法还好，如果统计所有的，还是用Count()类吧。]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java异常之java.sql.SQLException Illegal mix of collations (utf8_general_ci]]></title>
    <url>%2F2018%2F05%2F22%2Fjava%2F</url>
    <content type="text"><![CDATA[之前的服务器重装了，所以项目代码重新布置了。明明之前能跑的程序，在我PC机上也是能跑的，放到服务器下就报了这个异常。java.sql.SQLException: Illegal mix of collations (utf8_general_ci,IMPLICIT) and (utf8_general_ci,COERCIBLE) for operation ‘=’跟据异常信息，大致可以理解为是字符集问题。 后来查了下原因，我的数据库使用的mysql建的，字符集为utf-8，数据库排序规则为utf8_unicode_ci.数据库的表是通过hibernate自动生成的，但是少了一个人事部的基本信息表，所以后来把人事表导进来，但是人事表的排序规则用的是unicode_general_ci，正好是异常提示中的内容。 查明了原因，我把数据库删了，因为服务器刚布好，还没投入使用，库内无数据，如果使用过程中出现这种问题，就改那个捣乱的表。当然，系统在使用过程中，也不可能出现这个问题。]]></content>
      <tags>
        <tag>java</tag>
        <tag>异常解决</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python异常之UnicodeEncodeError 'ascii']]></title>
    <url>%2F2018%2F05%2F16%2Fpython1%2F</url>
    <content type="text"><![CDATA[很多时候,你想打印一些数据，想直观的看看结果。可是！你在python中的print()语句报错了。如下：1234Traceback (most recent call last): File &quot;test_aci.py&quot;, line 2, in &lt;module&gt; print(&apos;xt\u7ecf&apos;)UnicodeEncodeError: &apos;ascii&apos; codec can&apos;t encode character &apos;\u7ecf&apos; in position 2: ordinal not in range(128) 你查阅各种博客，知道了是编码问题，可是试了几种方法无法解决。可能在前面加# coding=utf-8,这种思路是对的，但是解决的是sys编码.而你遇到的是print编码问题，就好像你手流血了，你包扎脚。..。。就没有什么好点的方法吗？那我们怎么包扎手呢？在你的py文件合适的位置加上这么一句：如果你不知道在哪，在import 后面加就行。。python2:1234if sys.stdout.encoding != &apos;UTF-8&apos;: sys.stdout = codecs.getwriter(&apos;utf-8&apos;)(sys.stdout, &apos;strict&apos;)if sys.stderr.encoding != &apos;UTF-8&apos;: sys.stderr = codecs.getwriter(&apos;utf-8&apos;)(sys.stderr, &apos;strict&apos;) python3:1234if sys.stdout.encoding != &apos;UTF-8&apos;: sys.stdout = codecs.getwriter(&apos;utf-8&apos;)(sys.stdout.buffer, &apos;strict&apos;)if sys.stderr.encoding != &apos;UTF-8&apos;: sys.stderr = codecs.getwriter(&apos;utf-8&apos;)(sys.stderr.buffer, &apos;strict&apos;) 问题解决，完美！ 可以看出，这是stdout问题，不是加个#coding:utf-8就可以的。这主要是环境配置问题，你当然可以修改环境配置，是个一劳永逸的办法，但很多时候，你拿不到root权限呀，哈哈。 每天一点，进步不难。]]></content>
      <tags>
        <tag>python</tag>
        <tag>异常解决</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas之concat]]></title>
    <url>%2F2018%2F05%2F16%2Fpandas1%2F</url>
    <content type="text"><![CDATA[如果我们有两个dataFrame,该怎么合在一起呢？用concat123456789101112131415161718192021222324252627282930 import pandas as pd inp1 = [&#123;'c1': 10, 'c2': 100&#125;, &#123;'c1': 11, 'c2': 110&#125;, &#123;'c1': 12, 'c2': 120&#125;] inp2 = [&#123;'c1': 20, 'c2': 100&#125;, &#123;'c1': 21, 'c2': 110&#125;, &#123;'c1': 22, 'c2': 120&#125;] df1 = pd.DataFrame(inp1) print(df1) print('-' * 10) df2 = pd.DataFrame(inp2) print(df2) print('-' * 10) df = pd.concat([df1, df2], ignore_index=True) print(df)#output: c1 c20 10 1001 11 1102 12 120---------- c1 c20 20 1001 21 1102 22 120---------- c1 c20 10 1001 11 1102 12 1203 20 1004 21 1105 22 120 每天一点，学习不难。]]></content>
      <tags>
        <tag>pandas</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令之grep]]></title>
    <url>%2F2018%2F05%2F16%2Flinux1%2F</url>
    <content type="text"><![CDATA[今天在跑分类模型，发现了一些比较奇怪的结果，严重影响了分类器性能，所有就想看看这些数据是啥，因为数据是加密的，只能看到id,所以这里就把id取出来，693d0f86a8a90d5a138f75ac7160490c，这里经过md5加密的，然后经过查表解密，得到其明文id,这里假设id为im_12345,然后我们就可以去明文中查找带这个id的数据了。数据放在linux下，我们在终端中用grep &#39;im_12345&#39; xx.txt//xx.txt为要查找的文件名就可以返回我们想看的那条数据了。然后发现，返回了一大堆数据，实在不知道哪条才是于是我们再做一次grep即grep &#39;im_12345&#39; xx.txt|prep &#39;2017-12-12&#39; 总结，grep pattern1 file|grep pattern2也就是在pattern1匹配的基础上再进行一次grep哈哈，每天一点，进步不难。]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras 笔记之Mask层]]></title>
    <url>%2F2018%2F05%2F12%2Fkeras-mask%2F</url>
    <content type="text"><![CDATA[keras 的Mask层先看下官方文档的解释 Masking层keras.layers.core.Masking(mask_value=0.0)使用给定的值对输入的序列信号进行“屏蔽”，用以定位需要跳过的时间步 对于输入张量的时间步，即输入张量的第1维度（维度从0开始算，见例子），如果输入张量在该时间步上都等于mask_value，则该时间步将在模型接下来的所有层（只要支持masking）被跳过（屏蔽）。 如果模型接下来的一些层不支持masking，却接受到masking过的数据，则抛出异常。 例子考虑输入数据x是一个形如(samples,timesteps,features)的张量，现将其送入LSTM层。因为你缺少时间步为3和5的信号，所以你希望将其掩盖。这时候应该： 赋值x[:,3,:] = 0.，x[:,5,:] = 0. 在LSTM层之前插入mask_value=0.的Masking层 model = Sequential()model.add(Masking(mask_value=0., input_shape=(timesteps, features)))model.add(LSTM(32)) 在用LSTM等模型处理文本数据时，因为文本是变长的，所以在处理的过程中，要先进行长度的统一。常用的方法为X_data = sequence.pad_sequence(maxlen=10,value=0,padding=’post’)此步骤将X_data统一长度为10.如[1,2,3,4,5]–&gt;变为[1,2,3,4,5,0,0,0,0,0]这样就可以把X_data 输入到model的Embedding等层。然而，交给LSTM处理时，还有对数据进行反padding.也就是把后面的0去掉。这个时候就是Mask层派上用场的时候了。Mask(0)经过Mask后，可以忽略X_data中所有的0，当然，把后面补的0去掉是可以理解的。那如果句中有0呢？一般情况下，如文本处理，会把文本映射成index，这样最大的好处就是节约空间。有些大文本数据，几百个G，经过了index映射，也就还剩几个G。这是题外话了，我们在keras的Embedding层会讲的。而这个时候index中的0,往往是一些无法转成词向量的低频词，这些词没有词向量，去掉对整个文本的处理也没有影响，所以在Mask中和补上的0一起忽略就好啦。这里的忽略是什么意思呢？也就是不处理。很多朋友以为Mask后会直接把0去掉。其实不是的。可以做一些实验，如model的Mask后接个LSTM层，对LSTM输出每个时间步的值，发现，如果设置了Mask层，则上面[1,2,3,4,5,00000]的数据处理结果，前5位是经过了计算，补0的对应的位置的值，和第5位的值相同，也就是说LSTM对后面补0的位置并没有计算。]]></content>
      <tags>
        <tag>Keras</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow学习笔记3]]></title>
    <url>%2F2018%2F04%2F27%2Ftensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%2F</url>
    <content type="text"><![CDATA[实战,今天给大家举个回归例子，来说明下tensorflow的训练过程。 先贴上代码和注释。 123456789101112131415# 引入包import numpy as npimport tensorflow as tfimport matplotlib.pyplot as plt#准备好数据，x为生成-1到1之间的100个数，y为2*x+1+噪声data_x = np.linspace(-1, 1, 100)data_y = np.multiply(data_x, 2) + 1 + np.random.uniform(-0.5, 0.5, 100)#画图，将x,y的数据投上去fig = plt.figure()ax = fig.add_subplot(1,1,1)ax.scatter(data_x, data_y)plt.ion()plt.show() 这里看下图吧这里的点点就是我们的数据，大概按一条斜线分布，所以我们找到一条回归线来拟合这些数据。 1234567891011121314151617181920212223242526272829303132333435363738394041#准备好placeholder,占位符，也叫容器，也就是个坑，用来放数据的x_placeholder = tf.placeholder(tf.float32, name='X')y_placeholder = tf.placeholder(tf.float32, name='Y')#变量，W = tf.Variable(dtype=tf.float32,name='W',initial_value=tf.random_normal([1]))b = tf.Variable(dtype=tf.float32,name='W',initial_value=tf.random_normal([1]))#y_pred 给出回归式的Opy_pred = tf.add(tf.multiply(x_placeholder,W),b)#求loss,给出优化器loss = tf.reduce_mean(tf.square(y_placeholder-y_pred,name='loss'))optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)#初始化变量init = tf.global_variables_initializer()#session会话with tf.Session() as sess: sess.run(init) #迭代100次 for i in range(100): total_loss = 0 for x, y in zip(data_x,data_y ): #将之前graph中的loss和optimizer跑起来，训练过程就在这，因为有个minimize(loss)的操作 _, los = sess.run([optimizer, loss], feed_dict=&#123;x_placeholder: x, y_placeholder: y&#125;) total_loss += los #求预测值 yp = sess.run(y_pred,feed_dict=&#123;x_placeholder:data_x, y_placeholder: data_y&#125;) if i % 5 == 0: #每5步画一次线 lines = ax.plot(data_x, yp, 'r-', lw=5) print('Epoch &#123;0&#125;: &#123;1&#125;'.format(i, total_loss / 100)) plt.pause(0.1) try: #为了不让线一直赖着不走，我们每画完就要删除。 ax.lines.pop(0) except Exception: print('error') 这里总结一下。本例解决的是一个线性回归问题，用tensorflow仍先画图，再运行。当然，还有准备数据。其中画图部分包括：占位符定义，变量（权重）定义，运算式定义，loss定义，再加上一个让Loss减小的优化器运行部分包括：将数据喂给相应的op，并run起来，画图操作是为了让大家更直观。]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow学习笔记2]]></title>
    <url>%2F2018%2F04%2F26%2Ftensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%2F</url>
    <content type="text"><![CDATA[上期我们说到tensorflow分两步，Graph和run。在公司用tensorflow搭了下lstm做分类的模型，结果很奇怪。同样的模型结构，我用Keras跑出来auc为0.7,而用tensorflow时loss一直跳，而且auc基本在0.5左右。然后在排错过程中，我想到了graph，把graph打出来后，发现了几个问题，如最后一层忘记加sigmoid激活。 尴尬！！！不过反映了tensorflow graph的一个好处，可以让我们像看图一样检查模型。。。 今天主要讲下常量，变量和占位符。 常量12345678a = tf.contant([3,6])#表示生成一个长度为2的向量a = [3,6]，此时a是一个op，在sess.run之后才有值#tf.zeros()#tf.ones()#都为常量######------------------#tf.random_normal(shape=,dtype=,name=&apos;&apos;)#tf.random_uniform()#以上也是常量，只不过是随机常量 变量123456789101112b = tf.Variable(2,name=&apos;scalar&apos;)#生成一个变量，并为该变量赋初值为2，仍在sess.run()后才有值为2。#变量在使用之前要初始化！！！初始化变量有三种方式#1）全部初始化init = tf.global_variables_initializer()sess.run(init)#2)初始化部分变量init_ab = tf.variables_initializer([a,b],name =&apos;init_ab&apos;)#3)初始化单个变量sess.run(b.initializer)#当然，此时b被初始化后是个Tensor,print(b) #Tensor(&apos;Variable/....&apos;)print(b.eval()) #[[0.444,0.111,....],...,[...]],使用eval()输出变量内容a 记得常量为一个op,而Variable()是一个类占位符placeholder() a = tf.placeholder(dtype,shape=,name=&#39;&#39;)占位符就是个容器，先占个坑，里面什么都没有，在run的时候，可以通过 sess.run([a,b],feed_dict={a:a_data,b:b_data})来将值放到之前占的坑里。 一般我们用占位符来存放用于训练的数据，当然我们可以直接用数据来构建op，但是有点外行了。。用placeholder的方式，可以让我们数据送入的更自由，可以按自己需要的batch来送入。简单总结一下，常量constant()用来构建一些在模型中不需要改变的量，变量Variable()一般用来构建权重等，需要不断更新，占位符placeholder一般用来作数据的容器。]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow学习笔记1]]></title>
    <url>%2F2018%2F04%2F26%2Ftensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%2F</url>
    <content type="text"><![CDATA[明天要去实习了，心里有些激动，有些紧张，还有些舍不得，舍不得学校的样子，和某个人。 进入正题。 tensorflow是谷歌开源的一款深度学习框架，我用的是python，这个框架是目前最火的框架之一，很有学习的必要。 首先tensorflow，tensor + flow，tensor –&gt; 张量，flow –&gt;流动, 也就是张量在图里流动。这里讲两点，张量就是，0维的叫标量（数字），1维的叫向量，2维的叫矩阵，那3+维只能叫张量了;而在图里流动，这里的图（graph）是tensorflow的重要思想之一，tensorflow把运算结构画成graph，其中有节点op，有边，边代表张量的流动，节点代表运算。 在tensorflow代码中，分为两大块，第一块用来画图，即把所有的运算画在一张图上，并没有实际运算；第二块用来运算，Session().这样做看似很麻烦，其实有个好处，我们看到了图之后，可以有选择性的运算某些节点，而一些没必要的节点可以不运算，节约了大量的时间。如下面的代码：123456789101112131415x=2y=3add_op=tf.add(x,y)mul_op=tf.mul(x,y)useless=tf.mul(x,add_op)pow_op=tf.pow(add_op,mul_op)with tf.Session() as sess: z=sess.run(pow_op) 可以看到如果只run(pow_op),那么useless就不会被运算。原因是，当我们run时，会从图中找该op所依赖的op，而pow_op没有依赖useless,所以就没有执行啦。记得查看graph的时候，可不是像打开jpg一样哟。。要cd到graph存放的目录下，Tensorboard --logdir=&#39;目录 &#39;才行。待续。。。]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[找实习]]></title>
    <url>%2F2018%2F04%2F26%2F%E6%89%BE%E5%AE%9E%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[说件开心的事，我种的海棠花开了。 再说件开心的事，讯飞实习录用了，上午接到的电话，下周二入职。 这个实习等好久了，前段时间看到大家都在找实习，心里发慌，于是自己也厚着脸皮找朋友内推。蚂蚁金服的一面，就给我拒了，心里的酸，每天早上天没亮就往外反，搞得每天都睡不好觉，感觉自己找不到工作了。可能从来没找过工作，没接受过这种刺激，那几天还在调整心情，也感激下我的小女友，一直给我鼓励。后来一边投实习，一边总结经验，感觉没有实践经验是很大的减分项，所以又找朋友帮我推了下讯飞的实习，这个不是暑期实习，所以面试相对简单些，结束后朋友告诉我主管对我印象还不错，应该通过了。可是，人事部一直没给通知。总担心那边出什么意外情况，这两天脑海里上演了好多大戏，也被自己的戏给吓到过。不过还好，终于还是给了通知，虽然有点晚。 晚上，本来想去科大参加腾讯的宣讲，谁曾想临时改了场地，在另一个校区，干脆不去了，坐下来老老实实的做携程的笔试题，前面的选择和问答题还好，选择题基本都是基础知识，考点也比较细。问答有个基因的条件概率题，大概做出来了，还一道讲一下深度学习中，relu比sigmoid和tanh好在什么地方。最后一个题，编程，没看懂题，这里记录一下。最长路径问题。 现有ABCDE五个字符，以及M个字符串，每个字符串由这五个字符和1-9的整数间隔组成，如：A2B3D，表示存在A-&gt;B B-&gt;D的路径，且路径长为2和3，可以推出A-&gt;D的一条路径长为5.求最长的一条路径的长度，如果任何一处路径出现环（如A-&gt;…-&gt;A的路径），则返回-1.输入： 第一行 为字符串的个数M 第二行 开始为M个字符串输出： 最长的一条路径的长度，如果出现环，返回-1如输入 4 A2B3D 、 A4C2E 、 A5D、 C3B输出 10 刚开始理解错了，所以信心满满写了20分钟代码，最后测试没通过。等读懂题，黄花菜早结冰了。。 这些笔试编程题都比较奇葩，多做做应该会有不小进步。]]></content>
      <categories>
        <category>心情</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>心情</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初来博客]]></title>
    <url>%2F2018%2F04%2F26%2F%E5%88%9D%E6%9D%A5%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[我的第一篇博客，也不知道写点啥。 研究生生涯转眼间过去了大半，曾经的舒适区已经渐渐被现实挤得越来越小。细细想来，过去的这段时间，我究竟在忙些什么？想了很久，才知道答案：瞎忙。 在本科时候，自己学习了JAVA EE开发，也做过一些小网站，本来要去找工作，可是拿到了保研名额，所以想在舒适区里再窝几年，那个时候的我，虽然怂，不敢面对陌生的人生，可是却有着不可一世的傲气，因为我是专业前三，因为我是安徽省优秀毕业生，因为我是学霸，其实说到底，因为我顶着光环，或者说我意想出来的光环。 读研，进研究所，接触了机器学习和自然语言处理，发现自己原来会的JAVA开发只是皮毛中的皮毛，我自诩精通技术，却对Machine Learning和NLP一无所知，于是我开始变得谦逊，因为实在没有底气骄傲，尴尬，哈哈。。。 现在，研二生活也渐渐近尾声，常常有种恐惧感，就像风筝要飞了，手中的线越来越少，你看到了，却无能为力。还好，有篇论文已经写出来了，在投，在等结果；Machine Learning 和 NLP也算入门（不敢吹牛），一些算法也做过推导和代码编写。从开始不懂LR（逻辑回归），到现在懂了一点，也算进步吧。哈哈，开玩笑，懂了不只一点，比一点多一些。 那压力在哪呢？在找工作。 前两天投了几个实习，结果不太乐观。面试知识点比较细，虽然基本都能答上来，但总是给人一种不太精通的感觉。所以最近也在恶补数学原理和代码实现，毕竟这是理科和工科的结合嘛。说到这，不禁有点疑问，为什么非要做人工智能相关呢？明明不会，何苦为难。我也说不清，只是感觉这个有挑战有乐趣，工作不就应该这些么？比尔说过一句话：人们往往高估自己一年内能做的事，却又低估自己十年内能做的事。可能我现在还有半只脚在门外，不过认定一个目标往前走，总有踏进去的一天吧。 回想自己这两年研，AI坚持在学，毕竟想吃这碗饭；也在坚持健身，常常弹琴；有人爱，有事做，有所期待。 我在瞎忙？好像不是。]]></content>
      <tags>
        <tag>心情</tag>
      </tags>
  </entry>
</search>
