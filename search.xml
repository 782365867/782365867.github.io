<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>echart的series中label formatter回调函数使用及样式。</title>
      <link href="/2018/08/03/echart/"/>
      <url>/2018/08/03/echart/</url>
      <content type="html"><![CDATA[<p>今天朋友写论文用到Echart做图，但是个性化的需要不知怎么做，找我帮忙。我很久没看前端的东西了，不过Echart那玩意好像很有意思，所以就接了下来。</p><p>先看需求。<img src="https://i.imgur.com/geuR6aA.png" alt=""><br>1 将紫色框部分改成自己的内容<br>2 将红色框内容去掉<br>3 将蓝色框内容改成自己定义的类别，（三种）</p><p>于是看了下代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line">app.title = <span class="string">'嵌套环形图'</span>;</span><br><span class="line"></span><br><span class="line">option = &#123;</span><br><span class="line">    tooltip: &#123;</span><br><span class="line">        trigger: <span class="string">'item'</span>,</span><br><span class="line">        formatter: <span class="string">"&#123;a&#125; &lt;br/&gt;&#123;b&#125;: &#123;c&#125; (&#123;d&#125;%)"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    legend: &#123;</span><br><span class="line">        orient: <span class="string">'vertical'</span>,</span><br><span class="line">        x: <span class="string">'left'</span>,</span><br><span class="line">        data:[<span class="string">'直达'</span>,<span class="string">'营销广告'</span>,<span class="string">'搜索引擎'</span>,<span class="string">'邮件营销'</span>,<span class="string">'联盟广告'</span>,<span class="string">'视频广告'</span>,<span class="string">'百度'</span>,<span class="string">'谷歌'</span>,<span class="string">'必应'</span>,<span class="string">'其他'</span>]</span><br><span class="line">    &#125;,</span><br><span class="line">    series: [</span><br><span class="line">        &#123;</span><br><span class="line">            name:<span class="string">'访问来源'</span>,</span><br><span class="line">            type:<span class="string">'pie'</span>,</span><br><span class="line">            selectedMode: <span class="string">'single'</span>,</span><br><span class="line">            radius: [<span class="number">0</span>, <span class="string">'30%'</span>],</span><br><span class="line"></span><br><span class="line">            label: &#123;</span><br><span class="line">                normal: &#123;</span><br><span class="line">                    position: <span class="string">'inner'</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            labelLine: &#123;</span><br><span class="line">                normal: &#123;</span><br><span class="line">                    show: <span class="literal">false</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            data:[</span><br><span class="line">                &#123;<span class="attr">value</span>:<span class="number">335</span>, <span class="attr">name</span>:<span class="string">'直达'</span>, <span class="attr">selected</span>:<span class="literal">true</span>&#125;,</span><br><span class="line">                &#123;<span class="attr">value</span>:<span class="number">679</span>, <span class="attr">name</span>:<span class="string">'营销广告'</span>&#125;,</span><br><span class="line">                &#123;<span class="attr">value</span>:<span class="number">1548</span>, <span class="attr">name</span>:<span class="string">'搜索引擎'</span>&#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            name:<span class="string">'访问来源'</span>,</span><br><span class="line">            type:<span class="string">'pie'</span>,</span><br><span class="line">            radius: [<span class="string">'40%'</span>, <span class="string">'55%'</span>],</span><br><span class="line">            label: &#123;</span><br><span class="line">                normal: &#123;</span><br><span class="line">                    formatter: <span class="string">'&#123;a|&#123;a&#125;&#125;&#123;abg|&#125;\n&#123;hr|&#125;\n  &#123;b|&#123;b&#125;：&#125;&#123;c&#125;  &#123;per|&#123;d&#125;%&#125;  '</span>,</span><br><span class="line">                    backgroundColor: <span class="string">'#eee'</span>,</span><br><span class="line">                    borderColor: <span class="string">'#aaa'</span>,</span><br><span class="line">                    borderWidth: <span class="number">1</span>,</span><br><span class="line">                    borderRadius: <span class="number">4</span>,</span><br><span class="line">                    <span class="comment">// shadowBlur:3,</span></span><br><span class="line">                    <span class="comment">// shadowOffsetX: 2,</span></span><br><span class="line">                    <span class="comment">// shadowOffsetY: 2,</span></span><br><span class="line">                    <span class="comment">// shadowColor: '#999',</span></span><br><span class="line">                    <span class="comment">// padding: [0, 7],</span></span><br><span class="line">                    rich: &#123;</span><br><span class="line">                        a: &#123;</span><br><span class="line">                            color: <span class="string">'#999'</span>,</span><br><span class="line">                            lineHeight: <span class="number">22</span>,</span><br><span class="line">                            align: <span class="string">'center'</span></span><br><span class="line">                        &#125;,</span><br><span class="line">                        <span class="comment">// abg: &#123;</span></span><br><span class="line">                        <span class="comment">//     backgroundColor: '#333',</span></span><br><span class="line">                        <span class="comment">//     width: '100%',</span></span><br><span class="line">                        <span class="comment">//     align: 'right',</span></span><br><span class="line">                        <span class="comment">//     height: 22,</span></span><br><span class="line">                        <span class="comment">//     borderRadius: [4, 4, 0, 0]</span></span><br><span class="line">                        <span class="comment">// &#125;,</span></span><br><span class="line">                        hr: &#123;</span><br><span class="line">                            borderColor: <span class="string">'#aaa'</span>,</span><br><span class="line">                            width: <span class="string">'100%'</span>,</span><br><span class="line">                            borderWidth: <span class="number">0.5</span>,</span><br><span class="line">                            height: <span class="number">0</span></span><br><span class="line">                        &#125;,</span><br><span class="line">                        b: &#123;</span><br><span class="line">                            fontSize: <span class="number">16</span>,</span><br><span class="line">                            lineHeight: <span class="number">33</span></span><br><span class="line">                        &#125;,</span><br><span class="line">                        per: &#123;</span><br><span class="line">                            color: <span class="string">'#eee'</span>,</span><br><span class="line">                            backgroundColor: <span class="string">'#334455'</span>,</span><br><span class="line">                            padding: [<span class="number">2</span>, <span class="number">4</span>],</span><br><span class="line">                            borderRadius: <span class="number">2</span></span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            data:[</span><br><span class="line">                &#123;<span class="attr">value</span>:<span class="number">335</span>, <span class="attr">name</span>:<span class="string">'直达'</span>&#125;,</span><br><span class="line">                &#123;<span class="attr">value</span>:<span class="number">310</span>, <span class="attr">name</span>:<span class="string">'邮件营销'</span>&#125;,</span><br><span class="line">                &#123;<span class="attr">value</span>:<span class="number">234</span>, <span class="attr">name</span>:<span class="string">'联盟广告'</span>&#125;,</span><br><span class="line">                &#123;<span class="attr">value</span>:<span class="number">135</span>, <span class="attr">name</span>:<span class="string">'视频广告'</span>&#125;,</span><br><span class="line">                &#123;<span class="attr">value</span>:<span class="number">1048</span>, <span class="attr">name</span>:<span class="string">'百度'</span>&#125;,</span><br><span class="line">                &#123;<span class="attr">value</span>:<span class="number">251</span>, <span class="attr">name</span>:<span class="string">'谷歌'</span>&#125;,</span><br><span class="line">                &#123;<span class="attr">value</span>:<span class="number">147</span>, <span class="attr">name</span>:<span class="string">'必应'</span>&#125;,</span><br><span class="line">                &#123;<span class="attr">value</span>:<span class="number">102</span>, <span class="attr">name</span>:<span class="string">'其他'</span>&#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>经过注释调试，发现需求1只要改最后一个data集合就好，把name和value换成自己自定义的内容。<br>而需求2 3 主要在<figure class="highlight plain"><figcaption><span>'&#123;a|&#123;a&#125;&#125;&#123;abg|&#125;\n&#123;hr|&#125;\n  &#123;b|&#123;b&#125;：&#125;&#123;c&#125;  &#123;per|&#123;d&#125;%&#125;  '```这里。把、`&#123;c&#125;`去掉即可。</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">最难搞的是需求3，用了2小时搞明白的。</span><br><span class="line">这里是官方文档里的介绍</span><br><span class="line">http://echarts.baidu.com/option.html#grid.tooltip.formatter</span><br><span class="line">formatter有两种写法：一种字符串模板，另一种是回调函数。</span><br><span class="line">字符串模型里写的是，我们要改的地方是该series中name的值，这里name只能有一个值，改name是不能了，所以我们可以像改需求1那样，在data中传一个变量。用回调函数，文档中对回调函数也没有写明白。这里用简单的语言再梳理一下。主要有两点：将需要的个性化信息显示出来，然后展现出好看的样式。</span><br><span class="line">第一点：</span><br><span class="line">我们的个性化需求在data里写</span><br></pre></td></tr></table></figure></p><p>data:[<br>    {value:335, name:’直达’,flag:’type_1’},<br>    {value:310, name:’邮件营销’,flag:’type_1’},<br>    {value:234, name:’联盟广告’,flag:’type_1’},<br>    {value:135, name:’视频广告’,flag:’type_2’},<br>    {value:1048, name:’百度’,flag:’type_2’},<br>    {value:251, name:’谷歌’,flag:’type_2’},<br>    {value:147, name:’必应’,flag:’type_3’},<br>    {value:102, name:’其他’,flag:’type_3’}<br>]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">也就是把我们需求3的内容```type_1,type_2,type_3```写在变量flag中，然后通过回调函数取出来，这里谁回调的先不管。</span><br><span class="line">回调函数的写法：</span><br><span class="line">```js</span><br><span class="line"> formatter:function(params)&#123; </span><br><span class="line">str =  params.data.flag+ params.name+params.percent</span><br><span class="line">return str</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure></p><p>这里params就是我们的data，一条一条往里传。<br>看效果：<img src="https://i.imgur.com/RgNFyDC.png" alt=""><br>可以看到，内容改过了，就是丑了点。我们想改成原来的样子怎么办呢？这里正是官方文档中最让我头疼的，找了半天没找到，在网上也不找到解释的比较好的。<br>我们套用原有样式，只需要把我们的参数<code>params.name</code>这种替换到原来<code>{a}</code>这种的位置，我们的参数和其他内容用<code>+</code>连接。如下：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> formatter:<span class="function"><span class="keyword">function</span>(<span class="params">params</span>)</span>&#123; </span><br><span class="line">str = <span class="string">'&#123;a|'</span>+ params.data.flag + <span class="string">'&#125;&#123;abg|&#125;\n&#123;hr|&#125;\n  &#123;b|'</span>+params.name+<span class="string">'：&#125; &#123;per|'</span>+params.percent + <span class="string">'%&#125;'</span></span><br><span class="line"><span class="keyword">return</span> str</span><br><span class="line">&#125;,</span><br><span class="line"><span class="comment">// &#123;a| 这种表示a样式，而&#123;a&#125;这种有固定的含义，&#123;a&#125;（系列名称），&#123;b&#125;（数据项名称），&#123;c&#125;（数值）, &#123;d&#125;（百分比），样式里有条竖杠！记得区分。</span></span><br></pre></td></tr></table></figure></p><p>最终效果：<img src="https://i.imgur.com/eePZLaJ.png" alt=""></p>]]></content>
      
      
        <tags>
            
            <tag> echart </tag>
            
            <tag> 可视化 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>安装zookeeper时候，可以查看进程启动，但是状态显示报错 Error contacting service. It is probably not running</title>
      <link href="/2018/07/12/zookeeper/"/>
      <url>/2018/07/12/zookeeper/</url>
      <content type="html"><![CDATA[<p>最近想学一下大数据，这年头，不会大数据，都不敢说自己是21世纪新青年。</p><p>大数据有很多要学的框架，比如hadoop spark storm zookeeper等等，今天拿zookeeper入个门，安装过程中遇到点坑，分享一下。</p><p>Zookeeper是一个分布式<strong>协调</strong>服务；就是为用户的分布式应用程序提供协调服务<br>A、zookeeper是为别的分布式程序服务的<br>B、Zookeeper本身就是一个分布式程序（只要有半数以上节点存活，zk就能正常服务）<br>C、Zookeeper所提供的服务涵盖：<strong>主从协调</strong>、<strong>服务器节点动态上下线</strong>、统一配置管理、分布式共享锁、统一名称服务……<br>D、虽然说可以提供各种服务，但是zookeeper在底层其实只提供了两个功能：<br> —管理(存储，读取)用户程序提交的<strong>数据</strong>；<br> —并为用户程序提供数据节点<strong>监听</strong>服务；</p><p>1 部署<br>用的是三台虚拟机，centOS 7<br>jdk 8<br>zookeeper3.4.5<br>三台机器网络己配好，host配置如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@shizhan1 conf]# cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">127.0.0.1 shizhan1</span><br><span class="line">192.168.33.3 shizhan1</span><br><span class="line">192.168.33.4 shizhan2</span><br><span class="line">192.168.33.5 shizhan3</span><br></pre></td></tr></table></figure></p><p>2 安装<br><a href="https://pan.baidu.com/s/1O-E6pq3zbzoqRvxHoj66eQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1O-E6pq3zbzoqRvxHoj66eQ</a> 放我网盘了，zookeeper3.4.5</p><p>下载到shizhan1（第一台机子）中，我放在了<code>/root/work/</code>下<br>解压：<br><code>tar -zxvf zookeeper-3.4.5.tar.gz</code><br>重命名：<br><code>mv zookeeper-3.4.5 zookeeper</code>（重命名文件夹zookeeper-3.4.5为zookeeper,太长 写起来好麻烦）</p><p>3 环境变量：<br><code>vi /etc/profile</code><br>添加内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export ZOOKEEPER_HOME=/root/work/zookeeper</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br></pre></td></tr></table></figure></p><p>source 一下，也就是把你写的环境变量激活一下<br><code>source /etc/profile</code></p><p>注意，这里三台机子都要改环境变量哟，第二部安装可以只在一个机子上装好，然后拷到其他机子上去，但是环境变量三个都要改。</p><p>4 配置文件<br><code>cd zookeeper/conf</code>进入conf目录<br><code>cp zoo_sample.cfg zoo.cfg</code> 把例子copy一个出来，命名为zoo.cfg,我们要改的就是这个。<br><code>vi zoo.cfg</code><br>添加内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/root/zkdata （要创建文件夹）</span><br><span class="line">dataLogDir=/root/zklogdata （要创建文件夹）</span><br><span class="line">server.1=shizhan1:2888:3888 (主机名, 心跳端口、数据端口)</span><br><span class="line">server.2=shizhan2:2888:3888</span><br><span class="line">server.3=shizhan3:2888:3888</span><br></pre></td></tr></table></figure></p><p>5 然后创建上面说的那俩文件夹,<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /root/zkdata</span><br><span class="line">mkdir /root/zklogdata</span><br></pre></td></tr></table></figure></p><p>注意，这里三台机子都要创建。<br>6 将配好的zookeeper拷到另外两台机子上<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /root/work/zookeeper hadoop@slave2:/home/hadoop/</span><br><span class="line">scp -r /root/work/zookeeper hadoop@slave3:/home/hadoop/</span><br></pre></td></tr></table></figure></p><p>7 在/root/zkdata/中创建myid文件，注意每台机子都要创建，但是值不一样，在步骤4中,如第一台，<code>server.1=shizhan1:2888:3888</code>，shizhan1是主机名，前面的serverl.1中的1,就是shizhan1这台机子上myid文件的值。以这台机子为例，其他两台的值分别为2 和 3。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /root/zkdata/</span><br><span class="line">echo 1 &gt; myid</span><br></pre></td></tr></table></figure></p><p>8 启动服务<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@shizhan1 bin]# zkServer.sh start</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /root/work/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure></p><p>9 查看服务<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@shizhan2 conf]# zkServer.sh status</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /root/work/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure></p><p>只有出现上面类似内容才是成功了。</p><p>排坑–<br>今天启动后，发现jps里有quorumpeermain<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Starting zookeeper ... already running as process 1398.</span><br><span class="line">[root@shizhan2 ~]# jps</span><br><span class="line">1398 QuorumPeerMain</span><br><span class="line">1559 Jps</span><br></pre></td></tr></table></figure></p><p>但是状态显示报错：Error contacting service. It is probably not running</p><p>1）在网上查阅资料一共有几种解决方法：<br>打开zkServer.sh 找到status)</p><p>STAT=<code>echo stat | nc localhost $(grep clientPort &quot;$ZOOCFG&quot; | sed -e &#39;s/.*=//&#39;) 2&gt; /dev/null| grep Mode</code><br>在nc与localhost之间加上 -q 1 （是数字1而不是字母l）<br>如果已存在则去掉<br>注:因为我用的zookeeper是3.4.5版本，所以在我的zkServer.sh脚本文件里根本没有这一行,所以没有生效</p><p>2）调用sh zkServer.sh status 遇到这个问题。百度，google了后发现有人是修改sh脚本里的一个nc的参数来解决，可在3.4.5的sh文件里并没有找到nc的调用。配置文档里指定的log目录没有创建导致出错，手动增加目录后重启，问题解决。<br>注:我想不是日志的问题所以这个方法根本就没有试</p><p>3）创建数据目录,也就是在你zoo.cfg配置文件里dataDir指定的那个目录下创建myid文件,并且指定id,改id为你zoo.cfg文件中<code>server.1=shizhan1:2888:3888</code>中的 1.只要在myid头部写入1即可.</p><p>4）因为防火墙没有关闭。关闭防火墙：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#查看防火墙状态</span><br><span class="line"></span><br><span class="line"> service iptables status </span><br><span class="line"></span><br><span class="line">#关闭防火墙</span><br><span class="line"> service iptables stop</span><br><span class="line">#查看防火墙开机启动状态</span><br><span class="line"> chkconfig iptables --list</span><br><span class="line">#关闭防火墙开机启动</span><br><span class="line"> chkconfig iptables off</span><br></pre></td></tr></table></figure></p><p> 注意：我的确在开始时候没有关闭防火墙，但是当我关闭防火墙之后也没有解决问题。</p><p>如果是centOS 7，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@shizhan2 conf]# systemctl stop firewalld.service</span><br><span class="line">[root@shizhan2 conf]# systemctl disable firewalld.service</span><br></pre></td></tr></table></figure></p><p>我就是关防火墙的时候，用了iptables 那个命令，提示中显示如下，我就大概一看，以为是说防火墙没开，其实人家早就换新命令啦。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@shizhan1 work]# service iptables stop</span><br><span class="line">Redirecting to /bin/systemctl stop iptables.service</span><br><span class="line">Failed to stop iptables.service: Unit iptables.service not loaded.</span><br></pre></td></tr></table></figure></p><p>5） 没有建立主机和ip之间的映射关系。</p><p>  建立主机和ip之间映射关系的命令为 vim /etc/hosts   在文件的末端加入各个主机和ip地址之间的映射关系就可以了。</p><p>  注意：只有在建立了映射关系之后，才可以将在同一个网段下的机器利用主机名进行文件传递。问题解决！</p>]]></content>
      
      
        <tags>
            
            <tag> 异常解决 </tag>
            
            <tag> zookeeper </tag>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ERROR 1130 (HY000) Host &#39;114.213.234.22&#39; is not allowed to connect to this MySQL server</title>
      <link href="/2018/07/05/mysql/"/>
      <url>/2018/07/05/mysql/</url>
      <content type="html"><![CDATA[<p>给学校一个部门写了个网站，这个网站要用到学校的人事信息表，但是人事信息经常变动，所以跟学校人事部门联系下，每天将新的人事表推送到我们的数据库中。</p><p>然后人事部门发来个截图，说连接不上。<br><img src="https://i.imgur.com/VZD4tkX.png" alt=""></p><p>大概感觉是权限问题，查了些资料，原来mysql的安全机制，默认只允许本地连接。<br>于是进入数据库中，改了下配置。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">use mysql;</span><br><span class="line">update user set host = &apos;%&apos; where user = &apos;root&apos;;</span><br><span class="line">flush privileges;</span><br><span class="line">quit</span><br></pre></td></tr></table></figure></p><p>把 host 设为 % 就是解除了本地主机访问的限制。</p>]]></content>
      
      
        <tags>
            
            <tag> 异常解决 </tag>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>揭开迷雾，来一顿美味的Capsule盛宴</title>
      <link href="/2018/06/22/capsnet/"/>
      <url>/2018/06/22/capsnet/</url>
      <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>由深度学习先驱 Hinton 开源的 Capsule 论文《Dynamic Routing Between Capsules》，无疑是去年深度学习界最热点的消息之一。得益于各种媒体的各种吹捧，Capsule 被冠以了各种神秘的色彩，诸如“抛弃了梯度下降”、“推倒深度学习重来”等字眼层出不穷，但也有人觉得 Capsule 不外乎是一个新的炒作概念。</p><p>本文试图揭开让人迷惘的云雾，领悟 Capsule 背后的原理和魅力，品尝这一顿 Capsule 盛宴。同时，笔者补做了一个自己设计的实验，这个实验能比原论文的实验更有力说明 Capsule 的确产生效果了。</p><h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h1><p>Capsule 的论文已经放出几个月了，网上已经有很多大佬进行解读，也有大佬开源实现了 CapsuleNet，这些内容都加速了我对 Capsule 的理解。然而，我觉得美中不足的是，网上多数的解读，都只是在论文的翻译上粉饰了一点文字，并没有对 Capsule 的原理进行解读。比如“动态路由”那部分，基本上就是照搬论文的算法，然后说一下迭代 3 次就收敛了。但收敛出什么来？论文没有说，解读也没有说，这显然是不能让人满意的。也难怪知乎上有读者评论说：</p><blockquote><p>所谓的 capsule 为 dl 又贡献了一个花里胡哨的 trick 概念。说它是 trick，因为 hinton 没有说为什么 routing 算法为什么需要那么几步，循环套着循环，有什么理论依据吗？还是就是凑出来的？<br>如何评价深度学习之父 Hinton 发布的 Capsule 论文？ - 知乎</p></blockquote><p>这个评论虽然过激，然而也是很中肯的：凭啥 Hinton 摆出来一套算法又不解释，我们就要稀里糊涂的跟着玩？</p><h1 id="2-Capsule-盛宴"><a href="#2-Capsule-盛宴" class="headerlink" title="2 Capsule 盛宴"></a>2 Capsule 盛宴</h1><h2 id="2-1-宴会特色"><a href="#2-1-宴会特色" class="headerlink" title="2.1 宴会特色"></a>2.1 宴会特色</h2><p>这次 Capsule 盛宴的特色是“vector in vector out”，取代了以往的“scaler in scaler out”，也就是神经元的输入输出都变成了向量，从而算是对神经网络理论的一次革命。然而真的是这样子吗？难道我们以往就没有做过“vector in vector out”的任务了吗？有，而且多的是！NLP 中，一个词向量序列的输入，不就可以看成“vector in”了吗？这个词向量序列经过 RNN/CNN/Attention 的编码，输出一个新序列，不就是“vector out”了吗？在目前的深度学习中，从来不缺乏“vector in vector out”的案例，因此显然这不能算是 Capsule 的革命。</p><p>Capsule 的革命在于：它提出了一种新的“vector in vector out”的传递方案，并且这种方案在很大程度上是可解释的。</p><p>如果问深度学习（神经网络）为什么有效，我一般会这样回答：神经网络通过层层叠加完成了对输入的层层抽象，这个过程某种程度上模拟了人的层次分类做法，从而完成对最终目标的输出，并且具有比较好的泛化能力。的确，神经网络应该是这样做的，然而它并不能告诉我们它确确实实是这样做的，这就是神经网络的难解释性，也就是很多人会将深度学习视为黑箱的原因之一。</p><p>让我们来看 Hinton 是怎么来通过 Capsule 突破这一点的。</p><h2 id="2-2-大盆菜"><a href="#2-2-大盆菜" class="headerlink" title="2.2 大盆菜"></a>2.2 大盆菜</h2><p>如果要用一道菜来比如 Capsule，我想到了“大盆菜”：</p><blockquote><p>盆菜作为客家菜的菜式出现由来以久，一般也称为大盘菜，大盘菜源于客家人传统的“发财大盘菜”，顾名思义就是用一个大大的盘子，将食物都放到里面，融汇出一种特有滋味。丰富的材料一层层叠进大盘之中，最易吸收肴汁的材料通常放在下面。吃的时候每桌一盘，一层一层吃下去，汁液交融，味道馥郁而香浓，令人大有渐入佳景之快。</p></blockquote><p>Capsule 就是针对着这个“层层递进”的目标来设计的，但坦白说，Capsule 论文的文笔真的不敢恭维，因此本文尽量不与论文中的符号相同，以免读者再次云里雾里。让我们来看个图。<br><img src="https://i.imgur.com/Y8T9FED.png" alt=""><br>如图所示，底层的胶囊和高层的胶囊构成一些连接关系。等等，什么是“胶囊”？其实，只要把一个向量当作一个整体来看，它就是一个“胶囊”，是的，你没看错，你可以这样理解：神经元就是标量，胶囊就是向量，就这么粗暴！Hinton 的理解是：每一个胶囊表示一个属性，而胶囊的向量则表示这个属性的“标架”。也就是说，我们以前只是用一个标量表示有没有这个特征（比如有没有羽毛），现在我们用一个向量来表示，不仅仅表示有没有，还表示“有什么样的”（比如有什么颜色、什么纹理的羽毛），如果这样理解，就是说在对单个特征的表达上更丰富了。</p><p>说到这里，我感觉有点像 NLP 中的词向量，以前我们只是用 one hot 来表示一个词，也就是表示有没有这个词而已。现在我们用词向量来表示一个词，显然词向量表达的特征更丰富，不仅可以表示有没有，还可以表示哪些词有相近含义。词向量就是 NLP 中的“胶囊”？这个类比可能有点牵强，但我觉得意思已经对了。</p><p>那么，这些胶囊要怎么运算，才能体现出“层层抽象”、“层层分类”的特性呢？让我们先看其中一部分连接：<br><img src="https://i.imgur.com/qxWajRQ.png" alt=""></p><p>图上只展示了 \(\mu_1\)的连接。这也就是说，目前已经有了\(\mu_1\)这个特征，（假设是羽毛），那么我想知道它属于上层特征\(\nu_{1}，\nu_{2}，\nu_{3}，\nu_{4}\)假设分别代表了鸡、鸭、鱼、狗）中的哪一个。分类问题我们显然已经是很熟悉了，不就是内积后 softmax 吗？于是单靠\(\mu_{1}\)这个特征，我们推导出它是属于鸡、鸭、鱼、狗的概率分别是$$(P_{1|1},P_{2|1},P_{3|1},P_{4|1})=\frac{1}{Z_1}(e^\left \langle \mu1,\nu1  \right \rangle,e^\left \langle \mu1,\nu2 \right \rangle,e^\left \langle \mu1,\nu3  \right \rangle,e^\left \langle \mu1,\nu4  \right \rangle)\tag{1}$$<br>我们当然期望 \(P_{1|1}\)和\(P_{2|1}\)会明显大于 \(P_{3|1}\)和\(P_{4|1}\)。不过，单靠这个特征还不够，我们还需要综合各个特征，于是可以把上述操作对各个 \(\mu_i\)都做一遍，继而得到<br>\((P_{1|2},P_{2|2},P_{3|2},P_{4|2})\)、\((P_{1|3},P_{2|3},P_{3|3},P_{4|3})\)、…<br>问题是，现在得到这么多预测结果，那我究竟要选择哪个呢？而且我又不是真的要做分类，我要的是融合这些特征，构成更高级的特征。于是 Hinton 认为，既然\(\mu_i\)这个特征得到的概率分布是 \((P_{1|i},P_{2|i},P_{3|i},P_{4|i})\),那么我把这个特征切成四份，分别为 \((P_{1|i}\mu_i,P_{2|i}\mu_i,P_{3|i}\mu_i,P_{4|i}\mu_i)\)，然后把这几个特征分别传给 \(\nu_1,\nu_2,\nu_3,\nu_4\),最后\(\nu_1,\nu_2,\nu_3,\nu_4\)其实就是各个底层传入的特征的累加，这样不就好了？<br>$$\nu_j=squash(\sum_{i}p_{j|i}\mu_i) = squash(\sum_i\frac{e^\left \langle \mu_i,\nu_j \right \rangle}{Z_i}\mu_i)\tag{2}$$<br>从上往下看，那么 Capsule 就是每个底层特征分别做分类，然后将分类结果整合。这时 \(\nu_j\)应该尽量与所有\(\mu_i\)都比较靠近，靠近的度量是内积。因此，从下往上看的话，可以认为 \(\nu_j\)实际上就是各个\(\mu_i\)的某个聚类中心，而 Capsule 的核心思想就是输出是输入的某种聚类结果。</p><p>现在来看这个 squash 是什么玩意，它怎么来的呢？</p><h2 id="2-3-浓缩果汁"><a href="#2-3-浓缩果汁" class="headerlink" title="2.3 浓缩果汁"></a>2.3 浓缩果汁</h2><p>squash 在英文中也有浓缩果汁之意，我们就当它是一杯果汁品尝吧。这杯果汁的出现，是因为 Hinton 希望 Capsule 能有的一个性质是：胶囊的模长能够代表这个特征的概率。</p><p>其实我不喜欢概率这个名词，因为概率让我们联想到归一化，而归一化事实上是一件很麻烦的事情。我觉得可以称为是特征的“显著程度”，这就好解释了，模长越大，这个特征越显著。而我们又希望有一个有界的指标来对这个“显著程度”进行衡量，所以就只能对这个模长进行压缩了，所谓“浓缩就是精华”嘛。Hinton 选取的压缩方案是：</p><p>$$squash(x) = \frac{\left | x \right |^2}{1+\left | x \right |^2}\frac{x}{\left | x \right |}\tag{3}$$<br>其中\(x/\left | x \right |\)是很好理解的，就是将模长变为 1，那么前半部分怎么理解呢？为什么这样选择？事实上，将模长压缩到 0～1 的方案有很多，并不确定 Hinton 选择目前这个方案的思路。<br>然而，一个值得思考的问题是：如果在中间层，那么这个压缩处理是不是必要的呢？因为已经有了后面说的动态路由在里边，因此即使去掉squash 函数，网络也已经具有了非线性了，因此直觉上并没有必要在中间层也引入特征压缩，正如普通神经网络也不一定要用 sigmoid 函数压缩到 0～1。我觉得这个要在实践中好好检验一下。</p><h1 id="3-动态路由"><a href="#3-动态路由" class="headerlink" title="3 动态路由"></a>3 动态路由</h1><p>注意到(2)式，为了求\(\nu_j\) 需要求 softmax，可是为了求 softmax 又需要知道 \(\nu_j\)，这不是个鸡生蛋、蛋生鸡的问题了吗？这时候就要上“主菜”了，即“动态路由”（Dynamic Routing），它能够根据自身的特性来更新（部分）参数，从而初步达到了 Hinton 的放弃梯度下降的目标。</p><p>这道“主菜”究竟是不是这样的呢？它是怎么想出来的？最终收敛到哪里去？让我们先上两道小菜，然后再慢慢来品尝这道主菜。</p><h2 id="3-1-小菜-1"><a href="#3-1-小菜-1" class="headerlink" title="3.1 小菜 1"></a>3.1 小菜 1</h2><p>让我们先回到普通的神经网络，大家知道，激活函数在神经网络中的地位是举足轻重的。当然，激活函数本身很简单，比如一个 tanh 激活的全连接层，用 tensorflow 写起来就是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = tf.matmul(W, x) + b</span><br><span class="line">y = tf.tanh(y)</span><br></pre></td></tr></table></figure></p><p>可是，如果我想用 \(x = y + cos(y)\)的反函数来激活呢？也就是说，你得给我解出 \(y=f(x)\)，然后再用它来做激活函数。</p><p>然而数学家告诉我们，这个东西的反函数是一个超越函数，也就是不可能用初等函数有限地表示出来。那这样不就是故意刁难么？不要紧，我们有迭代：$$y_n+1=x - cos y_n$$<br>选择y0=x，代入上式迭代几次，基本上就可以得到比较准确的y 了。假如迭代三次，那就是\(y = x - cos(x - cos( x - cosx))\)用 tensorflow 写出来就是<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y = tf.matmul(W, x) + b</span><br><span class="line">Y = y</span><br><span class="line">for i in range(3):</span><br><span class="line">Y = y - tf.cos(Y)</span><br></pre></td></tr></table></figure></p><p>如果读者已经“预习”过 Capsule，那么就会发现这跟 Capsule 的动态路由很像。</p><h2 id="3-2-小菜-2"><a href="#3-2-小菜-2" class="headerlink" title="3.2 小菜 2"></a>3.2 小菜 2</h2><p>再来看一个例子，这个例子可能在 NLP 中有很多对应的情景，但图像领域其实也不少。考虑一个向量序列 \(x_1,x_2,…x_n\)，我现在要想办法将这 n个向量整合成一个向量 x(encoder)，然后用这个向量来做分类。</p><p>也许读者会想到用 LSTM。但我这里仅仅想要将它表示为原来向量的线性组合，也就是：<br>$$ x = \sum ^{n}_{i=1}\lambda_ix_i$$<br>这里的\(\lambda_i\)相当于衡量\(x\)与\(x_i\)的相似度。然而问题来了，在 x 出现之前，凭什么能够确定这个相似度呢？这不也是一个鸡生蛋、蛋生鸡的问题吗？解决这个问题的一个方案也是迭代。首先我们也可以定义一个基于 softmax 的相似度指标，然后让<br>$$x = \sum ^n_{i=1}\frac{e^\left \langle x,x_i \right \rangle}{Z}x_i$$<br>一开始，我们一无所知，所以只好取 x为各个 \(x_i\) 的均值，然后代入右边就可以算出一个，再把它代入右边，反复迭代就行，一般迭代有限次就可以收敛，于是就可以将这个迭代过程嵌入到神经网络中了。</p><p>如果说小菜 1 跟动态路由只是神似，那么小菜 2 已经跟动态路由是神似＋形似了。不过我并没有看到已有的工作是这样做的，这个小菜只是我的头脑风暴。</p><h2 id="3-3-上主菜～"><a href="#3-3-上主菜～" class="headerlink" title="3.3 上主菜～"></a>3.3 上主菜～</h2><p>其实有了这两个小菜，动态路由这道主菜根本就不神秘了。为了得到各个 \(\nu_j\)，一开始先让它们全都等于\(\mu_i\)的均值，然后反复迭代就好。说白了，输出是输入的聚类结果，而聚类通常都需要迭代算法，这个迭代算法就称为“动态路由”。至于这个动态路由的细节，其实是不固定的，取决于聚类的算法，比如关于 Capsule 的新文章《MATRIX CAPSULES WITH EM ROUTING》就使用了 Gaussian Mixture Model 来聚类。</p><p>理解到这里，就可以写出本文的动态路由的算法了：</p><blockquote><p>动态路由算法<br>初始化\(b_{ij}=0\)<br>迭代r次：<br>  \(c_i = softmax(b_j)\)<br>  \(s_j = \sum _ic_{ij}\mu_i\)<br>  \(\nu_j = squash(s_j)\)<br>  \(b_{ij} = b_{ij}+\left \langle \mu_i,\nu_j \right \rangle\)<br>返回\(\nu_j\)</p></blockquote><p>这里用的原文中的算法，苏老师根据自己的理解进行了改写，可以参考其博客。</p><h1 id="4-模型细节"><a href="#4-模型细节" class="headerlink" title="4 模型细节"></a>4 模型细节</h1><p>下面介绍 Capsule 实现的细节</p><h2 id="4-1-全连接版"><a href="#4-1-全连接版" class="headerlink" title="4.1 全连接版"></a>4.1 全连接版</h2><p>先不管是 Hinton 版还是我的版本，按照这个动态路由的算法，<br>\(\nu_j\)能够迭代地算出来，那不就没有参数了吗？真的抛弃了反向传播了？</p><p>非也非也～如果真的这样的话，各个 \(\nu_j\) 都一样了。前面已经说了，\(\nu_j\)是作为输入 \(\mu_i\)的某种聚类中心出现的，而从不同角度看输入，得到的聚类结果显然是不一样的。那么为了实现“多角度看特征”，于是可以在每个胶囊传入下一个胶囊之前，都要先乘上一个矩阵做变换，所以 (2)式实际上应该要变为<br>$$\nu_j = squash(\sum_i \frac{e ^\left \langle \hat{\mu}_{j|i},\nu_j \right \rangle}{Z_i}\hat{\mu}{j|i}),\hat{\mu}{j|i} = W_{ji}\mu_i\tag{4}$$<br>这里的 \(W_{ji}\)是待训练的矩阵，这里的乘法是矩阵乘法，也就是矩阵乘以向量。所以，Capsule 变成了下图<img src="https://i.imgur.com/u30irii.png" alt=""></p><blockquote><p>这时候就可以得到完整动态路由了</p></blockquote><blockquote><p>动态路由算法<br>初始化\(b_{ij}=0\)<br>迭代r次：<br>  \(c_i = softmax(b_j)\)<br>  \(s_j = \sum_ic_{ij}\hat{\mu}_{j|i}\)<br>  \(\nu_j = squash(s_j)\)<br>  \(b_{ij} = b_{ij}+\left \langle \hat{\mu}_{j|i},\nu_j \right \rangle\)<br>返回\(\nu_j\)<br>这样的 Capsule 层，显然相当于普通神经网络中的全连接层。</p></blockquote><h2 id="4-2-共享版"><a href="#4-2-共享版" class="headerlink" title="4.2 共享版"></a>4.2 共享版</h2><p>众所周知，全连接层只能处理定长输入，全连接版的 Capsule 也不例外。而 CNN 处理的图像大小通常是不定的，提取的特征数目就不定了，这种情形下，全连接层的 Capsule 就不适用了。因为在前一图就可以看到，参数矩阵的个数等于输入输入胶囊数目乘以输出胶囊数目，既然输入数目不固定，那么就不能用全连接了。</p><p>所以跟 CNN 的权值共享一样，我们也需要一个权值共享版的 Capsule。所谓共享版，是指对于固定的上层胶囊j，它与所有的底层胶囊的连接的变换矩阵是共用的，即\(W_{ji} = W_{j}\)，</p><p>如图所示，共享版其实不难理解，就是自下而上地看，就是所有输入向量经过同一个矩阵进行映射后，完成聚类进行输出，将这个过程重复几次，就输出几个向量（胶囊）；又或者自上而下地看，将每个变换矩阵看成是上层胶囊的识别器，上层胶囊通过这个矩阵来识别出底层胶囊是不是有这个特征。因此很明显，这个版本的胶囊的参数量并不依赖于输入的胶囊个数，因此可以轻松接在 CNN 后面。对于共享版，(2)式要变为<br>$$\nu_j = squash(\sum_i \frac{e ^\left \langle \hat{\mu}_{j|i},\nu_j \right \rangle}{Z_i}\hat{\mu}{j|i}),\hat{\mu}{j|i} = W_{ji}\mu_i\tag{5}$$<br>至于动态路由算法就没有改变了。</p><h2 id="4-3-反向传播"><a href="#4-3-反向传播" class="headerlink" title="4.3 反向传播"></a>4.3 反向传播</h2><p>尽管我不是很喜欢反向传播这个名词，然而这里似乎不得不用上这个名字了。</p><p>现在又有了 \(W_{ji}\)，那么这些参数怎么训练呢？答案是反向传播。读者也许比较晕的是：现在既有动态路由，又有反向传播了，究竟两者怎么配合？其实这个真的就最简单不过了。就好像“小菜 1”那样，把算法的迭代几步（论文中是 3 步），加入到模型中，从形式上来看，就是往模型中添加了三层罢了，剩下的该做什么还是什么，最后构建一个 loss 来反向传播。</p><p>这样看来，Capsule 里边不仅有反向传播，而且只有反向传播，因为动态路由已经作为了模型的一部分，都不算在迭代算法里边了。</p><h2 id="4-4-做了什么"><a href="#4-4-做了什么" class="headerlink" title="4.4 做了什么"></a>4.4 做了什么</h2><p>是时候回顾一下了，Capsule 究竟做了什么？其实用一种最直接的方式来讲，Capsule 就是提供了一种新的“vector in vector out”的方案，这样看跟 CNN、RNN、Attention 层都没太大区别了；从 Hinton 的本意看，就是提供了一种新的、基于聚类思想来代替池化完成特征的整合的方案，这种新方案的特征表达能力更加强大。</p><p>本文转自苏老师：<br><a href="https://kexue.fm/archives/4819/" target="_blank" rel="noopener">https://kexue.fm/archives/4819/</a></p>]]></content>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 胶囊网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>k-max-pooling的keras实现</title>
      <link href="/2018/06/19/kmaxpooling/"/>
      <url>/2018/06/19/kmaxpooling/</url>
      <content type="html"><![CDATA[<p>pooling（池化）是卷积神经网络中常用操作，主要目的是减小张量大小，便于计算，可以认为是一步降维操作。前两天项目上需要用到k-max-pooling操作，项目代码是用keras写的，keras没有k-max-pooling层，所以要自己写。</p><p>卷积神经网络的最经典的模型是卷积+池化+全连接。<br>其中卷积操作通过一个滑动的卷积窗口来强化提取特征，可以认为卷积窗是一个权重矩阵，而卷积操作中的每一步，都是一个矩阵的加权求和，随着卷积窗口滑动，每一步的和被记录下来，形成一个新的矩阵。我们有多少种卷积窗，经过一层卷积就留下几个矩阵。然而，如果经过多层卷积，那最后矩阵就越来越大，那我们对资源耗费太大，有没有什么好的操作呢？–pooling</p><p>简单理解，pooling就是把一个大矩阵，变成小矩阵。如max-pooling，如size=2,就是在原来的矩阵中，每2*2个块中，取其中最大的一个记录下来，然后下一个块，再下一个十几个，最后记录下来的值就成了一个新矩阵，而这个新矩阵的大小，比以前缩小了4倍。有人会问，那不是损失了信息了么？当然损失了信息，但是我们认为损失的信息并没那么重要，就像一幅超清图，和一张高清图，虽然没那么清晰了，但这张图所描述的信息我们还是一眼能看出来。max-pooling操作，可以这么理解，给你一张图片，你分成了好多块，每块里面，别的地方颜色很浅，就一个地方颜色很深，那你是不是第一眼先看到那个黑点呢？</p><p>再说一下mean-pooling.<br>和max-pooling类似，只不过把每块中取最大值的操作，换成了对每块取均值。这样就会把每个点的值都考虑到，没有max-pooling操作那样粗暴。</p><p>还有一种常用的操作是k-max-pooling.这种是在max-pooling上改进来的，因为max-pooling操作太简单粗暴了，k-max-pooling认为每一块不只一个点重要，前几个亮点都比较重要，所以在每一个pooling块中取了前k大的值。</p><p>keras中pooling操作好像只有maxpooling和meanpooing.于是自己写了个代码，实现k-max-pooling。<br>因为项目中用的是LSTM处理文本，我这里就对LSTM层的结果进行处理了，LSTM层输出三维张量(shape=[in_dim1,in_dim2,in_dim3]),其中in_dim1表示样本数量，in_dim2表示一个样本（也就是一段话）的长度（如：这里认为一句话有100个词，那么短于100的句子后面补0，长于100的句子后面截断），in_dim3为lstm结点数量，也就是每个step输出的向量长度。<br>k-max-pooling操作，就是在dim2中，取出k个最大值，可以想象是取出k个最重要的词。<br>输入为LSTM层输出的三维张量，输出为一个二维张量，shape=[out_dim1,out_dim2],out_dim1为样本数，out_dim2为in_dim3*k。</p><p>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test59</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> keras.layers <span class="keyword">import</span> Lambda, Input</span><br><span class="line">    <span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line">    <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">    l = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">11</span>, <span class="number">22</span>, <span class="number">33</span>, <span class="number">44</span>, <span class="number">55</span>, <span class="number">66</span>, <span class="number">77</span>, <span class="number">88</span>, <span class="number">99</span>]]</span><br><span class="line">    data = np.reshape(l, [<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">    print(data)</span><br><span class="line"><span class="comment">###output###</span></span><br><span class="line"><span class="comment">#[[[ 1  2  3]</span></span><br><span class="line"><span class="comment">#  [ 4  5  6]</span></span><br><span class="line"><span class="comment">#  [ 7  8  9]]</span></span><br><span class="line"><span class="comment"># [[11 22 33]</span></span><br><span class="line"><span class="comment">#  [44 55 66]</span></span><br><span class="line"><span class="comment">#  [77 88 99]]]</span></span><br><span class="line"><span class="comment">#数据为2个样本，每个样本是3个step（可以认为是句子有3个词），每个step是一个长为3的向量（可以认为词向量长度为3）</span></span><br><span class="line">    input = Input(shape=[<span class="number">3</span>, <span class="number">3</span>], dtype=<span class="string">'int32'</span>)</span><br><span class="line">    la = Lambda(<span class="keyword">lambda</span> x: tf.reshape(tf.nn.top_k(tf.transpose(x,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>]),k=<span class="number">2</span>)[<span class="number">0</span>],shape=[<span class="number">-1</span>,<span class="number">6</span>]))(input)</span><br><span class="line">    model = Model(inputs=input, outputs=la)</span><br><span class="line">    pre = model.predict(data)</span><br><span class="line">    print(pre)</span><br><span class="line"><span class="comment">#[[ 7  4  8  5  9  6]</span></span><br><span class="line"><span class="comment">#[77 44 88 55 99 66]]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    test59()</span><br></pre></td></tr></table></figure></p><p>这里的k-max-pooling用Lambda写的，<code>tf.reshape(tf.nn.top_k(tf.transpose(x,[0,2,1]),k=2)[0],shape=[-1,6])</code>，分解成三步：<br><code>tf.transpose(x,[0,2,1]</code>–&gt;因为tf有个top_k方法，可以对取张量最后一维的前k大的数，所以我们把step所在维度调整到最后。transpose是一个转置操作。<br><code>tf.nn.top_k（，k=2）</code>–&gt;取出最后一个维度前k大的数。<br><code>tf.reshape(),top_k</code>操作对每个样本取出的结果是一个二维矩阵in_dim3 × k，所以把它转成向量，向量长度为in_dim3*k</p><p>当然，也可以写一个自定义层，处理步骤类似</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KMaxPooling</span><span class="params">(Layer)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    k-max-pooling</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k=<span class="number">1</span>, **kwargs)</span>:</span></span><br><span class="line">        super().__init__(**kwargs)</span><br><span class="line">        self.input_spec = InputSpec(ndim=<span class="number">3</span>)</span><br><span class="line">        self.k = k</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (input_shape[<span class="number">0</span>], (input_shape[<span class="number">2</span>] * self.k))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        <span class="comment"># swap last two dimensions since top_k will be applied along the last dimension</span></span><br><span class="line">        shifted_input = tf.transpose(inputs, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># extract top_k, returns two tensors [values, indices]</span></span><br><span class="line">        top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=<span class="keyword">True</span>, name=<span class="keyword">None</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># return flattened output</span></span><br><span class="line">        <span class="keyword">return</span> Flatten()(top_k)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> Keras </tag>
            
            <tag> tensorflow </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>n阶乘后面有多少个0</title>
      <link href="/2018/06/11/interview/"/>
      <url>/2018/06/11/interview/</url>
      <content type="html"><![CDATA[<p>开始整理下面试题和心得，这一部分包含Lintcode和平时遇到的一些题目。</p><p>题目描述：<br>给定一个正整数n,返回出n的阶乘尾部0的个数。linkcode链接：<br><a href="https://lintcode.com/problem/trailing-zeros/description" target="_blank" rel="noopener">https://lintcode.com/problem/trailing-zeros/description</a></p><p>分析：<br>看到这一题第一感觉先求阶乘，然后求出结果中尾部有多少0。<br>提交代码，超时。</p><p>于是查了下资料，一个数尾部有多少个0，先看这个数的因子，也就是因子分解，如10 分解为2 5，20 分解为2 2 5，100 分解为 2 2 5 5，多找几组数据，发现尾部0的个数为min{2的数，5的个数}。为什么会这样呢？因为尾部的0是由2<em>5得出的，当然也可以是4</em>5，但是4=2*2呀，最终0还是由2和5造出来的。一对2和5造出来一个0，n对2和5造出n个0，如果有10个2，8个5，则只能凑出8对，所以是min{…}.<br>阶乘也就是因子分解的过程，而且因子从小到大排好了。<br>2的个数肯定比5多，因为5大。<br>所以这个题，看因子中5出现的次数就可以啦。</p><p>这里n阶乘用n!表示。<br>5！  –&gt; 1 2 3 4 5 有一个5<br>10!  –&gt; 1 2 3 4 5 6 7 8 9 10 有2个5（10=2<em>5）<br>…<br>25!  –&gt; 1 2 3 4 5 …10 ..15 ..20 ..24 25 有6个5（25=5</em>5）</p><p>125！ = 5 10 15 …25 ..50 ..75 ..100 ..125 有多少呢？<br>我们发现每隔5个数出现一个5，隔25个数多出来1个5..隔125个数再多出一个5<br>也就是从n个抽出5<br>5 10 15 20 25 30 … = 5（<em>1 2 3 4 5 ）<br>然后从上面序列中再隔5抽<br>25 50 75 100 125 … = 5</em>5<em>（1 2 3 4 5 ）<br>再隔5抽<br>125 250 375 500 625 .. = 5</em>5*5（1 2 3 4 5 ）</p><p>规律很明显了</p><p>好了上代码 python3</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    @param: n: An integer</span><br><span class="line">    @return: An integer, denote the number of trailing zeros in n!</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def trailingZeros(self, n):</span><br><span class="line">        zeros = 0</span><br><span class="line">        while n &gt; 0:</span><br><span class="line">            zeros += n // 5</span><br><span class="line">            n //= 5</span><br><span class="line">        return int(zeros)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 面试 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python数据存储利器之pickle</title>
      <link href="/2018/06/05/python-1/"/>
      <url>/2018/06/05/python-1/</url>
      <content type="html"><![CDATA[<p>第一天上班那天，有位同事把部分工作交接给我，主要是处理过的一些数据，存的是txt格式。然后大概看了下，还挺整齐，用’,’分隔的。读进去才发现，本来应该是个List,然而由于存成了txt，所有的都变成了str。所以转来转去，费了两天时间。突然想起来好像有pickle这么个小玩意，就拿来试试。从此，再也不想用csv啥的存数据了</p><p>简单介绍下Pickle<br>pickle使用二进制协议完成对python对象的序列化和反序列化。“picking”是将python对象转为字节流，而“unpickling”则是从字节流中转出来。<br>也就是说，无论你要存的内容是字符串、数字、数组、列表、矩阵、DataFrame、np.array()等等，当你进行picking操作时，统统将这些内容转换成字节流保存到文件中，一般是二进制文件。然后当你想使用这些内容时，再从二进制文件中读出来。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br></pre></td></tr></table></figure></p><p>–picking–<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = ...</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'name.txt'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(data,f,protocol=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure></p><p>这样就把数据写进name.txt文件里了，注意这里是二进制，你用记事本打开是乱码。有个参数，protocol,是你选用的协议，默认为3，在python2中默认为0，如果你在python3用dump写进文件，去python2里load读取文件,会报异常。当然这种奇怪的需求很少，如果遇到了，把dump里的<code>protocol=0</code>就可以了。<br>当然，你的data可以有多个<br>如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a= <span class="number">1</span></span><br><span class="line">b =<span class="number">3</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'name.txt'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump([a,b],f)</span><br></pre></td></tr></table></figure></p><p>–unpicking–<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'name.txt'</span>,<span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = pickle.load(f)</span><br></pre></td></tr></table></figure></p><p>如果你存的数据是多个<br>那么：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'name.txt'</span>,<span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data= pickle.load(f)</span><br><span class="line">    a,b = data</span><br></pre></td></tr></table></figure></p><p>还有除了dump和load，还有dumps和loads。<br>dumps()和loads()用的不太多，简单说下，和dump类似，dumps把数据写成字节对象，但不用保存到文件。loads则是从字节对象中读出，不用读文件。</p>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>lda主题模型python实现篇</title>
      <link href="/2018/06/01/lda/"/>
      <url>/2018/06/01/lda/</url>
      <content type="html"><![CDATA[<p>最近在做一个动因分析的项目，自然想到了主题模型LDA。这次先把模型流程说下，原理后面再讲。<br>lda实现有很多开源库，这里用的是gensim.</p><h2 id="1-文本预处理"><a href="#1-文本预处理" class="headerlink" title="1 文本预处理"></a>1 文本预处理</h2><p>大概说下文本的样子，LDA是无监督模型，也就是说不需要标签，只要传入文本就好。LDA要学习文档-主题分布和主题-词分布，所以我们把一个人的数据join在一起作为一条文档。对文档进行分词，使用的jieba分词工具包。注意，这里要做去停用词处理，包括标点和一些没用的词，如“呵呵”，“哈哈”。做项目时，第一版没有去无用词，最后提出的主题都是“你”“我”“他”“你好”这样的东西，去掉之后可以较好提高结果质量。</p><h2 id="2-将上步处理的结果进行格式化表示"><a href="#2-将上步处理的结果进行格式化表示" class="headerlink" title="2 将上步处理的结果进行格式化表示"></a>2 将上步处理的结果进行格式化表示</h2><p>即将所有文档数表示成m*n的矩阵D,m表示有m篇文档，n表示这篇文档有n个词，n不定长。</p><h2 id="3-生成词典"><a href="#3-生成词典" class="headerlink" title="3 生成词典"></a>3 生成词典</h2><p>用gensim.corpora.Dictionary包<br>这个包讲下吧<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from gensim.corpora import Dictionary</span><br><span class="line">text = [[&apos;我&apos;, &apos;想吃&apos;, &apos;大龙虾&apos;, &apos;和&apos;, &apos;烤猪蹄&apos;]]</span><br><span class="line">dictionary = Dictionary(text)</span><br><span class="line">print((dictionary))</span><br><span class="line">doc = dictionary.doc2bow([&apos;我&apos;, &apos;想吃&apos;, &apos;大龙虾&apos;, &apos;和&apos;, &apos;我&apos;,&apos;你&apos;,&apos;烤猪蹄&apos;])</span><br><span class="line">print(doc)</span><br><span class="line"></span><br><span class="line">#####output#####</span><br><span class="line">Dictionary(5 unique tokens: [&apos;我&apos;, &apos;大龙虾&apos;, &apos;想吃&apos;, &apos;和&apos;, &apos;烤猪蹄&apos;])</span><br><span class="line">[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1)]</span><br></pre></td></tr></table></figure></p><p>可以看出，我们把我想吃大龙虾和烤猪蹄编成字典，共五个词，这里输出结果里看不出是个字典，其实是有下标的，0对应我，1对应大龙虾，…4对应烤猪蹄。<br>然后我们在下一步将文本变成词袋，这里用的文本是[‘我’, ‘想吃’, ‘大龙虾’, ‘和’, ‘我’,’你’,’烤猪蹄’],注意文本格式也是词为元素的列表。这句话是我自己构造的，只是为了说两点，语法请忽略。第一点：“我”这个词出现了两次，所以下标为2的地方，值为2；第二点，“你”这个词出现了1次，可是在词典中没有，所有直接被忽略。 </p><p>这样就可以用字典，将文本表示成词袋模型，词袋模型不懂的，见我另一篇文章，自然语言处理NLP的词如何表示。当我们做完了LDA模型后，对于新的文本，我们想看下它所在的主题分布，就要使用该字典再进行词袋编码，也就是说这个字典，我们以后也会用到，所以，我们在这里把词典保存起来。<br>保存词典可以用pickle,很好用。不懂的见我另一篇文章，神奇的pickle。</p><h2 id="4-训练LDA模型"><a href="#4-训练LDA模型" class="headerlink" title="4 训练LDA模型"></a>4 训练LDA模型</h2><p>这里用的是gensim.models.ldamodel包<br><code>ldamodel = LdaModel(text, num_topics=10, id2word=dictionary, passes=20)</code><br>使用这句话就可以直接训练LDA模型了，讲一下参数吧。<br>text:文本，已经表示成词袋了。<br>num_topics: 提取的主题数<br>id2word:词典<br>passes:类似于在机器学习中常见的epoch，也就是训练了多少轮。</p><p>然后我们得到了训练好的ldamodel.用这个模型可以做哪些事情呢？</p><h2 id="5-ldamodel使用"><a href="#5-ldamodel使用" class="headerlink" title="5 ldamodel使用"></a>5 ldamodel使用</h2><p>可以输出这个模型的各个主题下的主题词<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">print(ldamodel.print_topics(num_topics=10, num_words=10))</span><br><span class="line">###output###</span><br><span class="line">[(0, &apos;0.015*&quot;说&quot; + 0.011*&quot;吃&quot; + 0.008*&quot;想&quot; + 0.007*&quot;睡&quot; + 0.005*&quot;\u2005&quot; + 0.005*&quot;做&quot; + 0.005*&quot;明天&quot; + 0.005*&quot;买&quot; + 0.005*&quot;干嘛&quot; + 0.005*&quot;玩&quot;&apos;),</span><br><span class="line"> (1, &apos;0.017*&quot; &quot; + 0.010*&quot;说&quot; + 0.004*&quot;吃&quot; + 0.004*&quot;许华升&quot; + 0.004*&quot;\x14&quot; + 0.004*&quot;想&quot; + 0.003*&quot;做&quot; + 0.003*&quot;买&quot; + 0.003*&quot;ÿ&quot; + 0.003*&quot;钱&quot;&apos;),</span><br><span class="line"> (2, &apos;0.008*&quot;com&quot; + 0.007*&quot; &quot; + 0.004*&quot;手机&quot; + 0.003*&quot;女&quot; + 0.003*&quot;说&quot; + 0.003*&quot;www&quot; + 0.003*&quot;cc&quot; + 0.002*&quot;号&quot; + 0.002*&quot;qq&quot; + 0.002*&quot;视频&quot;&apos;), </span><br><span class="line"> (3, &apos;0.007*&quot;com&quot; + 0.006*&quot; &quot; + 0.005*&quot;38&quot; + 0.004*&quot;号&quot; + 0.004*&quot;贷&quot; + 0.003*&quot;3000&quot; + 0.003*&quot;10&quot; + 0.003*&quot;做&quot; + 0.003*&quot;说&quot; + 0.002*&quot;111&quot;&apos;), </span><br><span class="line"> (4, &apos;0.017*&quot; &quot; + 0.007*&quot;说&quot; + 0.006*&quot;做&quot; + 0.005*&quot;你好&quot; + 0.005*&quot;吃&quot; + 0.004*&quot;号&quot; + 0.004*&quot;\u2005&quot; + 0.004*&quot;想&quot; + 0.003*&quot;钱&quot; + 0.003*&quot;明天&quot;&apos;), </span><br><span class="line"> (5, &apos;0.013*&quot; &quot; + 0.012*&quot;说&quot; + 0.007*&quot;吃&quot; + 0.006*&quot;想&quot; + 0.005*&quot;睡&quot; + 0.005*&quot;做&quot; + 0.004*&quot;钱&quot; + 0.004*&quot;买&quot; + 0.004*&quot;回来&quot; + 0.004*&quot;干嘛&quot;&apos;), </span><br><span class="line"> (6, &apos;0.010*&quot; &quot; + 0.005*&quot;买大单&quot; + 0.005*&quot;贷&quot; + 0.004*&quot;说&quot; + 0.004*&quot;贷款&quot; + 0.004*&quot;钱&quot; + 0.003*&quot;com&quot; + 0.003*&quot;号&quot; + 0.003*&quot;奥特曼&quot; + 0.003*&quot;吃&quot;&apos;),</span><br><span class="line"> (7, &apos;0.022*&quot; &quot; + 0.010*&quot;说&quot; + 0.008*&quot;做&quot; + 0.007*&quot;吃&quot; + 0.005*&quot;想&quot; + 0.004*&quot;钱&quot; + 0.003*&quot;\u2005&quot; + 0.003*&quot;买&quot; + 0.003*&quot;谢谢&quot; + 0.003*&quot;明天&quot;&apos;),</span><br><span class="line"> (8, &apos;0.017*&quot; &quot; + 0.015*&quot;com&quot; + 0.006*&quot;www&quot; + 0.005*&quot;说&quot; + 0.004*&quot;号&quot; + 0.004*&quot;\u2005&quot; + 0.003*&quot;手机&quot; + 0.003*&quot;钱&quot; + 0.003*&quot;吃&quot; + 0.003*&quot;https&quot;&apos;), </span><br><span class="line"> (9, &apos;0.011*&quot;\u2005&quot; + 0.011*&quot;说&quot; + 0.010*&quot;做&quot; + 0.009*&quot; &quot; + 0.004*&quot;钱&quot; + 0.004*&quot;买&quot; + 0.004*&quot;发&quot; + 0.003*&quot;谢谢&quot; + 0.003*&quot;吃&quot; + 0.003*&quot;玩&quot;&apos;)]</span><br></pre></td></tr></table></figure></p><p>这里随便找了些数据，效果不是太明显，这里主要讲处理流程，不要被这结果干扰心情，不过工业应用中很多时候，实际结果和你理想的结果有很大差距。用一些正常的数据，是可以看出一些信息的。上次用汽车之家的评论数据做lda，主题信息就比较明显，有关于油耗的，有关于买车的等等。<br>也可以对新文本，找出其所在的主题分布。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def to_lda_vec(model, dictionary, text_list=None):</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    :param model: lda model</span><br><span class="line">    :param dictionary: Dictionary for toBow</span><br><span class="line">    :param text_list: texts</span><br><span class="line">    :return: texts about one topic</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    lda_list = []</span><br><span class="line">    for texts in text_list:</span><br><span class="line">        doc_bow = dictionary.doc2bow(texts)</span><br><span class="line">        doc_lda = model[doc_bow]</span><br><span class="line">lda_list.append(doc_lda)</span><br><span class="line"></span><br><span class="line">    return lda_list</span><br></pre></td></tr></table></figure></p><p>这个方法中的参数加了注释，这里可以看到有个参数是dictionary，这里就是我们前面训练lda时用的词典，前面保存的词典派上用场了。最后输出的lda_list是一个列表，列表中元素为每句话的doc_lda,doc_lda是这样子的[(5,0.342345),(6,0.1111)…]，也就是个list，无素为元组，元组包括两个值，第一个值表示主题id，第二个值表示属于该主题的概率。</p><p>也可以用于新文本数据的向量化，即将新的文本映射成主题向量，然后可以做分类，做聚类，做推荐。</p>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> lda </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python之list.count()和Count()类</title>
      <link href="/2018/05/23/python/"/>
      <url>/2018/05/23/python/</url>
      <content type="html"><![CDATA[<p>Count()类的使用<br>同事给了我一堆文本数据，让我帮个小忙。他想统计下每个词的词频，看看文本中提到最多的是什么，然后做后面分析。</p><p>不就是统计词频吗，虽然之前不经常做这个。但是拍脑袋一想，先分词，去停用词，把所有词放到一个列表里，统计，搞定。</p><p>于是五分钟写了个代码，计数那个地方，我用的List里的count方法。不怕丢人，我把代码放这了。。。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">count_word = []</span><br><span class="line">stop_words = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data/count.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    lines = f.readlines()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data/stop_words_new.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    stop_lines = f.readlines()</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> stop_lines:</span><br><span class="line">    stop_words.append(word.strip())</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">    words = jieba.lcut(line)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> stop_words:</span><br><span class="line">            count_word.append(i)</span><br><span class="line">count_word_set = list(set(count_word))</span><br><span class="line">counts = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> count_word_set:</span><br><span class="line">    counts.append(count_word.count(i))</span><br><span class="line">print(len(count_word_set))</span><br><span class="line">print(len(counts))</span><br><span class="line">df = pd.DataFrame(data=&#123;<span class="string">'word'</span>: count_word_set, <span class="string">'count'</span>: counts&#125;)</span><br><span class="line">df.sort_values(by=<span class="string">'count'</span>)</span><br><span class="line">df.to_csv(<span class="string">'data/count.txt'</span>, sep=<span class="string">'\t'</span>, encoding=<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure></p><p>然而，等了一晚上，发现没出来结果。他说数据量不大，几百万条文本。我这里用的set去重，然后遍历set中的每个词，再用list.count(词)来得词频，笨方法行不通。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">所以今天试了Count()类。</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">count_word = []</span><br><span class="line">count = Counter()</span><br><span class="line">stop_words = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data/count.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    lines = f.readlines()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data/stop_words_new.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    stop_lines = f.readlines()</span><br><span class="line">print(<span class="string">'data finish'</span>)</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> stop_lines:</span><br><span class="line">    stop_words.append(word.strip())</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">    words = jieba.lcut(line)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> stop_words:</span><br><span class="line">            count_word.append(i)</span><br><span class="line">print(<span class="string">'cut finish'</span>)</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> count_word:</span><br><span class="line">    count[word] += <span class="number">1</span></span><br><span class="line">cou = count.most_common()</span><br><span class="line">print(cou)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data/common.txt'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(cou, f)</span><br></pre></td></tr></table></figure></p><p>分完词就结束了。。如果只是想看某个词的词频，用list的count()方法还好，如果统计所有的，还是用Count()类吧。</p>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>java异常之java.sql.SQLException Illegal mix of collations (utf8_general_ci</title>
      <link href="/2018/05/22/java/"/>
      <url>/2018/05/22/java/</url>
      <content type="html"><![CDATA[<p>之前的服务器重装了，所以项目代码重新布置了。明明之前能跑的程序，在我PC机上也是能跑的，放到服务器下就报了这个异常。<br>java.sql.SQLException: Illegal mix of collations (utf8_general_ci,IMPLICIT) and (utf8_general_ci,COERCIBLE) for operation ‘=’<br>跟据异常信息，大致可以理解为是字符集问题。</p><p>后来查了下原因，我的数据库使用的mysql建的，字符集为utf-8，数据库排序规则为utf8_unicode_ci.数据库的表是通过hibernate自动生成的，但是少了一个人事部的基本信息表，所以后来把人事表导进来，但是人事表的排序规则用的是unicode_general_ci，正好是异常提示中的内容。</p><p>查明了原因，我把数据库删了，因为服务器刚布好，还没投入使用，库内无数据，如果使用过程中出现这种问题，就改那个捣乱的表。当然，系统在使用过程中，也不可能出现这个问题。</p>]]></content>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> 异常解决 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python异常之UnicodeEncodeError &#39;ascii&#39;</title>
      <link href="/2018/05/16/python1/"/>
      <url>/2018/05/16/python1/</url>
      <content type="html"><![CDATA[<p>很多时候,你想打印一些数据，想直观的看看结果。可是！你在python中的print()语句报错了。<br>如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;test_aci.py&quot;, line 2, in &lt;module&gt;</span><br><span class="line">    print(&apos;xt\u7ecf&apos;)</span><br><span class="line">UnicodeEncodeError: &apos;ascii&apos; codec can&apos;t encode character &apos;\u7ecf&apos; in position 2: ordinal not in range(128)</span><br></pre></td></tr></table></figure></p><p>你查阅各种博客，知道了是编码问题，可是试了几种方法无法解决。可能在前面加# coding=utf-8,这种思路是对的，但是解决的是sys编码.而你遇到的是print编码问题，就好像你手流血了，你包扎脚。..。。就没有什么好点的方法吗？<br>那我们怎么包扎手呢？<br>在你的py文件合适的位置加上这么一句：<br>如果你不知道在哪，在<code>import</code> 后面加就行。。<br>python2:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if sys.stdout.encoding != &apos;UTF-8&apos;:</span><br><span class="line">    sys.stdout = codecs.getwriter(&apos;utf-8&apos;)(sys.stdout, &apos;strict&apos;)</span><br><span class="line">if sys.stderr.encoding != &apos;UTF-8&apos;:</span><br><span class="line">    sys.stderr = codecs.getwriter(&apos;utf-8&apos;)(sys.stderr, &apos;strict&apos;)</span><br></pre></td></tr></table></figure></p><p>python3:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if sys.stdout.encoding != &apos;UTF-8&apos;:</span><br><span class="line">    sys.stdout = codecs.getwriter(&apos;utf-8&apos;)(sys.stdout.buffer, &apos;strict&apos;)</span><br><span class="line">if sys.stderr.encoding != &apos;UTF-8&apos;:</span><br><span class="line">    sys.stderr = codecs.getwriter(&apos;utf-8&apos;)(sys.stderr.buffer, &apos;strict&apos;)</span><br></pre></td></tr></table></figure></p><p>问题解决，完美！</p><p>可以看出，这是stdout问题，不是加个<code>#coding:utf-8</code>就可以的。这主要是环境配置问题，你当然可以修改环境配置，是个一劳永逸的办法，但很多时候，你拿不到root权限呀，哈哈。</p><p>每天一点，进步不难。</p>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 异常解决 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pandas之concat</title>
      <link href="/2018/05/16/pandas1/"/>
      <url>/2018/05/16/pandas1/</url>
      <content type="html"><![CDATA[<p>如果我们有两个dataFrame,该怎么合在一起呢？<br>用concat<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">    inp1 = [&#123;<span class="string">'c1'</span>: <span class="number">10</span>, <span class="string">'c2'</span>: <span class="number">100</span>&#125;, &#123;<span class="string">'c1'</span>: <span class="number">11</span>, <span class="string">'c2'</span>: <span class="number">110</span>&#125;, &#123;<span class="string">'c1'</span>: <span class="number">12</span>, <span class="string">'c2'</span>: <span class="number">120</span>&#125;]</span><br><span class="line">    inp2 = [&#123;<span class="string">'c1'</span>: <span class="number">20</span>, <span class="string">'c2'</span>: <span class="number">100</span>&#125;, &#123;<span class="string">'c1'</span>: <span class="number">21</span>, <span class="string">'c2'</span>: <span class="number">110</span>&#125;, &#123;<span class="string">'c1'</span>: <span class="number">22</span>, <span class="string">'c2'</span>: <span class="number">120</span>&#125;]</span><br><span class="line">    df1 = pd.DataFrame(inp1)</span><br><span class="line">    print(df1)</span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">10</span>)</span><br><span class="line">    df2 = pd.DataFrame(inp2)</span><br><span class="line">    print(df2)</span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">10</span>)</span><br><span class="line">    df = pd.concat([df1, df2], ignore_index=<span class="keyword">True</span>)</span><br><span class="line">    print(df)</span><br><span class="line"></span><br><span class="line"><span class="comment">#output:</span></span><br><span class="line"> c1   c2</span><br><span class="line"><span class="number">0</span>  <span class="number">10</span>  <span class="number">100</span></span><br><span class="line"><span class="number">1</span>  <span class="number">11</span>  <span class="number">110</span></span><br><span class="line"><span class="number">2</span>  <span class="number">12</span>  <span class="number">120</span></span><br><span class="line">----------</span><br><span class="line">   c1   c2</span><br><span class="line"><span class="number">0</span>  <span class="number">20</span>  <span class="number">100</span></span><br><span class="line"><span class="number">1</span>  <span class="number">21</span>  <span class="number">110</span></span><br><span class="line"><span class="number">2</span>  <span class="number">22</span>  <span class="number">120</span></span><br><span class="line">----------</span><br><span class="line">   c1   c2</span><br><span class="line"><span class="number">0</span>  <span class="number">10</span>  <span class="number">100</span></span><br><span class="line"><span class="number">1</span>  <span class="number">11</span>  <span class="number">110</span></span><br><span class="line"><span class="number">2</span>  <span class="number">12</span>  <span class="number">120</span></span><br><span class="line"><span class="number">3</span>  <span class="number">20</span>  <span class="number">100</span></span><br><span class="line"><span class="number">4</span>  <span class="number">21</span>  <span class="number">110</span></span><br><span class="line"><span class="number">5</span>  <span class="number">22</span>  <span class="number">120</span></span><br></pre></td></tr></table></figure></p><p>每天一点，学习不难。</p>]]></content>
      
      
        <tags>
            
            <tag> pandas </tag>
            
            <tag> 数据分析 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>linux命令之grep</title>
      <link href="/2018/05/16/linux1/"/>
      <url>/2018/05/16/linux1/</url>
      <content type="html"><![CDATA[<p>今天在跑分类模型，发现了一些比较奇怪的结果，严重影响了分类器性能，所有就想看看这些数据是啥，因为数据是加密的，只能看到id,所以这里就把id取出来，693d0f86a8a90d5a138f75ac7160490c，这里经过md5加密的，然后经过查表解密，得到其明文id,这里假设id为im_12345,然后我们就可以去明文中查找带这个id的数据了。数据放在linux下，我们在终端中用<br><code>grep &#39;im_12345&#39; xx.txt</code><br><code>//xx.txt为要查找的文件名</code><br>就可以返回我们想看的那条数据了。<br>然后发现，返回了一大堆数据，实在不知道哪条才是<br>于是我们再做一次grep<br>即<br><code>grep &#39;im_12345&#39; xx.txt|prep &#39;2017-12-12&#39;</code></p><p>总结，<code>grep pattern1 file|grep pattern2</code><br>也就是在pattern1匹配的基础上再进行一次grep<br>哈哈，每天一点，进步不难。</p>]]></content>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Keras 笔记之Mask层</title>
      <link href="/2018/05/12/keras-mask/"/>
      <url>/2018/05/12/keras-mask/</url>
      <content type="html"><![CDATA[<p>keras 的Mask层<br>先看下官方文档的解释</p><hr><p>Masking层<br>keras.layers.core.Masking(mask_value=0.0)<br>使用给定的值对输入的序列信号进行“屏蔽”，用以定位需要跳过的时间步</p><p>对于输入张量的时间步，即输入张量的第1维度（维度从0开始算，见例子），如果输入张量在该时间步上都等于mask_value，则该时间步将在模型接下来的所有层（只要支持masking）被跳过（屏蔽）。</p><p>如果模型接下来的一些层不支持masking，却接受到masking过的数据，则抛出异常。</p><p>例子<br>考虑输入数据x是一个形如(samples,timesteps,features)的张量，现将其送入LSTM层。因为你缺少时间步为3和5的信号，所以你希望将其掩盖。这时候应该：</p><p>赋值x[:,3,:] = 0.，x[:,5,:] = 0.</p><p>在LSTM层之前插入mask_value=0.的Masking层</p><p>model = Sequential()<br>model.add(Masking(mask_value=0., input_shape=(timesteps, features)))<br>model.add(LSTM(32))</p><hr><p>在用LSTM等模型处理文本数据时，因为文本是变长的，所以在处理的过程中，要先进行长度的统一。常用的方法为<br><code>X_data = sequence.pad_sequence</code>(maxlen=10,value=0,padding=’post’)<br>此步骤将X_data统一长度为10.<br>如[1,2,3,4,5]–&gt;变为[1,2,3,4,5,0,0,0,0,0]<br>这样就可以把X_data 输入到model的Embedding等层。<br>然而，交给LSTM处理时，还有对数据进行反padding.也就是把后面的0去掉。<br>这个时候就是Mask层派上用场的时候了。Mask(0)经过Mask后，可以忽略X_data中所有的0，当然，把后面补的0去掉是可以理解的。那如果句中有0呢？一般情况下，如文本处理，会把文本映射成index，这样最大的好处就是节约空间。有些大文本数据，几百个G，经过了index映射，也就还剩几个G。这是题外话了，我们在keras的Embedding层会讲的。而这个时候index中的0,往往是一些无法转成词向量的低频词，这些词没有词向量，去掉对整个文本的处理也没有影响，所以在Mask中和补上的0一起忽略就好啦。<br>这里的忽略是什么意思呢？也就是不处理。<br>很多朋友以为Mask后会直接把0去掉。其实不是的。<br>可以做一些实验，如model的Mask后接个LSTM层，对LSTM输出每个时间步的值，发现，如果设置了Mask层，则上面[1,2,3,4,5,00000]的数据处理结果，前5位是经过了计算，补0的对应的位置的值，和第5位的值相同，也就是说LSTM对后面补0的位置并没有计算。</p>]]></content>
      
      
        <tags>
            
            <tag> Keras </tag>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>tensorflow学习笔记3</title>
      <link href="/2018/04/27/tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03/"/>
      <url>/2018/04/27/tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03/</url>
      <content type="html"><![CDATA[<p>实战,今天给大家举个回归例子，来说明下tensorflow的训练过程。</p><p>先贴上代码和注释。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#准备好数据，x为生成-1到1之间的100个数，y为2*x+1+噪声</span></span><br><span class="line">data_x = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">data_y = np.multiply(data_x, <span class="number">2</span>) + <span class="number">1</span> + np.random.uniform(<span class="number">-0.5</span>, <span class="number">0.5</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#画图，将x,y的数据投上去</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">ax.scatter(data_x, data_y)</span><br><span class="line">plt.ion()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>这里看下图吧<br><img src="https://i.imgur.com/96MSzxY.png" alt=""><br>这里的点点就是我们的数据，大概按一条斜线分布，所以我们找到一条回归线来拟合这些数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#准备好placeholder,占位符，也叫容器，也就是个坑，用来放数据的</span></span><br><span class="line">x_placeholder = tf.placeholder(tf.float32, name=<span class="string">'X'</span>)</span><br><span class="line">y_placeholder = tf.placeholder(tf.float32, name=<span class="string">'Y'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#变量，</span></span><br><span class="line">W = tf.Variable(dtype=tf.float32,name=<span class="string">'W'</span>,initial_value=tf.random_normal([<span class="number">1</span>]))</span><br><span class="line">b = tf.Variable(dtype=tf.float32,name=<span class="string">'W'</span>,initial_value=tf.random_normal([<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#y_pred 给出回归式的Op</span></span><br><span class="line">y_pred = tf.add(tf.multiply(x_placeholder,W),b)</span><br><span class="line"></span><br><span class="line"><span class="comment">#求loss,给出优化器</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y_placeholder-y_pred,name=<span class="string">'loss'</span>))</span><br><span class="line"></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#session会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"><span class="comment">#迭代100次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        total_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(data_x,data_y ):</span><br><span class="line"><span class="comment">#将之前graph中的loss和optimizer跑起来，训练过程就在这，因为有个minimize(loss)的操作</span></span><br><span class="line">            _, los = sess.run([optimizer, loss], feed_dict=&#123;x_placeholder: x, y_placeholder: y&#125;)</span><br><span class="line">            total_loss += los</span><br><span class="line"><span class="comment">#求预测值</span></span><br><span class="line">        yp = sess.run(y_pred,feed_dict=&#123;x_placeholder:data_x, y_placeholder: data_y&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line"><span class="comment">#每5步画一次线</span></span><br><span class="line">            lines = ax.plot(data_x, yp, <span class="string">'r-'</span>, lw=<span class="number">5</span>)</span><br><span class="line">            print(<span class="string">'Epoch &#123;0&#125;: &#123;1&#125;'</span>.format(i, total_loss / <span class="number">100</span>))</span><br><span class="line">            plt.pause(<span class="number">0.1</span>)</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line"><span class="comment">#为了不让线一直赖着不走，我们每画完就要删除。</span></span><br><span class="line">                ax.lines.pop(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">except</span> Exception:</span><br><span class="line">                print(<span class="string">'error'</span>)</span><br></pre></td></tr></table></figure><p>这里总结一下。本例解决的是一个线性回归问题，用tensorflow仍先画图，再运行。当然，还有准备数据。<br>其中画图部分包括：占位符定义，变量（权重）定义，运算式定义，loss定义，再加上一个让Loss减小的优化器<br>运行部分包括：将数据喂给相应的op，并run起来，画图操作是为了让大家更直观。</p>]]></content>
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>tensorflow学习笔记2</title>
      <link href="/2018/04/26/tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/"/>
      <url>/2018/04/26/tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/</url>
      <content type="html"><![CDATA[<p>上期我们说到tensorflow分两步，Graph和run。在公司用tensorflow搭了下lstm做分类的模型，结果很奇怪。同样的模型结构，我用Keras跑出来auc为0.7,而用tensorflow时loss一直跳，而且auc基本在0.5左右。然后在排错过程中，我想到了graph，把graph打出来后，发现了几个问题，如最后一层忘记加sigmoid激活。 尴尬！！！不过反映了tensorflow graph的一个好处，可以让我们像看图一样检查模型。。。 今天主要讲下常量，变量和占位符。</p><p>常量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = tf.contant([3,6])#表示生成一个长度为2的向量a = [3,6]，此时a是一个op，在sess.run之后才有值</span><br><span class="line">#tf.zeros()</span><br><span class="line">#tf.ones()</span><br><span class="line">#都为常量</span><br><span class="line">######------------------</span><br><span class="line">#tf.random_normal(shape=,dtype=,name=&apos;&apos;)</span><br><span class="line">#tf.random_uniform()</span><br><span class="line">#以上也是常量，只不过是随机常量</span><br></pre></td></tr></table></figure></p><p>变量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">b = tf.Variable(2,name=&apos;scalar&apos;)#生成一个变量，并为该变量赋初值为2，仍在sess.run()后才有值为2。</span><br><span class="line">#变量在使用之前要初始化！！！初始化变量有三种方式</span><br><span class="line">#1）全部初始化</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line">#2)初始化部分变量</span><br><span class="line">init_ab = tf.variables_initializer([a,b],name =&apos;init_ab&apos;)</span><br><span class="line">#3)初始化单个变量</span><br><span class="line">sess.run(b.initializer)</span><br><span class="line">#当然，此时b被初始化后是个Tensor,</span><br><span class="line">print(b) #Tensor(&apos;Variable/....&apos;)</span><br><span class="line">print(b.eval()) #[[0.444,0.111,....],...,[...]],使用eval()输出变量内容a</span><br></pre></td></tr></table></figure></p><p>记得常量为一个op,而Variable()是一个类<br>占位符placeholder()</p><p><code>a = tf.placeholder(dtype,shape=,name=&#39;&#39;)</code>占位符就是个容器，先占个坑，里面什么都没有，在run的时候，可以通过 <code>sess.run([a,b],feed_dict={a:a_data,b:b_data})</code>来将值放到之前占的坑里。 一般我们用占位符来存放用于训练的数据，当然我们可以直接用数据来构建op，但是有点外行了。。用placeholder的方式，可以让我们数据送入的更自由，可以按自己需要的batch来送入。<br>简单总结一下，常量constant()用来构建一些在模型中不需要改变的量，变量Variable()一般用来构建权重等，需要不断更新，占位符placeholder一般用来作数据的容器。</p>]]></content>
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>tensorflow学习笔记1</title>
      <link href="/2018/04/26/tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/"/>
      <url>/2018/04/26/tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/</url>
      <content type="html"><![CDATA[<p>明天要去实习了，心里有些激动，有些紧张，还有些舍不得，舍不得学校的样子，和某个人。</p><p>进入正题。</p><p>tensorflow是谷歌开源的一款深度学习框架，我用的是python，这个框架是目前最火的框架之一，很有学习的必要。</p><p>首先tensorflow，tensor +　flow，tensor –&gt; 张量，flow –&gt;流动, 也就是张量在图里流动。这里讲两点，张量就是，0维的叫标量（数字），1维的叫向量，2维的叫矩阵，那3+维只能叫张量了;而在图里流动，这里的图（graph）是tensorflow的重要思想之一，tensorflow把运算结构画成graph，其中有节点op，有边，边代表张量的流动，节点代表运算。</p><p>在tensorflow代码中，分为两大块，第一块用来画图，即把所有的运算画在一张图上，并没有实际运算；第二块用来运算，Session().这样做看似很麻烦，其实有个好处，我们看到了图之后，可以有选择性的运算某些节点，而一些没必要的节点可以不运算，节约了大量的时间。如下面的代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x=2</span><br><span class="line"></span><br><span class="line">y=3</span><br><span class="line"></span><br><span class="line">add_op=tf.add(x,y)</span><br><span class="line"></span><br><span class="line">mul_op=tf.mul(x,y)</span><br><span class="line"></span><br><span class="line">useless=tf.mul(x,add_op)</span><br><span class="line"></span><br><span class="line">pow_op=tf.pow(add_op,mul_op)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line"></span><br><span class="line">     z=sess.run(pow_op)</span><br></pre></td></tr></table></figure></p><p><img src="/image/1.jpg" alt=""></p><p>可以看到如果只<code>run(pow_op)</code>,那么<code>useless</code>就不会被运算。原因是，当我们<code>run</code>时，会从图中找该op所依赖的op，而<code>pow_op</code>没有依赖<code>useless</code>,所以就没有执行啦。<br>记得查看graph的时候，可不是像打开jpg一样哟。。要<code>cd</code>到graph存放的目录下，<code>Tensorboard --logdir=&#39;目录 &#39;</code>才行。<br>待续。。。</p>]]></content>
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>找实习</title>
      <link href="/2018/04/26/%E6%89%BE%E5%AE%9E%E4%B9%A0/"/>
      <url>/2018/04/26/%E6%89%BE%E5%AE%9E%E4%B9%A0/</url>
      <content type="html"><![CDATA[<p>说件开心的事，我种的海棠花开了。</p><p>再说件开心的事，讯飞实习录用了，上午接到的电话，下周二入职。</p><p>这个实习等好久了，前段时间看到大家都在找实习，心里发慌，于是自己也厚着脸皮找朋友内推。蚂蚁金服的一面，就给我拒了，心里的酸，每天早上天没亮就往外反，搞得每天都睡不好觉，感觉自己找不到工作了。可能从来没找过工作，没接受过这种刺激，那几天还在调整心情，也感激下我的小女友，一直给我鼓励。后来一边投实习，一边总结经验，感觉没有实践经验是很大的减分项，所以又找朋友帮我推了下讯飞的实习，这个不是暑期实习，所以面试相对简单些，结束后朋友告诉我主管对我印象还不错，应该通过了。可是，人事部一直没给通知。总担心那边出什么意外情况，这两天脑海里上演了好多大戏，也被自己的戏给吓到过。不过还好，终于还是给了通知，虽然有点晚。</p><p>晚上，本来想去科大参加腾讯的宣讲，谁曾想临时改了场地，在另一个校区，干脆不去了，坐下来老老实实的做携程的笔试题，前面的选择和问答题还好，选择题基本都是基础知识，考点也比较细。问答有个基因的条件概率题，大概做出来了，还一道讲一下深度学习中，relu比sigmoid和tanh好在什么地方。最后一个题，编程，没看懂题，这里记录一下。最长路径问题。</p><p>现有ABCDE五个字符，以及M个字符串，每个字符串由这五个字符和1-9的整数间隔组成，如：A2B3D，表示存在A-&gt;B B-&gt;D的路径，且路径长为2和3，可以推出A-&gt;D的一条路径长为5.求最长的一条路径的长度，如果任何一处路径出现环（如A-&gt;…-&gt;A的路径），则返回-1.<br>输入： 第一行 为字符串的个数M 第二行 开始为M个字符串<br>输出： 最长的一条路径的长度，如果出现环，返回-1<br>如输入 4 A2B3D 、 A4C2E 、 A5D、 C3B<br>输出 10</p><p>刚开始理解错了，所以信心满满写了20分钟代码，最后测试没通过。等读懂题，黄花菜早结冰了。。</p><p>这些笔试编程题都比较奇葩，多做做应该会有不小进步。</p>]]></content>
      
      <categories>
          
          <category> 心情 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面试 </tag>
            
            <tag> 心情 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>初来博客</title>
      <link href="/2018/04/26/%E5%88%9D%E6%9D%A5%E5%8D%9A%E5%AE%A2/"/>
      <url>/2018/04/26/%E5%88%9D%E6%9D%A5%E5%8D%9A%E5%AE%A2/</url>
      <content type="html"><![CDATA[<p>我的第一篇博客，也不知道写点啥。</p><p>研究生生涯转眼间过去了大半，曾经的舒适区已经渐渐被现实挤得越来越小。细细想来，过去的这段时间，我究竟在忙些什么？想了很久，才知道答案：瞎忙。</p><p>在本科时候，自己学习了JAVA EE开发，也做过一些小网站，本来要去找工作，可是拿到了保研名额，所以想在舒适区里再窝几年，那个时候的我，虽然怂，不敢面对陌生的人生，可是却有着不可一世的傲气，因为我是专业前三，因为我是安徽省优秀毕业生，因为我是学霸，其实说到底，因为我顶着光环，或者说我意想出来的光环。</p><p>读研，进研究所，接触了机器学习和自然语言处理，发现自己原来会的JAVA开发只是皮毛中的皮毛，我自诩精通技术，却对Machine Learning和NLP一无所知，于是我开始变得谦逊，因为实在没有底气骄傲，尴尬，哈哈。。。</p><p>现在，研二生活也渐渐近尾声，常常有种恐惧感，就像风筝要飞了，手中的线越来越少，你看到了，却无能为力。还好，有篇论文已经写出来了，在投，在等结果；Machine Learning 和 NLP也算入门（不敢吹牛），一些算法也做过推导和代码编写。从开始不懂LR（逻辑回归），到现在懂了一点，也算进步吧。哈哈，开玩笑，懂了不只一点，比一点多一些。</p><p>那压力在哪呢？在找工作。</p><p>前两天投了几个实习，结果不太乐观。面试知识点比较细，虽然基本都能答上来，但总是给人一种不太精通的感觉。所以最近也在恶补数学原理和代码实现，毕竟这是理科和工科的结合嘛。说到这，不禁有点疑问，为什么非要做人工智能相关呢？明明不会，何苦为难。我也说不清，只是感觉这个有挑战有乐趣，工作不就应该这些么？比尔说过一句话：人们往往高估自己一年内能做的事，却又低估自己十年内能做的事。可能我现在还有半只脚在门外，不过认定一个目标往前走，总有踏进去的一天吧。</p><p>回想自己这两年研，AI坚持在学，毕竟想吃这碗饭；也在坚持健身，常常弹琴；有人爱，有事做，有所期待。</p><p>我在瞎忙？<br>好像不是。</p>]]></content>
      
      
        <tags>
            
            <tag> 心情 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
