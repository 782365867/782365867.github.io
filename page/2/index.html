<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <meta name="description" content="本人目前从事机器学习相关研究和工作，对机器学习和深度学习仍在入门阶段。开此博客，希望记录学习路上的一些体会，还有对生活的一些思考。" />
  

  
  <meta name="keywords" content="AI ML DL life" />
  
  
  
  
  
  
  <title>山风之行</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="本人目前从事机器学习相关研究和工作，对机器学习和深度学习仍在入门阶段。开此博客，希望记录学习路上的一些体会，还有对生活的一些思考。">
<meta name="keywords" content="AI ML DL life">
<meta property="og:type" content="website">
<meta property="og:title" content="山风之行">
<meta property="og:url" content="http://xurui.club/page/2/index.html">
<meta property="og:site_name" content="山风之行">
<meta property="og:description" content="本人目前从事机器学习相关研究和工作，对机器学习和深度学习仍在入门阶段。开此博客，希望记录学习路上的一些体会，还有对生活的一些思考。">
<meta property="og:locale" content="zh-Ch">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="山风之行">
<meta name="twitter:description" content="本人目前从事机器学习相关研究和工作，对机器学习和深度学习仍在入门阶段。开此博客，希望记录学习路上的一些体会，还有对生活的一些思考。">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
  <script src='//push.zhanzhang.baidu.com/push.js'></script>
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="山风之行" rel="home">山风之行</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">—— 修养 风范 血性 胆识</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">主页</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives">文章</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about">关于</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main">
  
    <article id="post-capsnet" class="post-capsnet post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2018/06/22/capsnet/">揭开迷雾，来一顿美味的Capsule盛宴</a>
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://xurui.club/2018/06/22/capsnet/" data-id="cjyco8227003eq4daqxwrgdop" class="leave-reply bdsharebuttonbox" data-cmd="more">分享</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>由深度学习先驱 Hinton 开源的 Capsule 论文《Dynamic Routing Between Capsules》，无疑是去年深度学习界最热点的消息之一。得益于各种媒体的各种吹捧，Capsule 被冠以了各种神秘的色彩，诸如“抛弃了梯度下降”、“推倒深度学习重来”等字眼层出不穷，但也有人觉得 Capsule 不外乎是一个新的炒作概念。</p>
<p>本文试图揭开让人迷惘的云雾，领悟 Capsule 背后的原理和魅力，品尝这一顿 Capsule 盛宴。同时，笔者补做了一个自己设计的实验，这个实验能比原论文的实验更有力说明 Capsule 的确产生效果了。</p>
<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h1><p>Capsule 的论文已经放出几个月了，网上已经有很多大佬进行解读，也有大佬开源实现了 CapsuleNet，这些内容都加速了我对 Capsule 的理解。然而，我觉得美中不足的是，网上多数的解读，都只是在论文的翻译上粉饰了一点文字，并没有对 Capsule 的原理进行解读。比如“动态路由”那部分，基本上就是照搬论文的算法，然后说一下迭代 3 次就收敛了。但收敛出什么来？论文没有说，解读也没有说，这显然是不能让人满意的。也难怪知乎上有读者评论说：</p>
<blockquote>
<p>所谓的 capsule 为 dl 又贡献了一个花里胡哨的 trick 概念。说它是 trick，因为 hinton 没有说为什么 routing 算法为什么需要那么几步，循环套着循环，有什么理论依据吗？还是就是凑出来的？<br>如何评价深度学习之父 Hinton 发布的 Capsule 论文？ - 知乎</p>
</blockquote>
<p>这个评论虽然过激，然而也是很中肯的：凭啥 Hinton 摆出来一套算法又不解释，我们就要稀里糊涂的跟着玩？</p>
<h1 id="2-Capsule-盛宴"><a href="#2-Capsule-盛宴" class="headerlink" title="2 Capsule 盛宴"></a>2 Capsule 盛宴</h1><h2 id="2-1-宴会特色"><a href="#2-1-宴会特色" class="headerlink" title="2.1 宴会特色"></a>2.1 宴会特色</h2><p>这次 Capsule 盛宴的特色是“vector in vector out”，取代了以往的“scaler in scaler out”，也就是神经元的输入输出都变成了向量，从而算是对神经网络理论的一次革命。然而真的是这样子吗？难道我们以往就没有做过“vector in vector out”的任务了吗？有，而且多的是！NLP 中，一个词向量序列的输入，不就可以看成“vector in”了吗？这个词向量序列经过 RNN/CNN/Attention 的编码，输出一个新序列，不就是“vector out”了吗？在目前的深度学习中，从来不缺乏“vector in vector out”的案例，因此显然这不能算是 Capsule 的革命。</p>
<p>Capsule 的革命在于：它提出了一种新的“vector in vector out”的传递方案，并且这种方案在很大程度上是可解释的。</p>
<p>如果问深度学习（神经网络）为什么有效，我一般会这样回答：神经网络通过层层叠加完成了对输入的层层抽象，这个过程某种程度上模拟了人的层次分类做法，从而完成对最终目标的输出，并且具有比较好的泛化能力。的确，神经网络应该是这样做的，然而它并不能告诉我们它确确实实是这样做的，这就是神经网络的难解释性，也就是很多人会将深度学习视为黑箱的原因之一。</p>
<p>让我们来看 Hinton 是怎么来通过 Capsule 突破这一点的。</p>
<h2 id="2-2-大盆菜"><a href="#2-2-大盆菜" class="headerlink" title="2.2 大盆菜"></a>2.2 大盆菜</h2><p>如果要用一道菜来比如 Capsule，我想到了“大盆菜”：</p>
<blockquote>
<p>盆菜作为客家菜的菜式出现由来以久，一般也称为大盘菜，大盘菜源于客家人传统的“发财大盘菜”，顾名思义就是用一个大大的盘子，将食物都放到里面，融汇出一种特有滋味。丰富的材料一层层叠进大盘之中，最易吸收肴汁的材料通常放在下面。吃的时候每桌一盘，一层一层吃下去，汁液交融，味道馥郁而香浓，令人大有渐入佳景之快。</p>
</blockquote>
<p>Capsule 就是针对着这个“层层递进”的目标来设计的，但坦白说，Capsule 论文的文笔真的不敢恭维，因此本文尽量不与论文中的符号相同，以免读者再次云里雾里。让我们来看个图。<br><img src="https://i.imgur.com/Y8T9FED.png" alt=""><br>如图所示，底层的胶囊和高层的胶囊构成一些连接关系。等等，什么是“胶囊”？其实，只要把一个向量当作一个整体来看，它就是一个“胶囊”，是的，你没看错，你可以这样理解：神经元就是标量，胶囊就是向量，就这么粗暴！Hinton 的理解是：每一个胶囊表示一个属性，而胶囊的向量则表示这个属性的“标架”。也就是说，我们以前只是用一个标量表示有没有这个特征（比如有没有羽毛），现在我们用一个向量来表示，不仅仅表示有没有，还表示“有什么样的”（比如有什么颜色、什么纹理的羽毛），如果这样理解，就是说在对单个特征的表达上更丰富了。</p>
<p>说到这里，我感觉有点像 NLP 中的词向量，以前我们只是用 one hot 来表示一个词，也就是表示有没有这个词而已。现在我们用词向量来表示一个词，显然词向量表达的特征更丰富，不仅可以表示有没有，还可以表示哪些词有相近含义。词向量就是 NLP 中的“胶囊”？这个类比可能有点牵强，但我觉得意思已经对了。</p>
<p>那么，这些胶囊要怎么运算，才能体现出“层层抽象”、“层层分类”的特性呢？让我们先看其中一部分连接：<br><img src="https://i.imgur.com/qxWajRQ.png" alt=""></p>
<p>图上只展示了 \(\mu_1\)的连接。这也就是说，目前已经有了\(\mu_1\)这个特征，（假设是羽毛），那么我想知道它属于上层特征\(\nu_{1}，\nu_{2}，\nu_{3}，\nu_{4}\)假设分别代表了鸡、鸭、鱼、狗）中的哪一个。分类问题我们显然已经是很熟悉了，不就是内积后 softmax 吗？于是单靠\(\mu_{1}\)这个特征，我们推导出它是属于鸡、鸭、鱼、狗的概率分别是$$(P_{1|1},P_{2|1},P_{3|1},P_{4|1})=\frac{1}{Z_1}(e^\left \langle \mu1,\nu1  \right \rangle,e^\left \langle \mu1,\nu2 \right \rangle,e^\left \langle \mu1,\nu3  \right \rangle,e^\left \langle \mu1,\nu4  \right \rangle)\tag{1}$$<br>我们当然期望 \(P_{1|1}\)和\(P_{2|1}\)会明显大于 \(P_{3|1}\)和\(P_{4|1}\)。不过，单靠这个特征还不够，我们还需要综合各个特征，于是可以把上述操作对各个 \(\mu_i\)都做一遍，继而得到<br>\((P_{1|2},P_{2|2},P_{3|2},P_{4|2})\)、\((P_{1|3},P_{2|3},P_{3|3},P_{4|3})\)、…<br>问题是，现在得到这么多预测结果，那我究竟要选择哪个呢？而且我又不是真的要做分类，我要的是融合这些特征，构成更高级的特征。于是 Hinton 认为，既然\(\mu_i\)这个特征得到的概率分布是 \((P_{1|i},P_{2|i},P_{3|i},P_{4|i})\),那么我把这个特征切成四份，分别为 \((P_{1|i}\mu_i,P_{2|i}\mu_i,P_{3|i}\mu_i,P_{4|i}\mu_i)\)，然后把这几个特征分别传给 \(\nu_1,\nu_2,\nu_3,\nu_4\),最后\(\nu_1,\nu_2,\nu_3,\nu_4\)其实就是各个底层传入的特征的累加，这样不就好了？<br>$$\nu_j=squash(\sum_{i}p_{j|i}\mu_i) = squash(\sum_i\frac{e^\left \langle \mu_i,\nu_j \right \rangle}{Z_i}\mu_i)\tag{2}$$<br>从上往下看，那么 Capsule 就是每个底层特征分别做分类，然后将分类结果整合。这时 \(\nu_j\)应该尽量与所有\(\mu_i\)都比较靠近，靠近的度量是内积。因此，从下往上看的话，可以认为 \(\nu_j\)实际上就是各个\(\mu_i\)的某个聚类中心，而 Capsule 的核心思想就是输出是输入的某种聚类结果。</p>
<p>现在来看这个 squash 是什么玩意，它怎么来的呢？</p>
<h2 id="2-3-浓缩果汁"><a href="#2-3-浓缩果汁" class="headerlink" title="2.3 浓缩果汁"></a>2.3 浓缩果汁</h2><p>squash 在英文中也有浓缩果汁之意，我们就当它是一杯果汁品尝吧。这杯果汁的出现，是因为 Hinton 希望 Capsule 能有的一个性质是：胶囊的模长能够代表这个特征的概率。</p>
<p>其实我不喜欢概率这个名词，因为概率让我们联想到归一化，而归一化事实上是一件很麻烦的事情。我觉得可以称为是特征的“显著程度”，这就好解释了，模长越大，这个特征越显著。而我们又希望有一个有界的指标来对这个“显著程度”进行衡量，所以就只能对这个模长进行压缩了，所谓“浓缩就是精华”嘛。Hinton 选取的压缩方案是：</p>
<p>$$squash(x) = \frac{\left | x \right |^2}{1+\left | x \right |^2}\frac{x}{\left | x \right |}\tag{3}$$<br>其中\(x/\left | x \right |\)是很好理解的，就是将模长变为 1，那么前半部分怎么理解呢？为什么这样选择？事实上，将模长压缩到 0～1 的方案有很多，并不确定 Hinton 选择目前这个方案的思路。<br>然而，一个值得思考的问题是：如果在中间层，那么这个压缩处理是不是必要的呢？因为已经有了后面说的动态路由在里边，因此即使去掉squash 函数，网络也已经具有了非线性了，因此直觉上并没有必要在中间层也引入特征压缩，正如普通神经网络也不一定要用 sigmoid 函数压缩到 0～1。我觉得这个要在实践中好好检验一下。</p>
<h1 id="3-动态路由"><a href="#3-动态路由" class="headerlink" title="3 动态路由"></a>3 动态路由</h1><p>注意到(2)式，为了求\(\nu_j\) 需要求 softmax，可是为了求 softmax 又需要知道 \(\nu_j\)，这不是个鸡生蛋、蛋生鸡的问题了吗？这时候就要上“主菜”了，即“动态路由”（Dynamic Routing），它能够根据自身的特性来更新（部分）参数，从而初步达到了 Hinton 的放弃梯度下降的目标。</p>
<p>这道“主菜”究竟是不是这样的呢？它是怎么想出来的？最终收敛到哪里去？让我们先上两道小菜，然后再慢慢来品尝这道主菜。</p>
<h2 id="3-1-小菜-1"><a href="#3-1-小菜-1" class="headerlink" title="3.1 小菜 1"></a>3.1 小菜 1</h2><p>让我们先回到普通的神经网络，大家知道，激活函数在神经网络中的地位是举足轻重的。当然，激活函数本身很简单，比如一个 tanh 激活的全连接层，用 tensorflow 写起来就是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = tf.matmul(W, x) + b</span><br><span class="line">y = tf.tanh(y)</span><br></pre></td></tr></table></figure></p>
<p>可是，如果我想用 \(x = y + cos(y)\)的反函数来激活呢？也就是说，你得给我解出 \(y=f(x)\)，然后再用它来做激活函数。</p>
<p>然而数学家告诉我们，这个东西的反函数是一个超越函数，也就是不可能用初等函数有限地表示出来。那这样不就是故意刁难么？不要紧，我们有迭代：$$y_n+1=x - cos y_n$$<br>选择y0=x，代入上式迭代几次，基本上就可以得到比较准确的y 了。假如迭代三次，那就是\(y = x - cos(x - cos( x - cosx))\)用 tensorflow 写出来就是<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y = tf.matmul(W, x) + b</span><br><span class="line">Y = y</span><br><span class="line">for i in range(3):</span><br><span class="line">	Y = y - tf.cos(Y)</span><br></pre></td></tr></table></figure></p>
<p>如果读者已经“预习”过 Capsule，那么就会发现这跟 Capsule 的动态路由很像。</p>
<h2 id="3-2-小菜-2"><a href="#3-2-小菜-2" class="headerlink" title="3.2 小菜 2"></a>3.2 小菜 2</h2><p>再来看一个例子，这个例子可能在 NLP 中有很多对应的情景，但图像领域其实也不少。考虑一个向量序列 \(x_1,x_2,…x_n\)，我现在要想办法将这 n个向量整合成一个向量 x(encoder)，然后用这个向量来做分类。</p>
<p>也许读者会想到用 LSTM。但我这里仅仅想要将它表示为原来向量的线性组合，也就是：<br>$$ x = \sum ^{n}_{i=1}\lambda_ix_i$$<br>这里的\(\lambda_i\)相当于衡量\(x\)与\(x_i\)的相似度。然而问题来了，在 x 出现之前，凭什么能够确定这个相似度呢？这不也是一个鸡生蛋、蛋生鸡的问题吗？解决这个问题的一个方案也是迭代。首先我们也可以定义一个基于 softmax 的相似度指标，然后让<br>$$x = \sum ^n_{i=1}\frac{e^\left \langle x,x_i \right \rangle}{Z}x_i$$<br>一开始，我们一无所知，所以只好取 x为各个 \(x_i\) 的均值，然后代入右边就可以算出一个，再把它代入右边，反复迭代就行，一般迭代有限次就可以收敛，于是就可以将这个迭代过程嵌入到神经网络中了。</p>
<p>如果说小菜 1 跟动态路由只是神似，那么小菜 2 已经跟动态路由是神似＋形似了。不过我并没有看到已有的工作是这样做的，这个小菜只是我的头脑风暴。</p>
<h2 id="3-3-上主菜～"><a href="#3-3-上主菜～" class="headerlink" title="3.3 上主菜～"></a>3.3 上主菜～</h2><p>其实有了这两个小菜，动态路由这道主菜根本就不神秘了。为了得到各个 \(\nu_j\)，一开始先让它们全都等于\(\mu_i\)的均值，然后反复迭代就好。说白了，输出是输入的聚类结果，而聚类通常都需要迭代算法，这个迭代算法就称为“动态路由”。至于这个动态路由的细节，其实是不固定的，取决于聚类的算法，比如关于 Capsule 的新文章《MATRIX CAPSULES WITH EM ROUTING》就使用了 Gaussian Mixture Model 来聚类。</p>
<p>理解到这里，就可以写出本文的动态路由的算法了：</p>
<blockquote>
<p>动态路由算法<br>初始化\(b_{ij}=0\)<br>迭代r次：<br>  \(c_i = softmax(b_j)\)<br>  \(s_j = \sum _ic_{ij}\mu_i\)<br>  \(\nu_j = squash(s_j)\)<br>  \(b_{ij} = b_{ij}+\left \langle \mu_i,\nu_j \right \rangle\)<br>返回\(\nu_j\)</p>
</blockquote>
<p>这里用的原文中的算法，苏老师根据自己的理解进行了改写，可以参考其博客。</p>
<h1 id="4-模型细节"><a href="#4-模型细节" class="headerlink" title="4 模型细节"></a>4 模型细节</h1><p>下面介绍 Capsule 实现的细节</p>
<h2 id="4-1-全连接版"><a href="#4-1-全连接版" class="headerlink" title="4.1 全连接版"></a>4.1 全连接版</h2><p>先不管是 Hinton 版还是我的版本，按照这个动态路由的算法，<br>\(\nu_j\)能够迭代地算出来，那不就没有参数了吗？真的抛弃了反向传播了？</p>
<p>非也非也～如果真的这样的话，各个 \(\nu_j\) 都一样了。前面已经说了，\(\nu_j\)是作为输入 \(\mu_i\)的某种聚类中心出现的，而从不同角度看输入，得到的聚类结果显然是不一样的。那么为了实现“多角度看特征”，于是可以在每个胶囊传入下一个胶囊之前，都要先乘上一个矩阵做变换，所以 (2)式实际上应该要变为<br>$$\nu_j = squash(\sum_i \frac{e ^\left \langle \hat{\mu}_{j|i},\nu_j \right \rangle}{Z_i}\hat{\mu}{j|i}),\hat{\mu}{j|i} = W_{ji}\mu_i\tag{4}$$<br>这里的 \(W_{ji}\)是待训练的矩阵，这里的乘法是矩阵乘法，也就是矩阵乘以向量。所以，Capsule 变成了下图<img src="https://i.imgur.com/u30irii.png" alt=""></p>
<blockquote>
<p>这时候就可以得到完整动态路由了</p>
</blockquote>
<blockquote>
<p>动态路由算法<br>初始化\(b_{ij}=0\)<br>迭代r次：<br>  \(c_i = softmax(b_j)\)<br>  \(s_j = \sum_ic_{ij}\hat{\mu}_{j|i}\)<br>  \(\nu_j = squash(s_j)\)<br>  \(b_{ij} = b_{ij}+\left \langle \hat{\mu}_{j|i},\nu_j \right \rangle\)<br>返回\(\nu_j\)<br>这样的 Capsule 层，显然相当于普通神经网络中的全连接层。</p>
</blockquote>
<h2 id="4-2-共享版"><a href="#4-2-共享版" class="headerlink" title="4.2 共享版"></a>4.2 共享版</h2><p>众所周知，全连接层只能处理定长输入，全连接版的 Capsule 也不例外。而 CNN 处理的图像大小通常是不定的，提取的特征数目就不定了，这种情形下，全连接层的 Capsule 就不适用了。因为在前一图就可以看到，参数矩阵的个数等于输入输入胶囊数目乘以输出胶囊数目，既然输入数目不固定，那么就不能用全连接了。</p>
<p>所以跟 CNN 的权值共享一样，我们也需要一个权值共享版的 Capsule。所谓共享版，是指对于固定的上层胶囊j，它与所有的底层胶囊的连接的变换矩阵是共用的，即\(W_{ji} = W_{j}\)，</p>
<p>如图所示，共享版其实不难理解，就是自下而上地看，就是所有输入向量经过同一个矩阵进行映射后，完成聚类进行输出，将这个过程重复几次，就输出几个向量（胶囊）；又或者自上而下地看，将每个变换矩阵看成是上层胶囊的识别器，上层胶囊通过这个矩阵来识别出底层胶囊是不是有这个特征。因此很明显，这个版本的胶囊的参数量并不依赖于输入的胶囊个数，因此可以轻松接在 CNN 后面。对于共享版，(2)式要变为<br>$$\nu_j = squash(\sum_i \frac{e ^\left \langle \hat{\mu}_{j|i},\nu_j \right \rangle}{Z_i}\hat{\mu}{j|i}),\hat{\mu}{j|i} = W_{ji}\mu_i\tag{5}$$<br>至于动态路由算法就没有改变了。</p>
<h2 id="4-3-反向传播"><a href="#4-3-反向传播" class="headerlink" title="4.3 反向传播"></a>4.3 反向传播</h2><p>尽管我不是很喜欢反向传播这个名词，然而这里似乎不得不用上这个名字了。</p>
<p>现在又有了 \(W_{ji}\)，那么这些参数怎么训练呢？答案是反向传播。读者也许比较晕的是：现在既有动态路由，又有反向传播了，究竟两者怎么配合？其实这个真的就最简单不过了。就好像“小菜 1”那样，把算法的迭代几步（论文中是 3 步），加入到模型中，从形式上来看，就是往模型中添加了三层罢了，剩下的该做什么还是什么，最后构建一个 loss 来反向传播。</p>
<p>这样看来，Capsule 里边不仅有反向传播，而且只有反向传播，因为动态路由已经作为了模型的一部分，都不算在迭代算法里边了。</p>
<h2 id="4-4-做了什么"><a href="#4-4-做了什么" class="headerlink" title="4.4 做了什么"></a>4.4 做了什么</h2><p>是时候回顾一下了，Capsule 究竟做了什么？其实用一种最直接的方式来讲，Capsule 就是提供了一种新的“vector in vector out”的方案，这样看跟 CNN、RNN、Attention 层都没太大区别了；从 Hinton 的本意看，就是提供了一种新的、基于聚类思想来代替池化完成特征的整合的方案，这种新方案的特征表达能力更加强大。</p>
<p>本文转自苏老师：<br><a href="https://kexue.fm/archives/4819/" target="_blank" rel="noopener">https://kexue.fm/archives/4819/</a></p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/06/22/capsnet/">
    <time datetime="2018-06-22T02:21:37.000Z" class="entry-date">
        2018-06-22
    </time>
</a>
    
    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/胶囊网络/">胶囊网络</a></li></ul>

    </footer>
</article>






  
    <article id="post-kmaxpooling" class="post-kmaxpooling post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2018/06/19/kmaxpooling/">k-max-pooling的keras实现</a>
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://xurui.club/2018/06/19/kmaxpooling/" data-id="cjyco81zw000gq4daci4yfly8" class="leave-reply bdsharebuttonbox" data-cmd="more">分享</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>pooling（池化）是卷积神经网络中常用操作，主要目的是减小张量大小，便于计算，可以认为是一步降维操作。前两天项目上需要用到k-max-pooling操作，项目代码是用keras写的，keras没有k-max-pooling层，所以要自己写。</p>
<p>卷积神经网络的最经典的模型是卷积+池化+全连接。<br>其中卷积操作通过一个滑动的卷积窗口来强化提取特征，可以认为卷积窗是一个权重矩阵，而卷积操作中的每一步，都是一个矩阵的加权求和，随着卷积窗口滑动，每一步的和被记录下来，形成一个新的矩阵。我们有多少种卷积窗，经过一层卷积就留下几个矩阵。然而，如果经过多层卷积，那最后矩阵就越来越大，那我们对资源耗费太大，有没有什么好的操作呢？–pooling</p>
<p>简单理解，pooling就是把一个大矩阵，变成小矩阵。如max-pooling，如size=2,就是在原来的矩阵中，每2*2个块中，取其中最大的一个记录下来，然后下一个块，再下一个十几个，最后记录下来的值就成了一个新矩阵，而这个新矩阵的大小，比以前缩小了4倍。有人会问，那不是损失了信息了么？当然损失了信息，但是我们认为损失的信息并没那么重要，就像一幅超清图，和一张高清图，虽然没那么清晰了，但这张图所描述的信息我们还是一眼能看出来。max-pooling操作，可以这么理解，给你一张图片，你分成了好多块，每块里面，别的地方颜色很浅，就一个地方颜色很深，那你是不是第一眼先看到那个黑点呢？</p>
<p>再说一下mean-pooling.<br>和max-pooling类似，只不过把每块中取最大值的操作，换成了对每块取均值。这样就会把每个点的值都考虑到，没有max-pooling操作那样粗暴。</p>
<p>还有一种常用的操作是k-max-pooling.这种是在max-pooling上改进来的，因为max-pooling操作太简单粗暴了，k-max-pooling认为每一块不只一个点重要，前几个亮点都比较重要，所以在每一个pooling块中取了前k大的值。</p>
<p>keras中pooling操作好像只有maxpooling和meanpooing.于是自己写了个代码，实现k-max-pooling。<br>因为项目中用的是LSTM处理文本，我这里就对LSTM层的结果进行处理了，LSTM层输出三维张量(shape=[in_dim1,in_dim2,in_dim3]),其中in_dim1表示样本数量，in_dim2表示一个样本（也就是一段话）的长度（如：这里认为一句话有100个词，那么短于100的句子后面补0，长于100的句子后面截断），in_dim3为lstm结点数量，也就是每个step输出的向量长度。<br>k-max-pooling操作，就是在dim2中，取出k个最大值，可以想象是取出k个最重要的词。<br>输入为LSTM层输出的三维张量，输出为一个二维张量，shape=[out_dim1,out_dim2],out_dim1为样本数，out_dim2为in_dim3*k。</p>
<p>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test59</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> keras.layers <span class="keyword">import</span> Lambda, Input</span><br><span class="line">    <span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line">    <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">    l = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">11</span>, <span class="number">22</span>, <span class="number">33</span>, <span class="number">44</span>, <span class="number">55</span>, <span class="number">66</span>, <span class="number">77</span>, <span class="number">88</span>, <span class="number">99</span>]]</span><br><span class="line">    data = np.reshape(l, [<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">    print(data)</span><br><span class="line"><span class="comment">###output###</span></span><br><span class="line"><span class="comment">#[[[ 1  2  3]</span></span><br><span class="line"><span class="comment">#  [ 4  5  6]</span></span><br><span class="line"><span class="comment">#  [ 7  8  9]]</span></span><br><span class="line"><span class="comment"># [[11 22 33]</span></span><br><span class="line"><span class="comment">#  [44 55 66]</span></span><br><span class="line"><span class="comment">#  [77 88 99]]]</span></span><br><span class="line"><span class="comment">#数据为2个样本，每个样本是3个step（可以认为是句子有3个词），每个step是一个长为3的向量（可以认为词向量长度为3）</span></span><br><span class="line">    input = Input(shape=[<span class="number">3</span>, <span class="number">3</span>], dtype=<span class="string">'int32'</span>)</span><br><span class="line">    la = Lambda(<span class="keyword">lambda</span> x: tf.reshape(tf.nn.top_k(tf.transpose(x,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>]),k=<span class="number">2</span>)[<span class="number">0</span>],shape=[<span class="number">-1</span>,<span class="number">6</span>]))(input)</span><br><span class="line">    model = Model(inputs=input, outputs=la)</span><br><span class="line">    pre = model.predict(data)</span><br><span class="line">    print(pre)</span><br><span class="line"><span class="comment">#[[ 7  4  8  5  9  6]</span></span><br><span class="line"><span class="comment">#[77 44 88 55 99 66]]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    test59()</span><br></pre></td></tr></table></figure></p>
<p>这里的k-max-pooling用Lambda写的，<code>tf.reshape(tf.nn.top_k(tf.transpose(x,[0,2,1]),k=2)[0],shape=[-1,6])</code>，分解成三步：<br><code>tf.transpose(x,[0,2,1]</code>–&gt;因为tf有个top_k方法，可以对取张量最后一维的前k大的数，所以我们把step所在维度调整到最后。transpose是一个转置操作。<br><code>tf.nn.top_k（，k=2）</code>–&gt;取出最后一个维度前k大的数。<br><code>tf.reshape(),top_k</code>操作对每个样本取出的结果是一个二维矩阵in_dim3 × k，所以把它转成向量，向量长度为in_dim3*k</p>
<p>当然，也可以写一个自定义层，处理步骤类似</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KMaxPooling</span><span class="params">(Layer)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    k-max-pooling</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k=<span class="number">1</span>, **kwargs)</span>:</span></span><br><span class="line">        super().__init__(**kwargs)</span><br><span class="line">        self.input_spec = InputSpec(ndim=<span class="number">3</span>)</span><br><span class="line">        self.k = k</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (input_shape[<span class="number">0</span>], (input_shape[<span class="number">2</span>] * self.k))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        <span class="comment"># swap last two dimensions since top_k will be applied along the last dimension</span></span><br><span class="line">        shifted_input = tf.transpose(inputs, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># extract top_k, returns two tensors [values, indices]</span></span><br><span class="line">        top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=<span class="keyword">True</span>, name=<span class="keyword">None</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># return flattened output</span></span><br><span class="line">        <span class="keyword">return</span> Flatten()(top_k)</span><br></pre></td></tr></table></figure>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/06/19/kmaxpooling/">
    <time datetime="2018-06-19T12:58:32.000Z" class="entry-date">
        2018-06-19
    </time>
</a>
    
    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Keras/">Keras</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>

    </footer>
</article>






  
    <article id="post-interview" class="post-interview post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2018/06/11/interview/">n阶乘后面有多少个0</a>
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://xurui.club/2018/06/11/interview/" data-id="cjyco81zn0009q4daq9x0tmsa" class="leave-reply bdsharebuttonbox" data-cmd="more">分享</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>开始整理下面试题和心得，这一部分包含Lintcode和平时遇到的一些题目。</p>
<p>题目描述：<br>给定一个正整数n,返回出n的阶乘尾部0的个数。linkcode链接：<br><a href="https://lintcode.com/problem/trailing-zeros/description" target="_blank" rel="noopener">https://lintcode.com/problem/trailing-zeros/description</a></p>
<p>分析：<br>看到这一题第一感觉先求阶乘，然后求出结果中尾部有多少0。<br>提交代码，超时。</p>
<p>于是查了下资料，一个数尾部有多少个0，先看这个数的因子，也就是因子分解，如10 分解为2 5，20 分解为2 2 5，100 分解为 2 2 5 5，多找几组数据，发现尾部0的个数为min{2的数，5的个数}。为什么会这样呢？因为尾部的0是由2<em>5得出的，当然也可以是4</em>5，但是4=2*2呀，最终0还是由2和5造出来的。一对2和5造出来一个0，n对2和5造出n个0，如果有10个2，8个5，则只能凑出8对，所以是min{…}.<br>阶乘也就是因子分解的过程，而且因子从小到大排好了。<br>2的个数肯定比5多，因为5大。<br>所以这个题，看因子中5出现的次数就可以啦。</p>
<p>这里n阶乘用n!表示。<br>5！  –&gt; 1 2 3 4 5 有一个5<br>10!  –&gt; 1 2 3 4 5 6 7 8 9 10 有2个5（10=2<em>5）<br>…<br>25!  –&gt; 1 2 3 4 5 …10 ..15 ..20 ..24 25 有6个5（25=5</em>5）</p>
<p>125！ = 5 10 15 …25 ..50 ..75 ..100 ..125 有多少呢？<br>我们发现每隔5个数出现一个5，隔25个数多出来1个5..隔125个数再多出一个5<br>也就是从n个抽出5<br>5 10 15 20 25 30 … = 5（<em>1 2 3 4 5 ）<br>然后从上面序列中再隔5抽<br>25 50 75 100 125 … = 5</em>5<em>（1 2 3 4 5 ）<br>再隔5抽<br>125 250 375 500 625 .. = 5</em>5*5（1 2 3 4 5 ）</p>
<p>规律很明显了</p>
<p>好了上代码 python3</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    @param: n: An integer</span><br><span class="line">    @return: An integer, denote the number of trailing zeros in n!</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def trailingZeros(self, n):</span><br><span class="line">        zeros = 0</span><br><span class="line">        while n &gt; 0:</span><br><span class="line">            zeros += n // 5</span><br><span class="line">            n //= 5</span><br><span class="line">        return int(zeros)</span><br></pre></td></tr></table></figure>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/06/11/interview/">
    <time datetime="2018-06-11T02:34:55.000Z" class="entry-date">
        2018-06-11
    </time>
</a>
    
    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/面试/">面试</a></li></ul>

    </footer>
</article>






  
    <article id="post-python-1" class="post-python-1 post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2018/06/05/python-1/">python数据存储利器之pickle</a>
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://xurui.club/2018/06/05/python-1/" data-id="cjyco8208000rq4daxonhme3f" class="leave-reply bdsharebuttonbox" data-cmd="more">分享</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>第一天上班那天，有位同事把部分工作交接给我，主要是处理过的一些数据，存的是txt格式。然后大概看了下，还挺整齐，用’,’分隔的。读进去才发现，本来应该是个List,然而由于存成了txt，所有的都变成了str。所以转来转去，费了两天时间。突然想起来好像有pickle这么个小玩意，就拿来试试。从此，再也不想用csv啥的存数据了</p>
<p>简单介绍下Pickle<br>pickle使用二进制协议完成对python对象的序列化和反序列化。“picking”是将python对象转为字节流，而“unpickling”则是从字节流中转出来。<br>也就是说，无论你要存的内容是字符串、数字、数组、列表、矩阵、DataFrame、np.array()等等，当你进行picking操作时，统统将这些内容转换成字节流保存到文件中，一般是二进制文件。然后当你想使用这些内容时，再从二进制文件中读出来。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br></pre></td></tr></table></figure></p>
<p>–picking–<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = ...</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'name.txt'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(data,f,protocol=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure></p>
<p>这样就把数据写进name.txt文件里了，注意这里是二进制，你用记事本打开是乱码。有个参数，protocol,是你选用的协议，默认为3，在python2中默认为0，如果你在python3用dump写进文件，去python2里load读取文件,会报异常。当然这种奇怪的需求很少，如果遇到了，把dump里的<code>protocol=0</code>就可以了。<br>当然，你的data可以有多个<br>如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a= <span class="number">1</span></span><br><span class="line">b =<span class="number">3</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'name.txt'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump([a,b],f)</span><br></pre></td></tr></table></figure></p>
<p>–unpicking–<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'name.txt'</span>,<span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = pickle.load(f)</span><br></pre></td></tr></table></figure></p>
<p>如果你存的数据是多个<br>那么：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'name.txt'</span>,<span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data= pickle.load(f)</span><br><span class="line">    a,b = data</span><br></pre></td></tr></table></figure></p>
<p>还有除了dump和load，还有dumps和loads。<br>dumps()和loads()用的不太多，简单说下，和dump类似，dumps把数据写成字节对象，但不用保存到文件。loads则是从字节对象中读出，不用读文件。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/06/05/python-1/">
    <time datetime="2018-06-05T01:52:11.000Z" class="entry-date">
        2018-06-05
    </time>
</a>
    
    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

    </footer>
</article>






  
    <article id="post-lda" class="post-lda post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2018/06/01/lda/">lda主题模型python实现篇</a>
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://xurui.club/2018/06/01/lda/" data-id="cjyco81zz000iq4dap2n0x9v0" class="leave-reply bdsharebuttonbox" data-cmd="more">分享</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>最近在做一个动因分析的项目，自然想到了主题模型LDA。这次先把模型流程说下，原理后面再讲。<br>lda实现有很多开源库，这里用的是gensim.</p>
<h2 id="1-文本预处理"><a href="#1-文本预处理" class="headerlink" title="1 文本预处理"></a>1 文本预处理</h2><p>大概说下文本的样子，LDA是无监督模型，也就是说不需要标签，只要传入文本就好。LDA要学习文档-主题分布和主题-词分布，所以我们把一个人的数据join在一起作为一条文档。对文档进行分词，使用的jieba分词工具包。注意，这里要做去停用词处理，包括标点和一些没用的词，如“呵呵”，“哈哈”。做项目时，第一版没有去无用词，最后提出的主题都是“你”“我”“他”“你好”这样的东西，去掉之后可以较好提高结果质量。</p>
<h2 id="2-将上步处理的结果进行格式化表示"><a href="#2-将上步处理的结果进行格式化表示" class="headerlink" title="2 将上步处理的结果进行格式化表示"></a>2 将上步处理的结果进行格式化表示</h2><p>即将所有文档数表示成m*n的矩阵D,m表示有m篇文档，n表示这篇文档有n个词，n不定长。</p>
<h2 id="3-生成词典"><a href="#3-生成词典" class="headerlink" title="3 生成词典"></a>3 生成词典</h2><p>用gensim.corpora.Dictionary包<br>这个包讲下吧<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from gensim.corpora import Dictionary</span><br><span class="line">text = [[&apos;我&apos;, &apos;想吃&apos;, &apos;大龙虾&apos;, &apos;和&apos;, &apos;烤猪蹄&apos;]]</span><br><span class="line">dictionary = Dictionary(text)</span><br><span class="line">print((dictionary))</span><br><span class="line">doc = dictionary.doc2bow([&apos;我&apos;, &apos;想吃&apos;, &apos;大龙虾&apos;, &apos;和&apos;, &apos;我&apos;,&apos;你&apos;,&apos;烤猪蹄&apos;])</span><br><span class="line">print(doc)</span><br><span class="line"></span><br><span class="line">#####output#####</span><br><span class="line">Dictionary(5 unique tokens: [&apos;我&apos;, &apos;大龙虾&apos;, &apos;想吃&apos;, &apos;和&apos;, &apos;烤猪蹄&apos;])</span><br><span class="line">[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1)]</span><br></pre></td></tr></table></figure></p>
<p>可以看出，我们把我想吃大龙虾和烤猪蹄编成字典，共五个词，这里输出结果里看不出是个字典，其实是有下标的，0对应我，1对应大龙虾，…4对应烤猪蹄。<br>然后我们在下一步将文本变成词袋，这里用的文本是[‘我’, ‘想吃’, ‘大龙虾’, ‘和’, ‘我’,’你’,’烤猪蹄’],注意文本格式也是词为元素的列表。这句话是我自己构造的，只是为了说两点，语法请忽略。第一点：“我”这个词出现了两次，所以下标为2的地方，值为2；第二点，“你”这个词出现了1次，可是在词典中没有，所有直接被忽略。 </p>
<p>这样就可以用字典，将文本表示成词袋模型，词袋模型不懂的，见我另一篇文章，自然语言处理NLP的词如何表示。当我们做完了LDA模型后，对于新的文本，我们想看下它所在的主题分布，就要使用该字典再进行词袋编码，也就是说这个字典，我们以后也会用到，所以，我们在这里把词典保存起来。<br>保存词典可以用pickle,很好用。不懂的见我另一篇文章，神奇的pickle。</p>
<h2 id="4-训练LDA模型"><a href="#4-训练LDA模型" class="headerlink" title="4 训练LDA模型"></a>4 训练LDA模型</h2><p>这里用的是gensim.models.ldamodel包<br><code>ldamodel = LdaModel(text, num_topics=10, id2word=dictionary, passes=20)</code><br>使用这句话就可以直接训练LDA模型了，讲一下参数吧。<br>text:文本，已经表示成词袋了。<br>num_topics: 提取的主题数<br>id2word:词典<br>passes:类似于在机器学习中常见的epoch，也就是训练了多少轮。</p>
<p>然后我们得到了训练好的ldamodel.用这个模型可以做哪些事情呢？</p>
<h2 id="5-ldamodel使用"><a href="#5-ldamodel使用" class="headerlink" title="5 ldamodel使用"></a>5 ldamodel使用</h2><p>可以输出这个模型的各个主题下的主题词<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">print(ldamodel.print_topics(num_topics=10, num_words=10))</span><br><span class="line">###output###</span><br><span class="line">[(0, &apos;0.015*&quot;说&quot; + 0.011*&quot;吃&quot; + 0.008*&quot;想&quot; + 0.007*&quot;睡&quot; + 0.005*&quot;\u2005&quot; + 0.005*&quot;做&quot; + 0.005*&quot;明天&quot; + 0.005*&quot;买&quot; + 0.005*&quot;干嘛&quot; + 0.005*&quot;玩&quot;&apos;),</span><br><span class="line"> (1, &apos;0.017*&quot; &quot; + 0.010*&quot;说&quot; + 0.004*&quot;吃&quot; + 0.004*&quot;许华升&quot; + 0.004*&quot;\x14&quot; + 0.004*&quot;想&quot; + 0.003*&quot;做&quot; + 0.003*&quot;买&quot; + 0.003*&quot;ÿ&quot; + 0.003*&quot;钱&quot;&apos;),</span><br><span class="line"> (2, &apos;0.008*&quot;com&quot; + 0.007*&quot; &quot; + 0.004*&quot;手机&quot; + 0.003*&quot;女&quot; + 0.003*&quot;说&quot; + 0.003*&quot;www&quot; + 0.003*&quot;cc&quot; + 0.002*&quot;号&quot; + 0.002*&quot;qq&quot; + 0.002*&quot;视频&quot;&apos;), </span><br><span class="line"> (3, &apos;0.007*&quot;com&quot; + 0.006*&quot; &quot; + 0.005*&quot;38&quot; + 0.004*&quot;号&quot; + 0.004*&quot;贷&quot; + 0.003*&quot;3000&quot; + 0.003*&quot;10&quot; + 0.003*&quot;做&quot; + 0.003*&quot;说&quot; + 0.002*&quot;111&quot;&apos;), </span><br><span class="line"> (4, &apos;0.017*&quot; &quot; + 0.007*&quot;说&quot; + 0.006*&quot;做&quot; + 0.005*&quot;你好&quot; + 0.005*&quot;吃&quot; + 0.004*&quot;号&quot; + 0.004*&quot;\u2005&quot; + 0.004*&quot;想&quot; + 0.003*&quot;钱&quot; + 0.003*&quot;明天&quot;&apos;), </span><br><span class="line"> (5, &apos;0.013*&quot; &quot; + 0.012*&quot;说&quot; + 0.007*&quot;吃&quot; + 0.006*&quot;想&quot; + 0.005*&quot;睡&quot; + 0.005*&quot;做&quot; + 0.004*&quot;钱&quot; + 0.004*&quot;买&quot; + 0.004*&quot;回来&quot; + 0.004*&quot;干嘛&quot;&apos;), </span><br><span class="line"> (6, &apos;0.010*&quot; &quot; + 0.005*&quot;买大单&quot; + 0.005*&quot;贷&quot; + 0.004*&quot;说&quot; + 0.004*&quot;贷款&quot; + 0.004*&quot;钱&quot; + 0.003*&quot;com&quot; + 0.003*&quot;号&quot; + 0.003*&quot;奥特曼&quot; + 0.003*&quot;吃&quot;&apos;),</span><br><span class="line"> (7, &apos;0.022*&quot; &quot; + 0.010*&quot;说&quot; + 0.008*&quot;做&quot; + 0.007*&quot;吃&quot; + 0.005*&quot;想&quot; + 0.004*&quot;钱&quot; + 0.003*&quot;\u2005&quot; + 0.003*&quot;买&quot; + 0.003*&quot;谢谢&quot; + 0.003*&quot;明天&quot;&apos;),</span><br><span class="line"> (8, &apos;0.017*&quot; &quot; + 0.015*&quot;com&quot; + 0.006*&quot;www&quot; + 0.005*&quot;说&quot; + 0.004*&quot;号&quot; + 0.004*&quot;\u2005&quot; + 0.003*&quot;手机&quot; + 0.003*&quot;钱&quot; + 0.003*&quot;吃&quot; + 0.003*&quot;https&quot;&apos;), </span><br><span class="line"> (9, &apos;0.011*&quot;\u2005&quot; + 0.011*&quot;说&quot; + 0.010*&quot;做&quot; + 0.009*&quot; &quot; + 0.004*&quot;钱&quot; + 0.004*&quot;买&quot; + 0.004*&quot;发&quot; + 0.003*&quot;谢谢&quot; + 0.003*&quot;吃&quot; + 0.003*&quot;玩&quot;&apos;)]</span><br></pre></td></tr></table></figure></p>
<p>这里随便找了些数据，效果不是太明显，这里主要讲处理流程，不要被这结果干扰心情，不过工业应用中很多时候，实际结果和你理想的结果有很大差距。用一些正常的数据，是可以看出一些信息的。上次用汽车之家的评论数据做lda，主题信息就比较明显，有关于油耗的，有关于买车的等等。<br>也可以对新文本，找出其所在的主题分布。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def to_lda_vec(model, dictionary, text_list=None):</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    :param model: lda model</span><br><span class="line">    :param dictionary: Dictionary for toBow</span><br><span class="line">    :param text_list: texts</span><br><span class="line">    :return: texts about one topic</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    lda_list = []</span><br><span class="line">    for texts in text_list:</span><br><span class="line">        doc_bow = dictionary.doc2bow(texts)</span><br><span class="line">        doc_lda = model[doc_bow]</span><br><span class="line">	lda_list.append(doc_lda)</span><br><span class="line"></span><br><span class="line">    return lda_list</span><br></pre></td></tr></table></figure></p>
<p>这个方法中的参数加了注释，这里可以看到有个参数是dictionary，这里就是我们前面训练lda时用的词典，前面保存的词典派上用场了。最后输出的lda_list是一个列表，列表中元素为每句话的doc_lda,doc_lda是这样子的[(5,0.342345),(6,0.1111)…]，也就是个list，无素为元组，元组包括两个值，第一个值表示主题id，第二个值表示属于该主题的概率。</p>
<p>也可以用于新文本数据的向量化，即将新的文本映射成主题向量，然后可以做分类，做聚类，做推荐。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/06/01/lda/">
    <time datetime="2018-06-01T06:06:35.000Z" class="entry-date">
        2018-06-01
    </time>
</a>
    
    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/lda/">lda</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

    </footer>
</article>






  
    <article id="post-python" class="post-python post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2018/05/23/python/">python之list.count()和Count()类</a>
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://xurui.club/2018/05/23/python/" data-id="cjyco8209000tq4daljgb4d2p" class="leave-reply bdsharebuttonbox" data-cmd="more">分享</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>Count()类的使用<br>同事给了我一堆文本数据，让我帮个小忙。他想统计下每个词的词频，看看文本中提到最多的是什么，然后做后面分析。</p>
<p>不就是统计词频吗，虽然之前不经常做这个。但是拍脑袋一想，先分词，去停用词，把所有词放到一个列表里，统计，搞定。</p>
<p>于是五分钟写了个代码，计数那个地方，我用的List里的count方法。不怕丢人，我把代码放这了。。。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">count_word = []</span><br><span class="line">stop_words = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data/count.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    lines = f.readlines()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data/stop_words_new.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    stop_lines = f.readlines()</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> stop_lines:</span><br><span class="line">    stop_words.append(word.strip())</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">    words = jieba.lcut(line)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> stop_words:</span><br><span class="line">            count_word.append(i)</span><br><span class="line">count_word_set = list(set(count_word))</span><br><span class="line">counts = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> count_word_set:</span><br><span class="line">    counts.append(count_word.count(i))</span><br><span class="line">print(len(count_word_set))</span><br><span class="line">print(len(counts))</span><br><span class="line">df = pd.DataFrame(data=&#123;<span class="string">'word'</span>: count_word_set, <span class="string">'count'</span>: counts&#125;)</span><br><span class="line">df.sort_values(by=<span class="string">'count'</span>)</span><br><span class="line">df.to_csv(<span class="string">'data/count.txt'</span>, sep=<span class="string">'\t'</span>, encoding=<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure></p>
<p>然而，等了一晚上，发现没出来结果。他说数据量不大，几百万条文本。我这里用的set去重，然后遍历set中的每个词，再用list.count(词)来得词频，笨方法行不通。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">所以今天试了Count()类。</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">count_word = []</span><br><span class="line">count = Counter()</span><br><span class="line">stop_words = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data/count.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    lines = f.readlines()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data/stop_words_new.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    stop_lines = f.readlines()</span><br><span class="line">print(<span class="string">'data finish'</span>)</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> stop_lines:</span><br><span class="line">    stop_words.append(word.strip())</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">    words = jieba.lcut(line)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> stop_words:</span><br><span class="line">            count_word.append(i)</span><br><span class="line">print(<span class="string">'cut finish'</span>)</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> count_word:</span><br><span class="line">    count[word] += <span class="number">1</span></span><br><span class="line">cou = count.most_common()</span><br><span class="line">print(cou)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data/common.txt'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(cou, f)</span><br></pre></td></tr></table></figure></p>
<p>分完词就结束了。。如果只是想看某个词的词频，用list的count()方法还好，如果统计所有的，还是用Count()类吧。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/05/23/python/">
    <time datetime="2018-05-23T07:53:42.000Z" class="entry-date">
        2018-05-23
    </time>
</a>
    
    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

    </footer>
</article>






  
    <article id="post-java" class="post-java post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2018/05/22/java/">java异常之java.sql.SQLException Illegal mix of collations (utf8_general_ci</a>
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://xurui.club/2018/05/22/java/" data-id="cjyco81zq000bq4da2s7ydgh6" class="leave-reply bdsharebuttonbox" data-cmd="more">分享</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>之前的服务器重装了，所以项目代码重新布置了。明明之前能跑的程序，在我PC机上也是能跑的，放到服务器下就报了这个异常。<br>java.sql.SQLException: Illegal mix of collations (utf8_general_ci,IMPLICIT) and (utf8_general_ci,COERCIBLE) for operation ‘=’<br>跟据异常信息，大致可以理解为是字符集问题。</p>
<p>后来查了下原因，我的数据库使用的mysql建的，字符集为utf-8，数据库排序规则为utf8_unicode_ci.数据库的表是通过hibernate自动生成的，但是少了一个人事部的基本信息表，所以后来把人事表导进来，但是人事表的排序规则用的是unicode_general_ci，正好是异常提示中的内容。</p>
<p>查明了原因，我把数据库删了，因为服务器刚布好，还没投入使用，库内无数据，如果使用过程中出现这种问题，就改那个捣乱的表。当然，系统在使用过程中，也不可能出现这个问题。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/05/22/java/">
    <time datetime="2018-05-22T12:02:04.000Z" class="entry-date">
        2018-05-22
    </time>
</a>
    
    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/java/">java</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/异常解决/">异常解决</a></li></ul>

    </footer>
</article>






  
    <article id="post-python1" class="post-python1 post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2018/05/16/python1/">python异常之UnicodeEncodeError &#39;ascii&#39;</a>
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://xurui.club/2018/05/16/python1/" data-id="cjyco820a000uq4da7reh8fri" class="leave-reply bdsharebuttonbox" data-cmd="more">分享</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>很多时候,你想打印一些数据，想直观的看看结果。可是！你在python中的print()语句报错了。<br>如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;test_aci.py&quot;, line 2, in &lt;module&gt;</span><br><span class="line">    print(&apos;xt\u7ecf&apos;)</span><br><span class="line">UnicodeEncodeError: &apos;ascii&apos; codec can&apos;t encode character &apos;\u7ecf&apos; in position 2: ordinal not in range(128)</span><br></pre></td></tr></table></figure></p>
<p>你查阅各种博客，知道了是编码问题，可是试了几种方法无法解决。可能在前面加# coding=utf-8,这种思路是对的，但是解决的是sys编码.而你遇到的是print编码问题，就好像你手流血了，你包扎脚。..。。就没有什么好点的方法吗？<br>那我们怎么包扎手呢？<br>在你的py文件合适的位置加上这么一句：<br>如果你不知道在哪，在<code>import</code> 后面加就行。。<br>python2:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if sys.stdout.encoding != &apos;UTF-8&apos;:</span><br><span class="line">    sys.stdout = codecs.getwriter(&apos;utf-8&apos;)(sys.stdout, &apos;strict&apos;)</span><br><span class="line">if sys.stderr.encoding != &apos;UTF-8&apos;:</span><br><span class="line">    sys.stderr = codecs.getwriter(&apos;utf-8&apos;)(sys.stderr, &apos;strict&apos;)</span><br></pre></td></tr></table></figure></p>
<p>python3:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if sys.stdout.encoding != &apos;UTF-8&apos;:</span><br><span class="line">    sys.stdout = codecs.getwriter(&apos;utf-8&apos;)(sys.stdout.buffer, &apos;strict&apos;)</span><br><span class="line">if sys.stderr.encoding != &apos;UTF-8&apos;:</span><br><span class="line">    sys.stderr = codecs.getwriter(&apos;utf-8&apos;)(sys.stderr.buffer, &apos;strict&apos;)</span><br></pre></td></tr></table></figure></p>
<p>问题解决，完美！</p>
<p>可以看出，这是stdout问题，不是加个<code>#coding:utf-8</code>就可以的。这主要是环境配置问题，你当然可以修改环境配置，是个一劳永逸的办法，但很多时候，你拿不到root权限呀，哈哈。</p>
<p>每天一点，进步不难。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/05/16/python1/">
    <time datetime="2018-05-16T06:17:00.000Z" class="entry-date">
        2018-05-16
    </time>
</a>
    
    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/异常解决/">异常解决</a></li></ul>

    </footer>
</article>






  
    <article id="post-pandas1" class="post-pandas1 post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2018/05/16/pandas1/">pandas之concat</a>
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://xurui.club/2018/05/16/pandas1/" data-id="cjyco8206000pq4da9c0rubvh" class="leave-reply bdsharebuttonbox" data-cmd="more">分享</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>如果我们有两个dataFrame,该怎么合在一起呢？<br>用concat<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">    inp1 = [&#123;<span class="string">'c1'</span>: <span class="number">10</span>, <span class="string">'c2'</span>: <span class="number">100</span>&#125;, &#123;<span class="string">'c1'</span>: <span class="number">11</span>, <span class="string">'c2'</span>: <span class="number">110</span>&#125;, &#123;<span class="string">'c1'</span>: <span class="number">12</span>, <span class="string">'c2'</span>: <span class="number">120</span>&#125;]</span><br><span class="line">    inp2 = [&#123;<span class="string">'c1'</span>: <span class="number">20</span>, <span class="string">'c2'</span>: <span class="number">100</span>&#125;, &#123;<span class="string">'c1'</span>: <span class="number">21</span>, <span class="string">'c2'</span>: <span class="number">110</span>&#125;, &#123;<span class="string">'c1'</span>: <span class="number">22</span>, <span class="string">'c2'</span>: <span class="number">120</span>&#125;]</span><br><span class="line">    df1 = pd.DataFrame(inp1)</span><br><span class="line">    print(df1)</span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">10</span>)</span><br><span class="line">    df2 = pd.DataFrame(inp2)</span><br><span class="line">    print(df2)</span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">10</span>)</span><br><span class="line">    df = pd.concat([df1, df2], ignore_index=<span class="keyword">True</span>)</span><br><span class="line">    print(df)</span><br><span class="line"></span><br><span class="line"><span class="comment">#output:</span></span><br><span class="line"> c1   c2</span><br><span class="line"><span class="number">0</span>  <span class="number">10</span>  <span class="number">100</span></span><br><span class="line"><span class="number">1</span>  <span class="number">11</span>  <span class="number">110</span></span><br><span class="line"><span class="number">2</span>  <span class="number">12</span>  <span class="number">120</span></span><br><span class="line">----------</span><br><span class="line">   c1   c2</span><br><span class="line"><span class="number">0</span>  <span class="number">20</span>  <span class="number">100</span></span><br><span class="line"><span class="number">1</span>  <span class="number">21</span>  <span class="number">110</span></span><br><span class="line"><span class="number">2</span>  <span class="number">22</span>  <span class="number">120</span></span><br><span class="line">----------</span><br><span class="line">   c1   c2</span><br><span class="line"><span class="number">0</span>  <span class="number">10</span>  <span class="number">100</span></span><br><span class="line"><span class="number">1</span>  <span class="number">11</span>  <span class="number">110</span></span><br><span class="line"><span class="number">2</span>  <span class="number">12</span>  <span class="number">120</span></span><br><span class="line"><span class="number">3</span>  <span class="number">20</span>  <span class="number">100</span></span><br><span class="line"><span class="number">4</span>  <span class="number">21</span>  <span class="number">110</span></span><br><span class="line"><span class="number">5</span>  <span class="number">22</span>  <span class="number">120</span></span><br></pre></td></tr></table></figure></p>
<p>每天一点，学习不难。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/05/16/pandas1/">
    <time datetime="2018-05-16T02:20:48.000Z" class="entry-date">
        2018-05-16
    </time>
</a>
    
    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pandas/">pandas</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/数据分析/">数据分析</a></li></ul>

    </footer>
</article>






  
    <article id="post-linux1" class="post-linux1 post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2018/05/16/linux1/">linux命令之grep</a>
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://xurui.club/2018/05/16/linux1/" data-id="cjyco8201000kq4darm061qx7" class="leave-reply bdsharebuttonbox" data-cmd="more">分享</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>今天在跑分类模型，发现了一些比较奇怪的结果，严重影响了分类器性能，所有就想看看这些数据是啥，因为数据是加密的，只能看到id,所以这里就把id取出来，693d0f86a8a90d5a138f75ac7160490c，这里经过md5加密的，然后经过查表解密，得到其明文id,这里假设id为im_12345,然后我们就可以去明文中查找带这个id的数据了。数据放在linux下，我们在终端中用<br><code>grep &#39;im_12345&#39; xx.txt</code><br><code>//xx.txt为要查找的文件名</code><br>就可以返回我们想看的那条数据了。<br>然后发现，返回了一大堆数据，实在不知道哪条才是<br>于是我们再做一次grep<br>即<br><code>grep &#39;im_12345&#39; xx.txt|prep &#39;2017-12-12&#39;</code></p>
<p>总结，<code>grep pattern1 file|grep pattern2</code><br>也就是在pattern1匹配的基础上再进行一次grep<br>哈哈，每天一点，进步不难。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/05/16/linux1/">
    <time datetime="2018-05-16T02:20:05.000Z" class="entry-date">
        2018-05-16
    </time>
</a>
    
    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/linux/">linux</a></li></ul>

    </footer>
</article>






  
  
    <nav id="pagination">
      <nav id="page-nav">
        <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
      </nav>
    </nav>
  


</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    <aside id="search" class="widget widget_search"><form role="search" method="get" accept-charset="utf-8" id="searchform" class="searchform" action="//google.com/search">
    <div>
        <input type="text" value="" name="s" id="s" />
        <input type="submit" id="searchsubmit" value="搜索" />
    </div>
</form></aside>
  
    
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/心情/">心情</a><span class="category-list-count">1</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2019/07/21/hive/">hive 之join的用法</a>
          </li>
        
          <li>
            <a href="/2019/07/13/linux/">linux之scp</a>
          </li>
        
          <li>
            <a href="/2019/07/11/hive笔记/">hive笔记1</a>
          </li>
        
          <li>
            <a href="/2018/09/11/interview-3/">1042. 托普利兹矩阵 python</a>
          </li>
        
          <li>
            <a href="/2018/09/11/interview-2/">lintcode 814. Shortest Path in Undirected Graph 之python 和 java 实现</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keras/">Keras</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/echart/">echart</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/">hive</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/lda/">lda</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pandas/">pandas</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/">sqoop</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensorflow/">tensorflow</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zookeeper/">zookeeper</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/可视化/">可视化</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/大数据/">大数据</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/异常解决/">异常解决</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/心情/">心情</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/技术/">技术</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据分析/">数据分析</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习/">深度学习</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/胶囊网络/">胶囊网络</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面试/">面试</a><span class="tag-list-count">5</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/Keras/" style="font-size: 11.67px;">Keras</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/echart/" style="font-size: 10px;">echart</a> <a href="/tags/hive/" style="font-size: 11.67px;">hive</a> <a href="/tags/java/" style="font-size: 13.33px;">java</a> <a href="/tags/lda/" style="font-size: 10px;">lda</a> <a href="/tags/linux/" style="font-size: 11.67px;">linux</a> <a href="/tags/mysql/" style="font-size: 11.67px;">mysql</a> <a href="/tags/pandas/" style="font-size: 10px;">pandas</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/sqoop/" style="font-size: 10px;">sqoop</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/zookeeper/" style="font-size: 10px;">zookeeper</a> <a href="/tags/可视化/" style="font-size: 10px;">可视化</a> <a href="/tags/大数据/" style="font-size: 11.67px;">大数据</a> <a href="/tags/异常解决/" style="font-size: 16.67px;">异常解决</a> <a href="/tags/心情/" style="font-size: 11.67px;">心情</a> <a href="/tags/技术/" style="font-size: 10px;">技术</a> <a href="/tags/数据分析/" style="font-size: 10px;">数据分析</a> <a href="/tags/深度学习/" style="font-size: 18.33px;">深度学习</a> <a href="/tags/胶囊网络/" style="font-size: 10px;">胶囊网络</a> <a href="/tags/面试/" style="font-size: 16.67px;">面试</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2019 Petrus Zhang
    All rights reserved.</p>
    <p>My Way to Better!</p>
</footer>


  <span>Hosted by <a href="https://pages.coding.me" >Coding Pages</a></span>
    <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":false,"bdPic":"","bdStyle":"2","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>

<script src="/js/jquery-2.0.3.min.js"></script>
<script type="text/javascript" src="/js/local_search.js"></script>

  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
</body>
</html>